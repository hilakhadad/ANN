{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 3 - Part A - ANN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 11 - Implementing a Multi-layer Artificial Neural Network from Scratch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtaining and preparing the MNIST dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n",
    "\n",
    "- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 examples)\n",
    "- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n",
    "- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 examples)\n",
    "- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Normalize to [-1, 1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = ((X / 255.) - .5) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize the first digit of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split into training, validation, and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_test:(21000, 784)\n",
      "shape of y_test:(21000,)\n",
      "shape of X_train:(44000, 784)\n",
      "shape of y_train:(44000,)\n",
      "shape of X_valid:(5000, 784)\n",
      "shape of y_valid:(5000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=123, stratify=y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)\n",
    "\n",
    "print(f\"shape of X_test:{X_test.shape}\")\n",
    "print(f\"shape of y_test:{y_test.shape}\")\n",
    "print(f\"shape of X_train:{X_train.shape}\")\n",
    "print(f\"shape of y_train:{y_train.shape}\")\n",
    "print(f\"shape of X_valid:{X_valid.shape}\")\n",
    "print(f\"shape of y_valid:{y_valid.shape}\")\n",
    "\n",
    "# optional to free up some memory by deleting non-used arrays:\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Original implementation of one layer perceptron"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "\n",
    "class NeuralNetMLP:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # hidden\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "\n",
    "        self.weight_h = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "\n",
    "        # output\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        a_h = sigmoid(z_h)\n",
    "\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden] dot [n_classes, n_hidden].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n",
    "        a_out = sigmoid(z_out)\n",
    "        return a_h, a_out\n",
    "\n",
    "    def backward(self, x, a_h, a_out, y):\n",
    "\n",
    "        #########################\n",
    "        ### Output layer weights\n",
    "        #########################\n",
    "\n",
    "        # onehot encoding\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "\n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## for convenient re-use\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = a_out * (1. - a_out) # sigmoid derivative\n",
    "\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule) placeholder\"\n",
    "\n",
    "        # gradient for output weights\n",
    "\n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_out__dw_out = a_h\n",
    "\n",
    "        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]\n",
    "        # output dim: [n_classes, n_hidden]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "\n",
    "\n",
    "        #################################\n",
    "        # Part 2: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "\n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_out\n",
    "\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "\n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative\n",
    "\n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "\n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out,\n",
    "                d_loss__d_w_h, d_loss__d_b_h)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "model = NeuralNetMLP(num_features=28*28,\n",
    "                     num_hidden=50,\n",
    "                     num_classes=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size\n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "\n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "\n",
    "# iterate over training epochs\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # iterate over minibatches\n",
    "    minibatch_gen = minibatch_generator(\n",
    "        X_train, y_train, minibatch_size)\n",
    "\n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "\n",
    "    break\n",
    "\n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation MSE: 0.3\n",
      "Initial validation accuracy: 9.2%\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets)\n",
    "\n",
    "\n",
    "_, probas = model.forward(X_valid)\n",
    "mse = mse_loss(y_valid, probas)\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(y_valid, predicted_labels)\n",
    "\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "\n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "\n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "\n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/i\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial valid MSE: 0.3\n",
      "Initial valid accuracy: 9.2%\n"
     ]
    }
   ],
   "source": [
    "mse, acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # iterate over minibatches\n",
    "        minibatch_gen = minibatch_generator(\n",
    "            X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "            #### Compute outputs ####\n",
    "            a_h, a_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Compute gradients ####\n",
    "            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = \\\n",
    "                model.backward(X_train_mini, a_h, a_out, y_train_mini)\n",
    "\n",
    "            #### Update weights ####\n",
    "            model.weight_h -= learning_rate * d_loss__d_w_h\n",
    "            model.bias_h -= learning_rate * d_loss__d_b_h\n",
    "            model.weight_out -= learning_rate * d_loss__d_w_out\n",
    "            model.bias_out -= learning_rate * d_loss__d_b_out\n",
    "\n",
    "        #### Epoch Logging ####\n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Train MSE: 0.06 | Train Acc: 72.79% | Valid Acc: 72.92%\n",
      "Epoch: 002/050 | Train MSE: 0.04 | Train Acc: 83.11% | Valid Acc: 82.48%\n",
      "Epoch: 003/050 | Train MSE: 0.03 | Train Acc: 86.96% | Valid Acc: 86.20%\n",
      "Epoch: 004/050 | Train MSE: 0.02 | Train Acc: 88.48% | Valid Acc: 88.10%\n",
      "Epoch: 005/050 | Train MSE: 0.02 | Train Acc: 89.45% | Valid Acc: 89.10%\n",
      "Epoch: 006/050 | Train MSE: 0.02 | Train Acc: 90.06% | Valid Acc: 89.64%\n",
      "Epoch: 007/050 | Train MSE: 0.02 | Train Acc: 90.53% | Valid Acc: 90.34%\n",
      "Epoch: 008/050 | Train MSE: 0.02 | Train Acc: 90.84% | Valid Acc: 90.66%\n",
      "Epoch: 009/050 | Train MSE: 0.02 | Train Acc: 91.33% | Valid Acc: 90.98%\n",
      "Epoch: 010/050 | Train MSE: 0.02 | Train Acc: 91.46% | Valid Acc: 91.24%\n",
      "Epoch: 011/050 | Train MSE: 0.01 | Train Acc: 91.67% | Valid Acc: 91.16%\n",
      "Epoch: 012/050 | Train MSE: 0.01 | Train Acc: 91.88% | Valid Acc: 91.48%\n",
      "Epoch: 013/050 | Train MSE: 0.01 | Train Acc: 92.23% | Valid Acc: 91.66%\n",
      "Epoch: 014/050 | Train MSE: 0.01 | Train Acc: 92.31% | Valid Acc: 91.94%\n",
      "Epoch: 015/050 | Train MSE: 0.01 | Train Acc: 92.50% | Valid Acc: 91.98%\n",
      "Epoch: 016/050 | Train MSE: 0.01 | Train Acc: 92.70% | Valid Acc: 92.22%\n",
      "Epoch: 017/050 | Train MSE: 0.01 | Train Acc: 92.87% | Valid Acc: 92.48%\n",
      "Epoch: 018/050 | Train MSE: 0.01 | Train Acc: 92.99% | Valid Acc: 92.54%\n",
      "Epoch: 019/050 | Train MSE: 0.01 | Train Acc: 93.07% | Valid Acc: 92.54%\n",
      "Epoch: 020/050 | Train MSE: 0.01 | Train Acc: 93.18% | Valid Acc: 92.56%\n",
      "Epoch: 021/050 | Train MSE: 0.01 | Train Acc: 93.27% | Valid Acc: 92.76%\n",
      "Epoch: 022/050 | Train MSE: 0.01 | Train Acc: 93.44% | Valid Acc: 92.88%\n",
      "Epoch: 023/050 | Train MSE: 0.01 | Train Acc: 93.51% | Valid Acc: 92.92%\n",
      "Epoch: 024/050 | Train MSE: 0.01 | Train Acc: 93.62% | Valid Acc: 93.06%\n",
      "Epoch: 025/050 | Train MSE: 0.01 | Train Acc: 93.71% | Valid Acc: 93.12%\n",
      "Epoch: 026/050 | Train MSE: 0.01 | Train Acc: 93.77% | Valid Acc: 93.24%\n",
      "Epoch: 027/050 | Train MSE: 0.01 | Train Acc: 93.90% | Valid Acc: 93.20%\n",
      "Epoch: 028/050 | Train MSE: 0.01 | Train Acc: 94.01% | Valid Acc: 93.26%\n",
      "Epoch: 029/050 | Train MSE: 0.01 | Train Acc: 94.03% | Valid Acc: 93.48%\n",
      "Epoch: 030/050 | Train MSE: 0.01 | Train Acc: 94.09% | Valid Acc: 93.26%\n",
      "Epoch: 031/050 | Train MSE: 0.01 | Train Acc: 94.14% | Valid Acc: 93.56%\n",
      "Epoch: 032/050 | Train MSE: 0.01 | Train Acc: 94.26% | Valid Acc: 93.52%\n",
      "Epoch: 033/050 | Train MSE: 0.01 | Train Acc: 94.31% | Valid Acc: 93.48%\n",
      "Epoch: 034/050 | Train MSE: 0.01 | Train Acc: 94.41% | Valid Acc: 93.70%\n",
      "Epoch: 035/050 | Train MSE: 0.01 | Train Acc: 94.49% | Valid Acc: 93.78%\n",
      "Epoch: 036/050 | Train MSE: 0.01 | Train Acc: 94.53% | Valid Acc: 93.74%\n",
      "Epoch: 037/050 | Train MSE: 0.01 | Train Acc: 94.53% | Valid Acc: 93.78%\n",
      "Epoch: 038/050 | Train MSE: 0.01 | Train Acc: 94.67% | Valid Acc: 93.86%\n",
      "Epoch: 039/050 | Train MSE: 0.01 | Train Acc: 94.76% | Valid Acc: 93.72%\n",
      "Epoch: 040/050 | Train MSE: 0.01 | Train Acc: 94.73% | Valid Acc: 93.90%\n",
      "Epoch: 041/050 | Train MSE: 0.01 | Train Acc: 94.80% | Valid Acc: 93.96%\n",
      "Epoch: 042/050 | Train MSE: 0.01 | Train Acc: 94.83% | Valid Acc: 93.86%\n",
      "Epoch: 043/050 | Train MSE: 0.01 | Train Acc: 94.92% | Valid Acc: 94.12%\n",
      "Epoch: 044/050 | Train MSE: 0.01 | Train Acc: 94.96% | Valid Acc: 94.12%\n",
      "Epoch: 045/050 | Train MSE: 0.01 | Train Acc: 95.00% | Valid Acc: 94.14%\n",
      "Epoch: 046/050 | Train MSE: 0.01 | Train Acc: 95.06% | Valid Acc: 94.14%\n",
      "Epoch: 047/050 | Train MSE: 0.01 | Train Acc: 95.11% | Valid Acc: 94.14%\n",
      "Epoch: 048/050 | Train MSE: 0.01 | Train Acc: 95.18% | Valid Acc: 94.32%\n",
      "Epoch: 049/050 | Train MSE: 0.01 | Train Acc: 95.19% | Valid Acc: 94.40%\n",
      "Epoch: 050/050 | Train MSE: 0.01 | Train Acc: 95.26% | Valid Acc: 94.32%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123) # for the training set shuffling\n",
    "\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc = train(\n",
    "    model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=50, learning_rate=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 94.28%\n"
     ]
    }
   ],
   "source": [
    "test_mse, test_acc_1_layer_ANN = compute_mse_and_acc(model, X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc_1_layer_ANN*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Original implementation of one layer perceptron - END"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implementing a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):                                        \n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "\n",
    "class NeuralNetMLP:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_hidden2, num_classes, random_seed=123):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # hidden\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        \n",
    "        self.weight_h = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "\n",
    "        self.weight_h2 = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden2, num_hidden))\n",
    "        self.bias_h2 = np.zeros(num_hidden2)\n",
    "        \n",
    "        # output\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_classes, num_hidden2))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        a_h = sigmoid(z_h)\n",
    "\n",
    "        # Hidden layer2\n",
    "        # input dim: [n_examples, n_hidden] dot [n_hidden2, n_hidden].T\n",
    "        # output dim: [n_examples, n_hidden2]\n",
    "        z_h2 = np.dot(a_h, self.weight_h2.T) + self.bias_h2\n",
    "        a_h2 = sigmoid(z_h2)\n",
    "\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden2] dot [n_classes, n_hidden2].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h2, self.weight_out.T) + self.bias_out\n",
    "        a_out = sigmoid(z_out)\n",
    "        return a_h, a_h2, a_out\n",
    "\n",
    "    def backward(self, x, a_h, a_h2, a_out, y):\n",
    "    \n",
    "        #########################\n",
    "        ### Output layer weights\n",
    "        #########################\n",
    "        \n",
    "        # onehot encoding\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "\n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## for convenient re-use\n",
    "        \n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = a_out * (1. - a_out) # sigmoid derivative\n",
    "\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule) placeholder\"\n",
    "\n",
    "        # gradient for output weights\n",
    "        \n",
    "        # [n_examples, n_hidden2]\n",
    "        d_z_out__dw_out = a_h2\n",
    "\n",
    "        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden2]\n",
    "        # output dim: [n_classes, n_hidden2]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "\n",
    "\n",
    "        #################################\n",
    "        # Part 2: dLoss/dHiddenWeights2\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct2 * dHiddenAct2/dHiddenNet2 * dHiddenNet2/dWeight2\n",
    "\n",
    "        # [n_classes, n_hidden2]\n",
    "        d_z_out__a_h2 = self.weight_out\n",
    "\n",
    "        # output dim: [n_examples, n_hidden2]\n",
    "        d_loss__a_h2 = np.dot(delta_out, d_z_out__a_h2)\n",
    "\n",
    "        # [n_examples, n_hidden2]\n",
    "        d_a_h__d_z_h2 = a_h2 * (1. - a_h2) # sigmoid derivative\n",
    "\n",
    "        delta_h2 = d_loss__a_h2 * d_a_h__d_z_h2\n",
    "\n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_h__d_w_h2 = a_h\n",
    "\n",
    "        # output dim: [n_hidden2, n_hidden]\n",
    "        d_loss__d_w_h2 = np.dot((d_loss__a_h2 * d_a_h__d_z_h2).T, d_z_h__d_w_h2)\n",
    "        d_loss__d_b_h2 = np.sum((d_loss__a_h2 * d_a_h__d_z_h2), axis=0)\n",
    "\n",
    "        #################################        \n",
    "        # Part 3: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "        \n",
    "        # [n_hidden2, n_hidden]\n",
    "        d_z_out__a_h = self.weight_h2\n",
    "        \n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_h2, d_z_out__a_h)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative\n",
    "        \n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        \n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out,\n",
    "                d_loss__d_w_h2, d_loss__d_b_h2,\n",
    "                d_loss__d_w_h, d_loss__d_b_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNetMLP(num_features=28*28,\n",
    "                     num_hidden=50,\n",
    "                     num_hidden2=50,\n",
    "                     num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Coding the neural network training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Defining data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "        \n",
    "# iterate over training epochs\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # iterate over minibatches\n",
    "    minibatch_gen = minibatch_generator(\n",
    "        X_train, y_train, minibatch_size)\n",
    "    \n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "        \n",
    "    break\n",
    "    \n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Defining a function to compute the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation MSE: 0.2\n",
      "Initial validation accuracy: 9.0%\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets) \n",
    "\n",
    "\n",
    "_,_, probas = model.forward(X_valid)\n",
    "mse = mse_loss(y_valid, probas)\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(y_valid, predicted_labels)\n",
    "\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _,_, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/i\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial valid MSE: 0.3\n",
      "Initial valid accuracy: 9.0%\n"
     ]
    }
   ],
   "source": [
    "mse, acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # iterate over minibatches\n",
    "        minibatch_gen = minibatch_generator(\n",
    "            X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "            \n",
    "            #### Compute outputs ####\n",
    "            a_h, a_h2, a_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Compute gradients ####\n",
    "            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h2, d_loss__d_b_h2, d_loss__d_w_h, d_loss__d_b_h = \\\n",
    "                model.backward(X_train_mini, a_h, a_h2, a_out, y_train_mini)\n",
    "\n",
    "            #### Update weights ####\n",
    "            model.weight_h -= learning_rate * d_loss__d_w_h\n",
    "            model.bias_h -= learning_rate * d_loss__d_b_h\n",
    "            model.weight_h2 -= learning_rate * d_loss__d_w_h2\n",
    "            model.bias_h2 -= learning_rate * d_loss__d_b_h2\n",
    "            model.weight_out -= learning_rate * d_loss__d_w_out\n",
    "            model.bias_out -= learning_rate * d_loss__d_b_out\n",
    "        \n",
    "        #### Epoch Logging ####        \n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Train MSE: 0.09 | Train Acc: 20.81% | Valid Acc: 20.74%\n",
      "Epoch: 002/050 | Train MSE: 0.09 | Train Acc: 26.82% | Valid Acc: 26.68%\n",
      "Epoch: 003/050 | Train MSE: 0.08 | Train Acc: 31.01% | Valid Acc: 30.58%\n",
      "Epoch: 004/050 | Train MSE: 0.07 | Train Acc: 50.75% | Valid Acc: 50.86%\n",
      "Epoch: 005/050 | Train MSE: 0.06 | Train Acc: 62.88% | Valid Acc: 63.08%\n",
      "Epoch: 006/050 | Train MSE: 0.05 | Train Acc: 70.61% | Valid Acc: 70.60%\n",
      "Epoch: 007/050 | Train MSE: 0.04 | Train Acc: 79.21% | Valid Acc: 79.32%\n",
      "Epoch: 008/050 | Train MSE: 0.03 | Train Acc: 84.49% | Valid Acc: 84.82%\n",
      "Epoch: 009/050 | Train MSE: 0.03 | Train Acc: 86.76% | Valid Acc: 87.00%\n",
      "Epoch: 010/050 | Train MSE: 0.02 | Train Acc: 87.76% | Valid Acc: 87.92%\n",
      "Epoch: 011/050 | Train MSE: 0.02 | Train Acc: 88.45% | Valid Acc: 88.42%\n",
      "Epoch: 012/050 | Train MSE: 0.02 | Train Acc: 89.08% | Valid Acc: 88.86%\n",
      "Epoch: 013/050 | Train MSE: 0.02 | Train Acc: 89.69% | Valid Acc: 89.58%\n",
      "Epoch: 014/050 | Train MSE: 0.02 | Train Acc: 90.04% | Valid Acc: 89.88%\n",
      "Epoch: 015/050 | Train MSE: 0.02 | Train Acc: 90.39% | Valid Acc: 89.84%\n",
      "Epoch: 016/050 | Train MSE: 0.02 | Train Acc: 90.70% | Valid Acc: 90.34%\n",
      "Epoch: 017/050 | Train MSE: 0.02 | Train Acc: 91.10% | Valid Acc: 90.58%\n",
      "Epoch: 018/050 | Train MSE: 0.02 | Train Acc: 91.34% | Valid Acc: 90.72%\n",
      "Epoch: 019/050 | Train MSE: 0.01 | Train Acc: 91.50% | Valid Acc: 90.98%\n",
      "Epoch: 020/050 | Train MSE: 0.01 | Train Acc: 91.77% | Valid Acc: 91.18%\n",
      "Epoch: 021/050 | Train MSE: 0.01 | Train Acc: 92.00% | Valid Acc: 91.38%\n",
      "Epoch: 022/050 | Train MSE: 0.01 | Train Acc: 92.15% | Valid Acc: 91.36%\n",
      "Epoch: 023/050 | Train MSE: 0.01 | Train Acc: 92.31% | Valid Acc: 91.86%\n",
      "Epoch: 024/050 | Train MSE: 0.01 | Train Acc: 92.59% | Valid Acc: 92.20%\n",
      "Epoch: 025/050 | Train MSE: 0.01 | Train Acc: 92.74% | Valid Acc: 92.14%\n",
      "Epoch: 026/050 | Train MSE: 0.01 | Train Acc: 92.89% | Valid Acc: 92.40%\n",
      "Epoch: 027/050 | Train MSE: 0.01 | Train Acc: 93.05% | Valid Acc: 92.62%\n",
      "Epoch: 028/050 | Train MSE: 0.01 | Train Acc: 93.26% | Valid Acc: 92.64%\n",
      "Epoch: 029/050 | Train MSE: 0.01 | Train Acc: 93.42% | Valid Acc: 92.84%\n",
      "Epoch: 030/050 | Train MSE: 0.01 | Train Acc: 93.56% | Valid Acc: 93.02%\n",
      "Epoch: 031/050 | Train MSE: 0.01 | Train Acc: 93.62% | Valid Acc: 93.26%\n",
      "Epoch: 032/050 | Train MSE: 0.01 | Train Acc: 93.76% | Valid Acc: 93.22%\n",
      "Epoch: 033/050 | Train MSE: 0.01 | Train Acc: 93.85% | Valid Acc: 93.16%\n",
      "Epoch: 034/050 | Train MSE: 0.01 | Train Acc: 93.97% | Valid Acc: 93.42%\n",
      "Epoch: 035/050 | Train MSE: 0.01 | Train Acc: 94.08% | Valid Acc: 93.56%\n",
      "Epoch: 036/050 | Train MSE: 0.01 | Train Acc: 94.19% | Valid Acc: 93.58%\n",
      "Epoch: 037/050 | Train MSE: 0.01 | Train Acc: 94.19% | Valid Acc: 93.48%\n",
      "Epoch: 038/050 | Train MSE: 0.01 | Train Acc: 94.46% | Valid Acc: 93.76%\n",
      "Epoch: 039/050 | Train MSE: 0.01 | Train Acc: 94.51% | Valid Acc: 93.78%\n",
      "Epoch: 040/050 | Train MSE: 0.01 | Train Acc: 94.67% | Valid Acc: 93.80%\n",
      "Epoch: 041/050 | Train MSE: 0.01 | Train Acc: 94.71% | Valid Acc: 93.96%\n",
      "Epoch: 042/050 | Train MSE: 0.01 | Train Acc: 94.83% | Valid Acc: 93.90%\n",
      "Epoch: 043/050 | Train MSE: 0.01 | Train Acc: 94.89% | Valid Acc: 94.10%\n",
      "Epoch: 044/050 | Train MSE: 0.01 | Train Acc: 94.93% | Valid Acc: 94.14%\n",
      "Epoch: 045/050 | Train MSE: 0.01 | Train Acc: 95.09% | Valid Acc: 94.22%\n",
      "Epoch: 046/050 | Train MSE: 0.01 | Train Acc: 95.16% | Valid Acc: 94.18%\n",
      "Epoch: 047/050 | Train MSE: 0.01 | Train Acc: 95.25% | Valid Acc: 94.30%\n",
      "Epoch: 048/050 | Train MSE: 0.01 | Train Acc: 95.32% | Valid Acc: 94.40%\n",
      "Epoch: 049/050 | Train MSE: 0.01 | Train Acc: 95.35% | Valid Acc: 94.58%\n",
      "Epoch: 050/050 | Train MSE: 0.01 | Train Acc: 95.43% | Valid Acc: 94.66%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123) # for the training set shuffling\n",
    "\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc = train(\n",
    "    model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 94.42%\n"
     ]
    }
   ],
   "source": [
    "test_mse_ANN, test_acc_ANN = compute_mse_and_acc(model, X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc_ANN*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ----------------------------------Pytorch Full ANN  start------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, photos, labels, transform=None, target_transform=None):\n",
    "        self.img_labels = labels\n",
    "        self.img_dir = photos\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img_dir[idx]\n",
    "        label = self.img_labels[idx]\n",
    "        image = torch.from_numpy(image).float()\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size_train = 100\n",
    "batch_size_test = 100\n",
    "learning_rate = 0.1\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  CustomImageDataset(X_temp, y_temp),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  CustomImageDataset(X_test, y_test),\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 50)\n",
    "        self.final = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(self.final(x))\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  test_acc_torch = 100. * correct / len(test_loader.dataset)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    test_acc_torch))\n",
    "  return test_acc_torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shair\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\shair\\AppData\\Local\\Temp/ipykernel_15064/2640556148.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\shair\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3044, Accuracy: 2682/21000 (12.77%)\n",
      "\n",
      "Train Epoch: 1 [0/49000 (0%)]\tLoss: 2.295865\n",
      "Train Epoch: 1 [1000/49000 (2%)]\tLoss: 2.230462\n",
      "Train Epoch: 1 [2000/49000 (4%)]\tLoss: 2.136461\n",
      "Train Epoch: 1 [3000/49000 (6%)]\tLoss: 2.064350\n",
      "Train Epoch: 1 [4000/49000 (8%)]\tLoss: 2.005983\n",
      "Train Epoch: 1 [5000/49000 (10%)]\tLoss: 1.950590\n",
      "Train Epoch: 1 [6000/49000 (12%)]\tLoss: 1.907362\n",
      "Train Epoch: 1 [7000/49000 (14%)]\tLoss: 1.865948\n",
      "Train Epoch: 1 [8000/49000 (16%)]\tLoss: 1.868908\n",
      "Train Epoch: 1 [9000/49000 (18%)]\tLoss: 1.809529\n",
      "Train Epoch: 1 [10000/49000 (20%)]\tLoss: 1.807331\n",
      "Train Epoch: 1 [11000/49000 (22%)]\tLoss: 1.790092\n",
      "Train Epoch: 1 [12000/49000 (24%)]\tLoss: 1.779144\n",
      "Train Epoch: 1 [13000/49000 (27%)]\tLoss: 1.764257\n",
      "Train Epoch: 1 [14000/49000 (29%)]\tLoss: 1.734679\n",
      "Train Epoch: 1 [15000/49000 (31%)]\tLoss: 1.772699\n",
      "Train Epoch: 1 [16000/49000 (33%)]\tLoss: 1.745229\n",
      "Train Epoch: 1 [17000/49000 (35%)]\tLoss: 1.694989\n",
      "Train Epoch: 1 [18000/49000 (37%)]\tLoss: 1.722894\n",
      "Train Epoch: 1 [19000/49000 (39%)]\tLoss: 1.705974\n",
      "Train Epoch: 1 [20000/49000 (41%)]\tLoss: 1.724299\n",
      "Train Epoch: 1 [21000/49000 (43%)]\tLoss: 1.700930\n",
      "Train Epoch: 1 [22000/49000 (45%)]\tLoss: 1.716087\n",
      "Train Epoch: 1 [23000/49000 (47%)]\tLoss: 1.701182\n",
      "Train Epoch: 1 [24000/49000 (49%)]\tLoss: 1.684449\n",
      "Train Epoch: 1 [25000/49000 (51%)]\tLoss: 1.693216\n",
      "Train Epoch: 1 [26000/49000 (53%)]\tLoss: 1.704916\n",
      "Train Epoch: 1 [27000/49000 (55%)]\tLoss: 1.669784\n",
      "Train Epoch: 1 [28000/49000 (57%)]\tLoss: 1.649178\n",
      "Train Epoch: 1 [29000/49000 (59%)]\tLoss: 1.648731\n",
      "Train Epoch: 1 [30000/49000 (61%)]\tLoss: 1.651107\n",
      "Train Epoch: 1 [31000/49000 (63%)]\tLoss: 1.718793\n",
      "Train Epoch: 1 [32000/49000 (65%)]\tLoss: 1.725536\n",
      "Train Epoch: 1 [33000/49000 (67%)]\tLoss: 1.658560\n",
      "Train Epoch: 1 [34000/49000 (69%)]\tLoss: 1.630216\n",
      "Train Epoch: 1 [35000/49000 (71%)]\tLoss: 1.627748\n",
      "Train Epoch: 1 [36000/49000 (73%)]\tLoss: 1.678176\n",
      "Train Epoch: 1 [37000/49000 (76%)]\tLoss: 1.691207\n",
      "Train Epoch: 1 [38000/49000 (78%)]\tLoss: 1.624681\n",
      "Train Epoch: 1 [39000/49000 (80%)]\tLoss: 1.625486\n",
      "Train Epoch: 1 [40000/49000 (82%)]\tLoss: 1.620690\n",
      "Train Epoch: 1 [41000/49000 (84%)]\tLoss: 1.619217\n",
      "Train Epoch: 1 [42000/49000 (86%)]\tLoss: 1.674197\n",
      "Train Epoch: 1 [43000/49000 (88%)]\tLoss: 1.661180\n",
      "Train Epoch: 1 [44000/49000 (90%)]\tLoss: 1.626882\n",
      "Train Epoch: 1 [45000/49000 (92%)]\tLoss: 1.653746\n",
      "Train Epoch: 1 [46000/49000 (94%)]\tLoss: 1.660077\n",
      "Train Epoch: 1 [47000/49000 (96%)]\tLoss: 1.659902\n",
      "Train Epoch: 1 [48000/49000 (98%)]\tLoss: 1.627766\n",
      "\n",
      "Test set: Avg. loss: 1.6378, Accuracy: 18348/21000 (87.37%)\n",
      "\n",
      "Train Epoch: 2 [0/49000 (0%)]\tLoss: 1.672060\n",
      "Train Epoch: 2 [1000/49000 (2%)]\tLoss: 1.640054\n",
      "Train Epoch: 2 [2000/49000 (4%)]\tLoss: 1.645250\n",
      "Train Epoch: 2 [3000/49000 (6%)]\tLoss: 1.630338\n",
      "Train Epoch: 2 [4000/49000 (8%)]\tLoss: 1.647087\n",
      "Train Epoch: 2 [5000/49000 (10%)]\tLoss: 1.625633\n",
      "Train Epoch: 2 [6000/49000 (12%)]\tLoss: 1.648866\n",
      "Train Epoch: 2 [7000/49000 (14%)]\tLoss: 1.631474\n",
      "Train Epoch: 2 [8000/49000 (16%)]\tLoss: 1.616373\n",
      "Train Epoch: 2 [9000/49000 (18%)]\tLoss: 1.637330\n",
      "Train Epoch: 2 [10000/49000 (20%)]\tLoss: 1.629067\n",
      "Train Epoch: 2 [11000/49000 (22%)]\tLoss: 1.649945\n",
      "Train Epoch: 2 [12000/49000 (24%)]\tLoss: 1.627713\n",
      "Train Epoch: 2 [13000/49000 (27%)]\tLoss: 1.625563\n",
      "Train Epoch: 2 [14000/49000 (29%)]\tLoss: 1.568058\n",
      "Train Epoch: 2 [15000/49000 (31%)]\tLoss: 1.606547\n",
      "Train Epoch: 2 [16000/49000 (33%)]\tLoss: 1.630890\n",
      "Train Epoch: 2 [17000/49000 (35%)]\tLoss: 1.659968\n",
      "Train Epoch: 2 [18000/49000 (37%)]\tLoss: 1.608114\n",
      "Train Epoch: 2 [19000/49000 (39%)]\tLoss: 1.646772\n",
      "Train Epoch: 2 [20000/49000 (41%)]\tLoss: 1.600003\n",
      "Train Epoch: 2 [21000/49000 (43%)]\tLoss: 1.621175\n",
      "Train Epoch: 2 [22000/49000 (45%)]\tLoss: 1.625462\n",
      "Train Epoch: 2 [23000/49000 (47%)]\tLoss: 1.624230\n",
      "Train Epoch: 2 [24000/49000 (49%)]\tLoss: 1.616893\n",
      "Train Epoch: 2 [25000/49000 (51%)]\tLoss: 1.560797\n",
      "Train Epoch: 2 [26000/49000 (53%)]\tLoss: 1.620414\n",
      "Train Epoch: 2 [27000/49000 (55%)]\tLoss: 1.603430\n",
      "Train Epoch: 2 [28000/49000 (57%)]\tLoss: 1.602296\n",
      "Train Epoch: 2 [29000/49000 (59%)]\tLoss: 1.638927\n",
      "Train Epoch: 2 [30000/49000 (61%)]\tLoss: 1.606704\n",
      "Train Epoch: 2 [31000/49000 (63%)]\tLoss: 1.647981\n",
      "Train Epoch: 2 [32000/49000 (65%)]\tLoss: 1.623219\n",
      "Train Epoch: 2 [33000/49000 (67%)]\tLoss: 1.593099\n",
      "Train Epoch: 2 [34000/49000 (69%)]\tLoss: 1.641482\n",
      "Train Epoch: 2 [35000/49000 (71%)]\tLoss: 1.578391\n",
      "Train Epoch: 2 [36000/49000 (73%)]\tLoss: 1.608020\n",
      "Train Epoch: 2 [37000/49000 (76%)]\tLoss: 1.580216\n",
      "Train Epoch: 2 [38000/49000 (78%)]\tLoss: 1.618125\n",
      "Train Epoch: 2 [39000/49000 (80%)]\tLoss: 1.603573\n",
      "Train Epoch: 2 [40000/49000 (82%)]\tLoss: 1.611433\n",
      "Train Epoch: 2 [41000/49000 (84%)]\tLoss: 1.616535\n",
      "Train Epoch: 2 [42000/49000 (86%)]\tLoss: 1.631350\n",
      "Train Epoch: 2 [43000/49000 (88%)]\tLoss: 1.589877\n",
      "Train Epoch: 2 [44000/49000 (90%)]\tLoss: 1.639880\n",
      "Train Epoch: 2 [45000/49000 (92%)]\tLoss: 1.588217\n",
      "Train Epoch: 2 [46000/49000 (94%)]\tLoss: 1.615547\n",
      "Train Epoch: 2 [47000/49000 (96%)]\tLoss: 1.645403\n",
      "Train Epoch: 2 [48000/49000 (98%)]\tLoss: 1.615521\n",
      "\n",
      "Test set: Avg. loss: 1.6021, Accuracy: 18600/21000 (88.57%)\n",
      "\n",
      "Train Epoch: 3 [0/49000 (0%)]\tLoss: 1.551404\n",
      "Train Epoch: 3 [1000/49000 (2%)]\tLoss: 1.614804\n",
      "Train Epoch: 3 [2000/49000 (4%)]\tLoss: 1.625280\n",
      "Train Epoch: 3 [3000/49000 (6%)]\tLoss: 1.573955\n",
      "Train Epoch: 3 [4000/49000 (8%)]\tLoss: 1.588985\n",
      "Train Epoch: 3 [5000/49000 (10%)]\tLoss: 1.593881\n",
      "Train Epoch: 3 [6000/49000 (12%)]\tLoss: 1.589178\n",
      "Train Epoch: 3 [7000/49000 (14%)]\tLoss: 1.599214\n",
      "Train Epoch: 3 [8000/49000 (16%)]\tLoss: 1.589517\n",
      "Train Epoch: 3 [9000/49000 (18%)]\tLoss: 1.611696\n",
      "Train Epoch: 3 [10000/49000 (20%)]\tLoss: 1.561436\n",
      "Train Epoch: 3 [11000/49000 (22%)]\tLoss: 1.594520\n",
      "Train Epoch: 3 [12000/49000 (24%)]\tLoss: 1.556746\n",
      "Train Epoch: 3 [13000/49000 (27%)]\tLoss: 1.574829\n",
      "Train Epoch: 3 [14000/49000 (29%)]\tLoss: 1.550629\n",
      "Train Epoch: 3 [15000/49000 (31%)]\tLoss: 1.621336\n",
      "Train Epoch: 3 [16000/49000 (33%)]\tLoss: 1.574000\n",
      "Train Epoch: 3 [17000/49000 (35%)]\tLoss: 1.595798\n",
      "Train Epoch: 3 [18000/49000 (37%)]\tLoss: 1.630216\n",
      "Train Epoch: 3 [19000/49000 (39%)]\tLoss: 1.598803\n",
      "Train Epoch: 3 [20000/49000 (41%)]\tLoss: 1.634952\n",
      "Train Epoch: 3 [21000/49000 (43%)]\tLoss: 1.581436\n",
      "Train Epoch: 3 [22000/49000 (45%)]\tLoss: 1.593886\n",
      "Train Epoch: 3 [23000/49000 (47%)]\tLoss: 1.605843\n",
      "Train Epoch: 3 [24000/49000 (49%)]\tLoss: 1.631586\n",
      "Train Epoch: 3 [25000/49000 (51%)]\tLoss: 1.572027\n",
      "Train Epoch: 3 [26000/49000 (53%)]\tLoss: 1.582370\n",
      "Train Epoch: 3 [27000/49000 (55%)]\tLoss: 1.609436\n",
      "Train Epoch: 3 [28000/49000 (57%)]\tLoss: 1.576815\n",
      "Train Epoch: 3 [29000/49000 (59%)]\tLoss: 1.588814\n",
      "Train Epoch: 3 [30000/49000 (61%)]\tLoss: 1.591568\n",
      "Train Epoch: 3 [31000/49000 (63%)]\tLoss: 1.593317\n",
      "Train Epoch: 3 [32000/49000 (65%)]\tLoss: 1.563463\n",
      "Train Epoch: 3 [33000/49000 (67%)]\tLoss: 1.605775\n",
      "Train Epoch: 3 [34000/49000 (69%)]\tLoss: 1.593500\n",
      "Train Epoch: 3 [35000/49000 (71%)]\tLoss: 1.580973\n",
      "Train Epoch: 3 [36000/49000 (73%)]\tLoss: 1.608778\n",
      "Train Epoch: 3 [37000/49000 (76%)]\tLoss: 1.613234\n",
      "Train Epoch: 3 [38000/49000 (78%)]\tLoss: 1.567766\n",
      "Train Epoch: 3 [39000/49000 (80%)]\tLoss: 1.584016\n",
      "Train Epoch: 3 [40000/49000 (82%)]\tLoss: 1.565716\n",
      "Train Epoch: 3 [41000/49000 (84%)]\tLoss: 1.613349\n",
      "Train Epoch: 3 [42000/49000 (86%)]\tLoss: 1.577801\n",
      "Train Epoch: 3 [43000/49000 (88%)]\tLoss: 1.590954\n",
      "Train Epoch: 3 [44000/49000 (90%)]\tLoss: 1.595400\n",
      "Train Epoch: 3 [45000/49000 (92%)]\tLoss: 1.588117\n",
      "Train Epoch: 3 [46000/49000 (94%)]\tLoss: 1.590101\n",
      "Train Epoch: 3 [47000/49000 (96%)]\tLoss: 1.572714\n",
      "Train Epoch: 3 [48000/49000 (98%)]\tLoss: 1.593783\n",
      "\n",
      "Test set: Avg. loss: 1.5901, Accuracy: 18655/21000 (88.83%)\n",
      "\n",
      "Train Epoch: 4 [0/49000 (0%)]\tLoss: 1.586676\n",
      "Train Epoch: 4 [1000/49000 (2%)]\tLoss: 1.618079\n",
      "Train Epoch: 4 [2000/49000 (4%)]\tLoss: 1.576601\n",
      "Train Epoch: 4 [3000/49000 (6%)]\tLoss: 1.579904\n",
      "Train Epoch: 4 [4000/49000 (8%)]\tLoss: 1.571768\n",
      "Train Epoch: 4 [5000/49000 (10%)]\tLoss: 1.552492\n",
      "Train Epoch: 4 [6000/49000 (12%)]\tLoss: 1.580232\n",
      "Train Epoch: 4 [7000/49000 (14%)]\tLoss: 1.558126\n",
      "Train Epoch: 4 [8000/49000 (16%)]\tLoss: 1.612974\n",
      "Train Epoch: 4 [9000/49000 (18%)]\tLoss: 1.566893\n",
      "Train Epoch: 4 [10000/49000 (20%)]\tLoss: 1.554541\n",
      "Train Epoch: 4 [11000/49000 (22%)]\tLoss: 1.550435\n",
      "Train Epoch: 4 [12000/49000 (24%)]\tLoss: 1.619431\n",
      "Train Epoch: 4 [13000/49000 (27%)]\tLoss: 1.631122\n",
      "Train Epoch: 4 [14000/49000 (29%)]\tLoss: 1.576087\n",
      "Train Epoch: 4 [15000/49000 (31%)]\tLoss: 1.604834\n",
      "Train Epoch: 4 [16000/49000 (33%)]\tLoss: 1.551334\n",
      "Train Epoch: 4 [17000/49000 (35%)]\tLoss: 1.561904\n",
      "Train Epoch: 4 [18000/49000 (37%)]\tLoss: 1.572116\n",
      "Train Epoch: 4 [19000/49000 (39%)]\tLoss: 1.550026\n",
      "Train Epoch: 4 [20000/49000 (41%)]\tLoss: 1.629092\n",
      "Train Epoch: 4 [21000/49000 (43%)]\tLoss: 1.581425\n",
      "Train Epoch: 4 [22000/49000 (45%)]\tLoss: 1.579814\n",
      "Train Epoch: 4 [23000/49000 (47%)]\tLoss: 1.587745\n",
      "Train Epoch: 4 [24000/49000 (49%)]\tLoss: 1.555847\n",
      "Train Epoch: 4 [25000/49000 (51%)]\tLoss: 1.600090\n",
      "Train Epoch: 4 [26000/49000 (53%)]\tLoss: 1.556508\n",
      "Train Epoch: 4 [27000/49000 (55%)]\tLoss: 1.587418\n",
      "Train Epoch: 4 [28000/49000 (57%)]\tLoss: 1.562070\n",
      "Train Epoch: 4 [29000/49000 (59%)]\tLoss: 1.605772\n",
      "Train Epoch: 4 [30000/49000 (61%)]\tLoss: 1.601493\n",
      "Train Epoch: 4 [31000/49000 (63%)]\tLoss: 1.617306\n",
      "Train Epoch: 4 [32000/49000 (65%)]\tLoss: 1.624072\n",
      "Train Epoch: 4 [33000/49000 (67%)]\tLoss: 1.535960\n",
      "Train Epoch: 4 [34000/49000 (69%)]\tLoss: 1.566997\n",
      "Train Epoch: 4 [35000/49000 (71%)]\tLoss: 1.571437\n",
      "Train Epoch: 4 [36000/49000 (73%)]\tLoss: 1.572837\n",
      "Train Epoch: 4 [37000/49000 (76%)]\tLoss: 1.595678\n",
      "Train Epoch: 4 [38000/49000 (78%)]\tLoss: 1.608121\n",
      "Train Epoch: 4 [39000/49000 (80%)]\tLoss: 1.530132\n",
      "Train Epoch: 4 [40000/49000 (82%)]\tLoss: 1.580404\n",
      "Train Epoch: 4 [41000/49000 (84%)]\tLoss: 1.570893\n",
      "Train Epoch: 4 [42000/49000 (86%)]\tLoss: 1.550297\n",
      "Train Epoch: 4 [43000/49000 (88%)]\tLoss: 1.601767\n",
      "Train Epoch: 4 [44000/49000 (90%)]\tLoss: 1.584455\n",
      "Train Epoch: 4 [45000/49000 (92%)]\tLoss: 1.589844\n",
      "Train Epoch: 4 [46000/49000 (94%)]\tLoss: 1.601400\n",
      "Train Epoch: 4 [47000/49000 (96%)]\tLoss: 1.611144\n",
      "Train Epoch: 4 [48000/49000 (98%)]\tLoss: 1.570991\n",
      "\n",
      "Test set: Avg. loss: 1.5806, Accuracy: 18750/21000 (89.29%)\n",
      "\n",
      "Train Epoch: 5 [0/49000 (0%)]\tLoss: 1.591255\n",
      "Train Epoch: 5 [1000/49000 (2%)]\tLoss: 1.562188\n",
      "Train Epoch: 5 [2000/49000 (4%)]\tLoss: 1.586595\n",
      "Train Epoch: 5 [3000/49000 (6%)]\tLoss: 1.552436\n",
      "Train Epoch: 5 [4000/49000 (8%)]\tLoss: 1.602186\n",
      "Train Epoch: 5 [5000/49000 (10%)]\tLoss: 1.577695\n",
      "Train Epoch: 5 [6000/49000 (12%)]\tLoss: 1.550769\n",
      "Train Epoch: 5 [7000/49000 (14%)]\tLoss: 1.546020\n",
      "Train Epoch: 5 [8000/49000 (16%)]\tLoss: 1.602775\n",
      "Train Epoch: 5 [9000/49000 (18%)]\tLoss: 1.575276\n",
      "Train Epoch: 5 [10000/49000 (20%)]\tLoss: 1.594516\n",
      "Train Epoch: 5 [11000/49000 (22%)]\tLoss: 1.561284\n",
      "Train Epoch: 5 [12000/49000 (24%)]\tLoss: 1.607992\n",
      "Train Epoch: 5 [13000/49000 (27%)]\tLoss: 1.548181\n",
      "Train Epoch: 5 [14000/49000 (29%)]\tLoss: 1.577408\n",
      "Train Epoch: 5 [15000/49000 (31%)]\tLoss: 1.520358\n",
      "Train Epoch: 5 [16000/49000 (33%)]\tLoss: 1.573080\n",
      "Train Epoch: 5 [17000/49000 (35%)]\tLoss: 1.567999\n",
      "Train Epoch: 5 [18000/49000 (37%)]\tLoss: 1.570600\n",
      "Train Epoch: 5 [19000/49000 (39%)]\tLoss: 1.556629\n",
      "Train Epoch: 5 [20000/49000 (41%)]\tLoss: 1.570682\n",
      "Train Epoch: 5 [21000/49000 (43%)]\tLoss: 1.565867\n",
      "Train Epoch: 5 [22000/49000 (45%)]\tLoss: 1.585909\n",
      "Train Epoch: 5 [23000/49000 (47%)]\tLoss: 1.575302\n",
      "Train Epoch: 5 [24000/49000 (49%)]\tLoss: 1.536683\n",
      "Train Epoch: 5 [25000/49000 (51%)]\tLoss: 1.579512\n",
      "Train Epoch: 5 [26000/49000 (53%)]\tLoss: 1.565024\n",
      "Train Epoch: 5 [27000/49000 (55%)]\tLoss: 1.585522\n",
      "Train Epoch: 5 [28000/49000 (57%)]\tLoss: 1.602655\n",
      "Train Epoch: 5 [29000/49000 (59%)]\tLoss: 1.570464\n",
      "Train Epoch: 5 [30000/49000 (61%)]\tLoss: 1.582286\n",
      "Train Epoch: 5 [31000/49000 (63%)]\tLoss: 1.580912\n",
      "Train Epoch: 5 [32000/49000 (65%)]\tLoss: 1.590217\n",
      "Train Epoch: 5 [33000/49000 (67%)]\tLoss: 1.556908\n",
      "Train Epoch: 5 [34000/49000 (69%)]\tLoss: 1.574041\n",
      "Train Epoch: 5 [35000/49000 (71%)]\tLoss: 1.555179\n",
      "Train Epoch: 5 [36000/49000 (73%)]\tLoss: 1.571715\n",
      "Train Epoch: 5 [37000/49000 (76%)]\tLoss: 1.553000\n",
      "Train Epoch: 5 [38000/49000 (78%)]\tLoss: 1.556761\n",
      "Train Epoch: 5 [39000/49000 (80%)]\tLoss: 1.564006\n",
      "Train Epoch: 5 [40000/49000 (82%)]\tLoss: 1.603047\n",
      "Train Epoch: 5 [41000/49000 (84%)]\tLoss: 1.618328\n",
      "Train Epoch: 5 [42000/49000 (86%)]\tLoss: 1.551405\n",
      "Train Epoch: 5 [43000/49000 (88%)]\tLoss: 1.621185\n",
      "Train Epoch: 5 [44000/49000 (90%)]\tLoss: 1.596263\n",
      "Train Epoch: 5 [45000/49000 (92%)]\tLoss: 1.563906\n",
      "Train Epoch: 5 [46000/49000 (94%)]\tLoss: 1.602895\n",
      "Train Epoch: 5 [47000/49000 (96%)]\tLoss: 1.566236\n",
      "Train Epoch: 5 [48000/49000 (98%)]\tLoss: 1.575112\n",
      "\n",
      "Test set: Avg. loss: 1.5768, Accuracy: 18814/21000 (89.59%)\n",
      "\n",
      "Train Epoch: 6 [0/49000 (0%)]\tLoss: 1.585734\n",
      "Train Epoch: 6 [1000/49000 (2%)]\tLoss: 1.591233\n",
      "Train Epoch: 6 [2000/49000 (4%)]\tLoss: 1.584025\n",
      "Train Epoch: 6 [3000/49000 (6%)]\tLoss: 1.554647\n",
      "Train Epoch: 6 [4000/49000 (8%)]\tLoss: 1.526847\n",
      "Train Epoch: 6 [5000/49000 (10%)]\tLoss: 1.553674\n",
      "Train Epoch: 6 [6000/49000 (12%)]\tLoss: 1.568795\n",
      "Train Epoch: 6 [7000/49000 (14%)]\tLoss: 1.561270\n",
      "Train Epoch: 6 [8000/49000 (16%)]\tLoss: 1.562987\n",
      "Train Epoch: 6 [9000/49000 (18%)]\tLoss: 1.538688\n",
      "Train Epoch: 6 [10000/49000 (20%)]\tLoss: 1.557989\n",
      "Train Epoch: 6 [11000/49000 (22%)]\tLoss: 1.570613\n",
      "Train Epoch: 6 [12000/49000 (24%)]\tLoss: 1.561423\n",
      "Train Epoch: 6 [13000/49000 (27%)]\tLoss: 1.577469\n",
      "Train Epoch: 6 [14000/49000 (29%)]\tLoss: 1.532981\n",
      "Train Epoch: 6 [15000/49000 (31%)]\tLoss: 1.545605\n",
      "Train Epoch: 6 [16000/49000 (33%)]\tLoss: 1.557833\n",
      "Train Epoch: 6 [17000/49000 (35%)]\tLoss: 1.612408\n",
      "Train Epoch: 6 [18000/49000 (37%)]\tLoss: 1.569276\n",
      "Train Epoch: 6 [19000/49000 (39%)]\tLoss: 1.579046\n",
      "Train Epoch: 6 [20000/49000 (41%)]\tLoss: 1.586465\n",
      "Train Epoch: 6 [21000/49000 (43%)]\tLoss: 1.551798\n",
      "Train Epoch: 6 [22000/49000 (45%)]\tLoss: 1.579663\n",
      "Train Epoch: 6 [23000/49000 (47%)]\tLoss: 1.576091\n",
      "Train Epoch: 6 [24000/49000 (49%)]\tLoss: 1.552996\n",
      "Train Epoch: 6 [25000/49000 (51%)]\tLoss: 1.576279\n",
      "Train Epoch: 6 [26000/49000 (53%)]\tLoss: 1.592283\n",
      "Train Epoch: 6 [27000/49000 (55%)]\tLoss: 1.565494\n",
      "Train Epoch: 6 [28000/49000 (57%)]\tLoss: 1.566997\n",
      "Train Epoch: 6 [29000/49000 (59%)]\tLoss: 1.581209\n",
      "Train Epoch: 6 [30000/49000 (61%)]\tLoss: 1.552139\n",
      "Train Epoch: 6 [31000/49000 (63%)]\tLoss: 1.581254\n",
      "Train Epoch: 6 [32000/49000 (65%)]\tLoss: 1.587675\n",
      "Train Epoch: 6 [33000/49000 (67%)]\tLoss: 1.560534\n",
      "Train Epoch: 6 [34000/49000 (69%)]\tLoss: 1.564125\n",
      "Train Epoch: 6 [35000/49000 (71%)]\tLoss: 1.601888\n",
      "Train Epoch: 6 [36000/49000 (73%)]\tLoss: 1.602620\n",
      "Train Epoch: 6 [37000/49000 (76%)]\tLoss: 1.604574\n",
      "Train Epoch: 6 [38000/49000 (78%)]\tLoss: 1.584548\n",
      "Train Epoch: 6 [39000/49000 (80%)]\tLoss: 1.554900\n",
      "Train Epoch: 6 [40000/49000 (82%)]\tLoss: 1.564287\n",
      "Train Epoch: 6 [41000/49000 (84%)]\tLoss: 1.563947\n",
      "Train Epoch: 6 [42000/49000 (86%)]\tLoss: 1.561994\n",
      "Train Epoch: 6 [43000/49000 (88%)]\tLoss: 1.548050\n",
      "Train Epoch: 6 [44000/49000 (90%)]\tLoss: 1.540040\n",
      "Train Epoch: 6 [45000/49000 (92%)]\tLoss: 1.548257\n",
      "Train Epoch: 6 [46000/49000 (94%)]\tLoss: 1.599085\n",
      "Train Epoch: 6 [47000/49000 (96%)]\tLoss: 1.544082\n",
      "Train Epoch: 6 [48000/49000 (98%)]\tLoss: 1.577878\n",
      "\n",
      "Test set: Avg. loss: 1.5716, Accuracy: 18901/21000 (90.00%)\n",
      "\n",
      "Train Epoch: 7 [0/49000 (0%)]\tLoss: 1.600880\n",
      "Train Epoch: 7 [1000/49000 (2%)]\tLoss: 1.568731\n",
      "Train Epoch: 7 [2000/49000 (4%)]\tLoss: 1.621047\n",
      "Train Epoch: 7 [3000/49000 (6%)]\tLoss: 1.575123\n",
      "Train Epoch: 7 [4000/49000 (8%)]\tLoss: 1.568641\n",
      "Train Epoch: 7 [5000/49000 (10%)]\tLoss: 1.546519\n",
      "Train Epoch: 7 [6000/49000 (12%)]\tLoss: 1.561288\n",
      "Train Epoch: 7 [7000/49000 (14%)]\tLoss: 1.535722\n",
      "Train Epoch: 7 [8000/49000 (16%)]\tLoss: 1.599434\n",
      "Train Epoch: 7 [9000/49000 (18%)]\tLoss: 1.546061\n",
      "Train Epoch: 7 [10000/49000 (20%)]\tLoss: 1.556283\n",
      "Train Epoch: 7 [11000/49000 (22%)]\tLoss: 1.537284\n",
      "Train Epoch: 7 [12000/49000 (24%)]\tLoss: 1.565823\n",
      "Train Epoch: 7 [13000/49000 (27%)]\tLoss: 1.596263\n",
      "Train Epoch: 7 [14000/49000 (29%)]\tLoss: 1.586110\n",
      "Train Epoch: 7 [15000/49000 (31%)]\tLoss: 1.576473\n",
      "Train Epoch: 7 [16000/49000 (33%)]\tLoss: 1.556555\n",
      "Train Epoch: 7 [17000/49000 (35%)]\tLoss: 1.534581\n",
      "Train Epoch: 7 [18000/49000 (37%)]\tLoss: 1.561293\n",
      "Train Epoch: 7 [19000/49000 (39%)]\tLoss: 1.584799\n",
      "Train Epoch: 7 [20000/49000 (41%)]\tLoss: 1.568702\n",
      "Train Epoch: 7 [21000/49000 (43%)]\tLoss: 1.561366\n",
      "Train Epoch: 7 [22000/49000 (45%)]\tLoss: 1.568406\n",
      "Train Epoch: 7 [23000/49000 (47%)]\tLoss: 1.559952\n",
      "Train Epoch: 7 [24000/49000 (49%)]\tLoss: 1.566199\n",
      "Train Epoch: 7 [25000/49000 (51%)]\tLoss: 1.547973\n",
      "Train Epoch: 7 [26000/49000 (53%)]\tLoss: 1.559800\n",
      "Train Epoch: 7 [27000/49000 (55%)]\tLoss: 1.580536\n",
      "Train Epoch: 7 [28000/49000 (57%)]\tLoss: 1.589868\n",
      "Train Epoch: 7 [29000/49000 (59%)]\tLoss: 1.571049\n",
      "Train Epoch: 7 [30000/49000 (61%)]\tLoss: 1.563472\n",
      "Train Epoch: 7 [31000/49000 (63%)]\tLoss: 1.595127\n",
      "Train Epoch: 7 [32000/49000 (65%)]\tLoss: 1.565446\n",
      "Train Epoch: 7 [33000/49000 (67%)]\tLoss: 1.572276\n",
      "Train Epoch: 7 [34000/49000 (69%)]\tLoss: 1.560942\n",
      "Train Epoch: 7 [35000/49000 (71%)]\tLoss: 1.571349\n",
      "Train Epoch: 7 [36000/49000 (73%)]\tLoss: 1.553492\n",
      "Train Epoch: 7 [37000/49000 (76%)]\tLoss: 1.576837\n",
      "Train Epoch: 7 [38000/49000 (78%)]\tLoss: 1.535095\n",
      "Train Epoch: 7 [39000/49000 (80%)]\tLoss: 1.560134\n",
      "Train Epoch: 7 [40000/49000 (82%)]\tLoss: 1.587936\n",
      "Train Epoch: 7 [41000/49000 (84%)]\tLoss: 1.541413\n",
      "Train Epoch: 7 [42000/49000 (86%)]\tLoss: 1.553002\n",
      "Train Epoch: 7 [43000/49000 (88%)]\tLoss: 1.585410\n",
      "Train Epoch: 7 [44000/49000 (90%)]\tLoss: 1.529938\n",
      "Train Epoch: 7 [45000/49000 (92%)]\tLoss: 1.539138\n",
      "Train Epoch: 7 [46000/49000 (94%)]\tLoss: 1.545858\n",
      "Train Epoch: 7 [47000/49000 (96%)]\tLoss: 1.561150\n",
      "Train Epoch: 7 [48000/49000 (98%)]\tLoss: 1.560149\n",
      "\n",
      "Test set: Avg. loss: 1.5705, Accuracy: 18895/21000 (89.98%)\n",
      "\n",
      "Train Epoch: 8 [0/49000 (0%)]\tLoss: 1.561901\n",
      "Train Epoch: 8 [1000/49000 (2%)]\tLoss: 1.610150\n",
      "Train Epoch: 8 [2000/49000 (4%)]\tLoss: 1.589800\n",
      "Train Epoch: 8 [3000/49000 (6%)]\tLoss: 1.572327\n",
      "Train Epoch: 8 [4000/49000 (8%)]\tLoss: 1.566721\n",
      "Train Epoch: 8 [5000/49000 (10%)]\tLoss: 1.588947\n",
      "Train Epoch: 8 [6000/49000 (12%)]\tLoss: 1.552740\n",
      "Train Epoch: 8 [7000/49000 (14%)]\tLoss: 1.607162\n",
      "Train Epoch: 8 [8000/49000 (16%)]\tLoss: 1.564316\n",
      "Train Epoch: 8 [9000/49000 (18%)]\tLoss: 1.569266\n",
      "Train Epoch: 8 [10000/49000 (20%)]\tLoss: 1.549035\n",
      "Train Epoch: 8 [11000/49000 (22%)]\tLoss: 1.539041\n",
      "Train Epoch: 8 [12000/49000 (24%)]\tLoss: 1.555808\n",
      "Train Epoch: 8 [13000/49000 (27%)]\tLoss: 1.596649\n",
      "Train Epoch: 8 [14000/49000 (29%)]\tLoss: 1.546625\n",
      "Train Epoch: 8 [15000/49000 (31%)]\tLoss: 1.600255\n",
      "Train Epoch: 8 [16000/49000 (33%)]\tLoss: 1.587910\n",
      "Train Epoch: 8 [17000/49000 (35%)]\tLoss: 1.586616\n",
      "Train Epoch: 8 [18000/49000 (37%)]\tLoss: 1.598995\n",
      "Train Epoch: 8 [19000/49000 (39%)]\tLoss: 1.555344\n",
      "Train Epoch: 8 [20000/49000 (41%)]\tLoss: 1.560418\n",
      "Train Epoch: 8 [21000/49000 (43%)]\tLoss: 1.568499\n",
      "Train Epoch: 8 [22000/49000 (45%)]\tLoss: 1.526637\n",
      "Train Epoch: 8 [23000/49000 (47%)]\tLoss: 1.598929\n",
      "Train Epoch: 8 [24000/49000 (49%)]\tLoss: 1.573124\n",
      "Train Epoch: 8 [25000/49000 (51%)]\tLoss: 1.553336\n",
      "Train Epoch: 8 [26000/49000 (53%)]\tLoss: 1.564441\n",
      "Train Epoch: 8 [27000/49000 (55%)]\tLoss: 1.574839\n",
      "Train Epoch: 8 [28000/49000 (57%)]\tLoss: 1.553948\n",
      "Train Epoch: 8 [29000/49000 (59%)]\tLoss: 1.560734\n",
      "Train Epoch: 8 [30000/49000 (61%)]\tLoss: 1.576250\n",
      "Train Epoch: 8 [31000/49000 (63%)]\tLoss: 1.585585\n",
      "Train Epoch: 8 [32000/49000 (65%)]\tLoss: 1.577693\n",
      "Train Epoch: 8 [33000/49000 (67%)]\tLoss: 1.550248\n",
      "Train Epoch: 8 [34000/49000 (69%)]\tLoss: 1.585570\n",
      "Train Epoch: 8 [35000/49000 (71%)]\tLoss: 1.562904\n",
      "Train Epoch: 8 [36000/49000 (73%)]\tLoss: 1.565977\n",
      "Train Epoch: 8 [37000/49000 (76%)]\tLoss: 1.533659\n",
      "Train Epoch: 8 [38000/49000 (78%)]\tLoss: 1.529626\n",
      "Train Epoch: 8 [39000/49000 (80%)]\tLoss: 1.546636\n",
      "Train Epoch: 8 [40000/49000 (82%)]\tLoss: 1.535663\n",
      "Train Epoch: 8 [41000/49000 (84%)]\tLoss: 1.560535\n",
      "Train Epoch: 8 [42000/49000 (86%)]\tLoss: 1.551529\n",
      "Train Epoch: 8 [43000/49000 (88%)]\tLoss: 1.590809\n",
      "Train Epoch: 8 [44000/49000 (90%)]\tLoss: 1.536621\n",
      "Train Epoch: 8 [45000/49000 (92%)]\tLoss: 1.579161\n",
      "Train Epoch: 8 [46000/49000 (94%)]\tLoss: 1.544756\n",
      "Train Epoch: 8 [47000/49000 (96%)]\tLoss: 1.572233\n",
      "Train Epoch: 8 [48000/49000 (98%)]\tLoss: 1.539214\n",
      "\n",
      "Test set: Avg. loss: 1.5694, Accuracy: 18902/21000 (90.01%)\n",
      "\n",
      "Train Epoch: 9 [0/49000 (0%)]\tLoss: 1.575478\n",
      "Train Epoch: 9 [1000/49000 (2%)]\tLoss: 1.583716\n",
      "Train Epoch: 9 [2000/49000 (4%)]\tLoss: 1.574358\n",
      "Train Epoch: 9 [3000/49000 (6%)]\tLoss: 1.580358\n",
      "Train Epoch: 9 [4000/49000 (8%)]\tLoss: 1.543544\n",
      "Train Epoch: 9 [5000/49000 (10%)]\tLoss: 1.582552\n",
      "Train Epoch: 9 [6000/49000 (12%)]\tLoss: 1.553033\n",
      "Train Epoch: 9 [7000/49000 (14%)]\tLoss: 1.551429\n",
      "Train Epoch: 9 [8000/49000 (16%)]\tLoss: 1.580897\n",
      "Train Epoch: 9 [9000/49000 (18%)]\tLoss: 1.622097\n",
      "Train Epoch: 9 [10000/49000 (20%)]\tLoss: 1.586732\n",
      "Train Epoch: 9 [11000/49000 (22%)]\tLoss: 1.532514\n",
      "Train Epoch: 9 [12000/49000 (24%)]\tLoss: 1.567758\n",
      "Train Epoch: 9 [13000/49000 (27%)]\tLoss: 1.554894\n",
      "Train Epoch: 9 [14000/49000 (29%)]\tLoss: 1.565432\n",
      "Train Epoch: 9 [15000/49000 (31%)]\tLoss: 1.549444\n",
      "Train Epoch: 9 [16000/49000 (33%)]\tLoss: 1.567036\n",
      "Train Epoch: 9 [17000/49000 (35%)]\tLoss: 1.546889\n",
      "Train Epoch: 9 [18000/49000 (37%)]\tLoss: 1.562922\n",
      "Train Epoch: 9 [19000/49000 (39%)]\tLoss: 1.561291\n",
      "Train Epoch: 9 [20000/49000 (41%)]\tLoss: 1.581562\n",
      "Train Epoch: 9 [21000/49000 (43%)]\tLoss: 1.525127\n",
      "Train Epoch: 9 [22000/49000 (45%)]\tLoss: 1.612596\n",
      "Train Epoch: 9 [23000/49000 (47%)]\tLoss: 1.558279\n",
      "Train Epoch: 9 [24000/49000 (49%)]\tLoss: 1.573459\n",
      "Train Epoch: 9 [25000/49000 (51%)]\tLoss: 1.560392\n",
      "Train Epoch: 9 [26000/49000 (53%)]\tLoss: 1.561763\n",
      "Train Epoch: 9 [27000/49000 (55%)]\tLoss: 1.567068\n",
      "Train Epoch: 9 [28000/49000 (57%)]\tLoss: 1.598502\n",
      "Train Epoch: 9 [29000/49000 (59%)]\tLoss: 1.579162\n",
      "Train Epoch: 9 [30000/49000 (61%)]\tLoss: 1.586281\n",
      "Train Epoch: 9 [31000/49000 (63%)]\tLoss: 1.556342\n",
      "Train Epoch: 9 [32000/49000 (65%)]\tLoss: 1.556449\n",
      "Train Epoch: 9 [33000/49000 (67%)]\tLoss: 1.548524\n",
      "Train Epoch: 9 [34000/49000 (69%)]\tLoss: 1.558187\n",
      "Train Epoch: 9 [35000/49000 (71%)]\tLoss: 1.570717\n",
      "Train Epoch: 9 [36000/49000 (73%)]\tLoss: 1.542743\n",
      "Train Epoch: 9 [37000/49000 (76%)]\tLoss: 1.569300\n",
      "Train Epoch: 9 [38000/49000 (78%)]\tLoss: 1.559073\n",
      "Train Epoch: 9 [39000/49000 (80%)]\tLoss: 1.560064\n",
      "Train Epoch: 9 [40000/49000 (82%)]\tLoss: 1.561781\n",
      "Train Epoch: 9 [41000/49000 (84%)]\tLoss: 1.554122\n",
      "Train Epoch: 9 [42000/49000 (86%)]\tLoss: 1.533327\n",
      "Train Epoch: 9 [43000/49000 (88%)]\tLoss: 1.589375\n",
      "Train Epoch: 9 [44000/49000 (90%)]\tLoss: 1.589594\n",
      "Train Epoch: 9 [45000/49000 (92%)]\tLoss: 1.575755\n",
      "Train Epoch: 9 [46000/49000 (94%)]\tLoss: 1.615701\n",
      "Train Epoch: 9 [47000/49000 (96%)]\tLoss: 1.537476\n",
      "Train Epoch: 9 [48000/49000 (98%)]\tLoss: 1.570351\n",
      "\n",
      "Test set: Avg. loss: 1.5659, Accuracy: 18936/21000 (90.17%)\n",
      "\n",
      "Train Epoch: 10 [0/49000 (0%)]\tLoss: 1.517070\n",
      "Train Epoch: 10 [1000/49000 (2%)]\tLoss: 1.543827\n",
      "Train Epoch: 10 [2000/49000 (4%)]\tLoss: 1.569235\n",
      "Train Epoch: 10 [3000/49000 (6%)]\tLoss: 1.543557\n",
      "Train Epoch: 10 [4000/49000 (8%)]\tLoss: 1.548899\n",
      "Train Epoch: 10 [5000/49000 (10%)]\tLoss: 1.556008\n",
      "Train Epoch: 10 [6000/49000 (12%)]\tLoss: 1.532578\n",
      "Train Epoch: 10 [7000/49000 (14%)]\tLoss: 1.563202\n",
      "Train Epoch: 10 [8000/49000 (16%)]\tLoss: 1.576887\n",
      "Train Epoch: 10 [9000/49000 (18%)]\tLoss: 1.568620\n",
      "Train Epoch: 10 [10000/49000 (20%)]\tLoss: 1.562211\n",
      "Train Epoch: 10 [11000/49000 (22%)]\tLoss: 1.575415\n",
      "Train Epoch: 10 [12000/49000 (24%)]\tLoss: 1.557323\n",
      "Train Epoch: 10 [13000/49000 (27%)]\tLoss: 1.580512\n",
      "Train Epoch: 10 [14000/49000 (29%)]\tLoss: 1.551545\n",
      "Train Epoch: 10 [15000/49000 (31%)]\tLoss: 1.535922\n",
      "Train Epoch: 10 [16000/49000 (33%)]\tLoss: 1.558753\n",
      "Train Epoch: 10 [17000/49000 (35%)]\tLoss: 1.542427\n",
      "Train Epoch: 10 [18000/49000 (37%)]\tLoss: 1.582583\n",
      "Train Epoch: 10 [19000/49000 (39%)]\tLoss: 1.575395\n",
      "Train Epoch: 10 [20000/49000 (41%)]\tLoss: 1.551356\n",
      "Train Epoch: 10 [21000/49000 (43%)]\tLoss: 1.563660\n",
      "Train Epoch: 10 [22000/49000 (45%)]\tLoss: 1.550182\n",
      "Train Epoch: 10 [23000/49000 (47%)]\tLoss: 1.590176\n",
      "Train Epoch: 10 [24000/49000 (49%)]\tLoss: 1.578173\n",
      "Train Epoch: 10 [25000/49000 (51%)]\tLoss: 1.558426\n",
      "Train Epoch: 10 [26000/49000 (53%)]\tLoss: 1.534773\n",
      "Train Epoch: 10 [27000/49000 (55%)]\tLoss: 1.549127\n",
      "Train Epoch: 10 [28000/49000 (57%)]\tLoss: 1.590245\n",
      "Train Epoch: 10 [29000/49000 (59%)]\tLoss: 1.555480\n",
      "Train Epoch: 10 [30000/49000 (61%)]\tLoss: 1.570284\n",
      "Train Epoch: 10 [31000/49000 (63%)]\tLoss: 1.582691\n",
      "Train Epoch: 10 [32000/49000 (65%)]\tLoss: 1.562922\n",
      "Train Epoch: 10 [33000/49000 (67%)]\tLoss: 1.574260\n",
      "Train Epoch: 10 [34000/49000 (69%)]\tLoss: 1.566641\n",
      "Train Epoch: 10 [35000/49000 (71%)]\tLoss: 1.589269\n",
      "Train Epoch: 10 [36000/49000 (73%)]\tLoss: 1.511850\n",
      "Train Epoch: 10 [37000/49000 (76%)]\tLoss: 1.572531\n",
      "Train Epoch: 10 [38000/49000 (78%)]\tLoss: 1.565371\n",
      "Train Epoch: 10 [39000/49000 (80%)]\tLoss: 1.589596\n",
      "Train Epoch: 10 [40000/49000 (82%)]\tLoss: 1.591562\n",
      "Train Epoch: 10 [41000/49000 (84%)]\tLoss: 1.569478\n",
      "Train Epoch: 10 [42000/49000 (86%)]\tLoss: 1.590541\n",
      "Train Epoch: 10 [43000/49000 (88%)]\tLoss: 1.531614\n",
      "Train Epoch: 10 [44000/49000 (90%)]\tLoss: 1.538142\n",
      "Train Epoch: 10 [45000/49000 (92%)]\tLoss: 1.565360\n",
      "Train Epoch: 10 [46000/49000 (94%)]\tLoss: 1.558983\n",
      "Train Epoch: 10 [47000/49000 (96%)]\tLoss: 1.572139\n",
      "Train Epoch: 10 [48000/49000 (98%)]\tLoss: 1.559925\n",
      "\n",
      "Test set: Avg. loss: 1.5648, Accuracy: 18928/21000 (90.13%)\n",
      "\n",
      "Train Epoch: 11 [0/49000 (0%)]\tLoss: 1.581603\n",
      "Train Epoch: 11 [1000/49000 (2%)]\tLoss: 1.547541\n",
      "Train Epoch: 11 [2000/49000 (4%)]\tLoss: 1.593734\n",
      "Train Epoch: 11 [3000/49000 (6%)]\tLoss: 1.533850\n",
      "Train Epoch: 11 [4000/49000 (8%)]\tLoss: 1.542632\n",
      "Train Epoch: 11 [5000/49000 (10%)]\tLoss: 1.573438\n",
      "Train Epoch: 11 [6000/49000 (12%)]\tLoss: 1.544442\n",
      "Train Epoch: 11 [7000/49000 (14%)]\tLoss: 1.597246\n",
      "Train Epoch: 11 [8000/49000 (16%)]\tLoss: 1.529411\n",
      "Train Epoch: 11 [9000/49000 (18%)]\tLoss: 1.549338\n",
      "Train Epoch: 11 [10000/49000 (20%)]\tLoss: 1.542124\n",
      "Train Epoch: 11 [11000/49000 (22%)]\tLoss: 1.547188\n",
      "Train Epoch: 11 [12000/49000 (24%)]\tLoss: 1.562612\n",
      "Train Epoch: 11 [13000/49000 (27%)]\tLoss: 1.546424\n",
      "Train Epoch: 11 [14000/49000 (29%)]\tLoss: 1.530625\n",
      "Train Epoch: 11 [15000/49000 (31%)]\tLoss: 1.523226\n",
      "Train Epoch: 11 [16000/49000 (33%)]\tLoss: 1.596510\n",
      "Train Epoch: 11 [17000/49000 (35%)]\tLoss: 1.548411\n",
      "Train Epoch: 11 [18000/49000 (37%)]\tLoss: 1.590554\n",
      "Train Epoch: 11 [19000/49000 (39%)]\tLoss: 1.521724\n",
      "Train Epoch: 11 [20000/49000 (41%)]\tLoss: 1.514678\n",
      "Train Epoch: 11 [21000/49000 (43%)]\tLoss: 1.510597\n",
      "Train Epoch: 11 [22000/49000 (45%)]\tLoss: 1.548345\n",
      "Train Epoch: 11 [23000/49000 (47%)]\tLoss: 1.595847\n",
      "Train Epoch: 11 [24000/49000 (49%)]\tLoss: 1.545671\n",
      "Train Epoch: 11 [25000/49000 (51%)]\tLoss: 1.549459\n",
      "Train Epoch: 11 [26000/49000 (53%)]\tLoss: 1.571360\n",
      "Train Epoch: 11 [27000/49000 (55%)]\tLoss: 1.538268\n",
      "Train Epoch: 11 [28000/49000 (57%)]\tLoss: 1.564307\n",
      "Train Epoch: 11 [29000/49000 (59%)]\tLoss: 1.542010\n",
      "Train Epoch: 11 [30000/49000 (61%)]\tLoss: 1.610101\n",
      "Train Epoch: 11 [31000/49000 (63%)]\tLoss: 1.532162\n",
      "Train Epoch: 11 [32000/49000 (65%)]\tLoss: 1.561376\n",
      "Train Epoch: 11 [33000/49000 (67%)]\tLoss: 1.548140\n",
      "Train Epoch: 11 [34000/49000 (69%)]\tLoss: 1.566141\n",
      "Train Epoch: 11 [35000/49000 (71%)]\tLoss: 1.579542\n",
      "Train Epoch: 11 [36000/49000 (73%)]\tLoss: 1.574910\n",
      "Train Epoch: 11 [37000/49000 (76%)]\tLoss: 1.524148\n",
      "Train Epoch: 11 [38000/49000 (78%)]\tLoss: 1.567926\n",
      "Train Epoch: 11 [39000/49000 (80%)]\tLoss: 1.570466\n",
      "Train Epoch: 11 [40000/49000 (82%)]\tLoss: 1.518755\n",
      "Train Epoch: 11 [41000/49000 (84%)]\tLoss: 1.578160\n",
      "Train Epoch: 11 [42000/49000 (86%)]\tLoss: 1.556832\n",
      "Train Epoch: 11 [43000/49000 (88%)]\tLoss: 1.552012\n",
      "Train Epoch: 11 [44000/49000 (90%)]\tLoss: 1.567886\n",
      "Train Epoch: 11 [45000/49000 (92%)]\tLoss: 1.551810\n",
      "Train Epoch: 11 [46000/49000 (94%)]\tLoss: 1.595489\n",
      "Train Epoch: 11 [47000/49000 (96%)]\tLoss: 1.581959\n",
      "Train Epoch: 11 [48000/49000 (98%)]\tLoss: 1.557383\n",
      "\n",
      "Test set: Avg. loss: 1.5644, Accuracy: 18921/21000 (90.10%)\n",
      "\n",
      "Train Epoch: 12 [0/49000 (0%)]\tLoss: 1.554000\n",
      "Train Epoch: 12 [1000/49000 (2%)]\tLoss: 1.556296\n",
      "Train Epoch: 12 [2000/49000 (4%)]\tLoss: 1.546458\n",
      "Train Epoch: 12 [3000/49000 (6%)]\tLoss: 1.575238\n",
      "Train Epoch: 12 [4000/49000 (8%)]\tLoss: 1.579762\n",
      "Train Epoch: 12 [5000/49000 (10%)]\tLoss: 1.526085\n",
      "Train Epoch: 12 [6000/49000 (12%)]\tLoss: 1.582062\n",
      "Train Epoch: 12 [7000/49000 (14%)]\tLoss: 1.560594\n",
      "Train Epoch: 12 [8000/49000 (16%)]\tLoss: 1.558667\n",
      "Train Epoch: 12 [9000/49000 (18%)]\tLoss: 1.554375\n",
      "Train Epoch: 12 [10000/49000 (20%)]\tLoss: 1.528839\n",
      "Train Epoch: 12 [11000/49000 (22%)]\tLoss: 1.572878\n",
      "Train Epoch: 12 [12000/49000 (24%)]\tLoss: 1.549322\n",
      "Train Epoch: 12 [13000/49000 (27%)]\tLoss: 1.554788\n",
      "Train Epoch: 12 [14000/49000 (29%)]\tLoss: 1.566551\n",
      "Train Epoch: 12 [15000/49000 (31%)]\tLoss: 1.556172\n",
      "Train Epoch: 12 [16000/49000 (33%)]\tLoss: 1.546389\n",
      "Train Epoch: 12 [17000/49000 (35%)]\tLoss: 1.569972\n",
      "Train Epoch: 12 [18000/49000 (37%)]\tLoss: 1.580175\n",
      "Train Epoch: 12 [19000/49000 (39%)]\tLoss: 1.543553\n",
      "Train Epoch: 12 [20000/49000 (41%)]\tLoss: 1.549462\n",
      "Train Epoch: 12 [21000/49000 (43%)]\tLoss: 1.560224\n",
      "Train Epoch: 12 [22000/49000 (45%)]\tLoss: 1.534976\n",
      "Train Epoch: 12 [23000/49000 (47%)]\tLoss: 1.531506\n",
      "Train Epoch: 12 [24000/49000 (49%)]\tLoss: 1.586592\n",
      "Train Epoch: 12 [25000/49000 (51%)]\tLoss: 1.540771\n",
      "Train Epoch: 12 [26000/49000 (53%)]\tLoss: 1.553235\n",
      "Train Epoch: 12 [27000/49000 (55%)]\tLoss: 1.570390\n",
      "Train Epoch: 12 [28000/49000 (57%)]\tLoss: 1.565915\n",
      "Train Epoch: 12 [29000/49000 (59%)]\tLoss: 1.584669\n",
      "Train Epoch: 12 [30000/49000 (61%)]\tLoss: 1.549105\n",
      "Train Epoch: 12 [31000/49000 (63%)]\tLoss: 1.564211\n",
      "Train Epoch: 12 [32000/49000 (65%)]\tLoss: 1.573265\n",
      "Train Epoch: 12 [33000/49000 (67%)]\tLoss: 1.532236\n",
      "Train Epoch: 12 [34000/49000 (69%)]\tLoss: 1.561880\n",
      "Train Epoch: 12 [35000/49000 (71%)]\tLoss: 1.592993\n",
      "Train Epoch: 12 [36000/49000 (73%)]\tLoss: 1.536147\n",
      "Train Epoch: 12 [37000/49000 (76%)]\tLoss: 1.579981\n",
      "Train Epoch: 12 [38000/49000 (78%)]\tLoss: 1.566704\n",
      "Train Epoch: 12 [39000/49000 (80%)]\tLoss: 1.559365\n",
      "Train Epoch: 12 [40000/49000 (82%)]\tLoss: 1.530428\n",
      "Train Epoch: 12 [41000/49000 (84%)]\tLoss: 1.584371\n",
      "Train Epoch: 12 [42000/49000 (86%)]\tLoss: 1.599440\n",
      "Train Epoch: 12 [43000/49000 (88%)]\tLoss: 1.599682\n",
      "Train Epoch: 12 [44000/49000 (90%)]\tLoss: 1.532051\n",
      "Train Epoch: 12 [45000/49000 (92%)]\tLoss: 1.536036\n",
      "Train Epoch: 12 [46000/49000 (94%)]\tLoss: 1.628087\n",
      "Train Epoch: 12 [47000/49000 (96%)]\tLoss: 1.537948\n",
      "Train Epoch: 12 [48000/49000 (98%)]\tLoss: 1.563918\n",
      "\n",
      "Test set: Avg. loss: 1.5630, Accuracy: 18937/21000 (90.18%)\n",
      "\n",
      "Train Epoch: 13 [0/49000 (0%)]\tLoss: 1.555724\n",
      "Train Epoch: 13 [1000/49000 (2%)]\tLoss: 1.560390\n",
      "Train Epoch: 13 [2000/49000 (4%)]\tLoss: 1.575507\n",
      "Train Epoch: 13 [3000/49000 (6%)]\tLoss: 1.608439\n",
      "Train Epoch: 13 [4000/49000 (8%)]\tLoss: 1.560409\n",
      "Train Epoch: 13 [5000/49000 (10%)]\tLoss: 1.530613\n",
      "Train Epoch: 13 [6000/49000 (12%)]\tLoss: 1.554077\n",
      "Train Epoch: 13 [7000/49000 (14%)]\tLoss: 1.562423\n",
      "Train Epoch: 13 [8000/49000 (16%)]\tLoss: 1.552290\n",
      "Train Epoch: 13 [9000/49000 (18%)]\tLoss: 1.572038\n",
      "Train Epoch: 13 [10000/49000 (20%)]\tLoss: 1.581415\n",
      "Train Epoch: 13 [11000/49000 (22%)]\tLoss: 1.530143\n",
      "Train Epoch: 13 [12000/49000 (24%)]\tLoss: 1.546058\n",
      "Train Epoch: 13 [13000/49000 (27%)]\tLoss: 1.552643\n",
      "Train Epoch: 13 [14000/49000 (29%)]\tLoss: 1.568943\n",
      "Train Epoch: 13 [15000/49000 (31%)]\tLoss: 1.548925\n",
      "Train Epoch: 13 [16000/49000 (33%)]\tLoss: 1.555519\n",
      "Train Epoch: 13 [17000/49000 (35%)]\tLoss: 1.513235\n",
      "Train Epoch: 13 [18000/49000 (37%)]\tLoss: 1.593536\n",
      "Train Epoch: 13 [19000/49000 (39%)]\tLoss: 1.529966\n",
      "Train Epoch: 13 [20000/49000 (41%)]\tLoss: 1.577836\n",
      "Train Epoch: 13 [21000/49000 (43%)]\tLoss: 1.535095\n",
      "Train Epoch: 13 [22000/49000 (45%)]\tLoss: 1.541347\n",
      "Train Epoch: 13 [23000/49000 (47%)]\tLoss: 1.557147\n",
      "Train Epoch: 13 [24000/49000 (49%)]\tLoss: 1.569801\n",
      "Train Epoch: 13 [25000/49000 (51%)]\tLoss: 1.552751\n",
      "Train Epoch: 13 [26000/49000 (53%)]\tLoss: 1.562220\n",
      "Train Epoch: 13 [27000/49000 (55%)]\tLoss: 1.548506\n",
      "Train Epoch: 13 [28000/49000 (57%)]\tLoss: 1.560335\n",
      "Train Epoch: 13 [29000/49000 (59%)]\tLoss: 1.597805\n",
      "Train Epoch: 13 [30000/49000 (61%)]\tLoss: 1.546886\n",
      "Train Epoch: 13 [31000/49000 (63%)]\tLoss: 1.548790\n",
      "Train Epoch: 13 [32000/49000 (65%)]\tLoss: 1.540338\n",
      "Train Epoch: 13 [33000/49000 (67%)]\tLoss: 1.597594\n",
      "Train Epoch: 13 [34000/49000 (69%)]\tLoss: 1.557532\n",
      "Train Epoch: 13 [35000/49000 (71%)]\tLoss: 1.515204\n",
      "Train Epoch: 13 [36000/49000 (73%)]\tLoss: 1.535055\n",
      "Train Epoch: 13 [37000/49000 (76%)]\tLoss: 1.549527\n",
      "Train Epoch: 13 [38000/49000 (78%)]\tLoss: 1.563707\n",
      "Train Epoch: 13 [39000/49000 (80%)]\tLoss: 1.593627\n",
      "Train Epoch: 13 [40000/49000 (82%)]\tLoss: 1.554907\n",
      "Train Epoch: 13 [41000/49000 (84%)]\tLoss: 1.550433\n",
      "Train Epoch: 13 [42000/49000 (86%)]\tLoss: 1.605210\n",
      "Train Epoch: 13 [43000/49000 (88%)]\tLoss: 1.554552\n",
      "Train Epoch: 13 [44000/49000 (90%)]\tLoss: 1.585493\n",
      "Train Epoch: 13 [45000/49000 (92%)]\tLoss: 1.594016\n",
      "Train Epoch: 13 [46000/49000 (94%)]\tLoss: 1.533735\n",
      "Train Epoch: 13 [47000/49000 (96%)]\tLoss: 1.538597\n",
      "Train Epoch: 13 [48000/49000 (98%)]\tLoss: 1.576697\n",
      "\n",
      "Test set: Avg. loss: 1.5615, Accuracy: 18980/21000 (90.38%)\n",
      "\n",
      "Train Epoch: 14 [0/49000 (0%)]\tLoss: 1.571244\n",
      "Train Epoch: 14 [1000/49000 (2%)]\tLoss: 1.547233\n",
      "Train Epoch: 14 [2000/49000 (4%)]\tLoss: 1.537150\n",
      "Train Epoch: 14 [3000/49000 (6%)]\tLoss: 1.553091\n",
      "Train Epoch: 14 [4000/49000 (8%)]\tLoss: 1.526907\n",
      "Train Epoch: 14 [5000/49000 (10%)]\tLoss: 1.537837\n",
      "Train Epoch: 14 [6000/49000 (12%)]\tLoss: 1.582361\n",
      "Train Epoch: 14 [7000/49000 (14%)]\tLoss: 1.605339\n",
      "Train Epoch: 14 [8000/49000 (16%)]\tLoss: 1.534438\n",
      "Train Epoch: 14 [9000/49000 (18%)]\tLoss: 1.574628\n",
      "Train Epoch: 14 [10000/49000 (20%)]\tLoss: 1.561057\n",
      "Train Epoch: 14 [11000/49000 (22%)]\tLoss: 1.525871\n",
      "Train Epoch: 14 [12000/49000 (24%)]\tLoss: 1.567209\n",
      "Train Epoch: 14 [13000/49000 (27%)]\tLoss: 1.518963\n",
      "Train Epoch: 14 [14000/49000 (29%)]\tLoss: 1.573216\n",
      "Train Epoch: 14 [15000/49000 (31%)]\tLoss: 1.541012\n",
      "Train Epoch: 14 [16000/49000 (33%)]\tLoss: 1.568051\n",
      "Train Epoch: 14 [17000/49000 (35%)]\tLoss: 1.542622\n",
      "Train Epoch: 14 [18000/49000 (37%)]\tLoss: 1.536690\n",
      "Train Epoch: 14 [19000/49000 (39%)]\tLoss: 1.541821\n",
      "Train Epoch: 14 [20000/49000 (41%)]\tLoss: 1.557106\n",
      "Train Epoch: 14 [21000/49000 (43%)]\tLoss: 1.521304\n",
      "Train Epoch: 14 [22000/49000 (45%)]\tLoss: 1.619449\n",
      "Train Epoch: 14 [23000/49000 (47%)]\tLoss: 1.594785\n",
      "Train Epoch: 14 [24000/49000 (49%)]\tLoss: 1.554564\n",
      "Train Epoch: 14 [25000/49000 (51%)]\tLoss: 1.551010\n",
      "Train Epoch: 14 [26000/49000 (53%)]\tLoss: 1.592257\n",
      "Train Epoch: 14 [27000/49000 (55%)]\tLoss: 1.551694\n",
      "Train Epoch: 14 [28000/49000 (57%)]\tLoss: 1.546049\n",
      "Train Epoch: 14 [29000/49000 (59%)]\tLoss: 1.608020\n",
      "Train Epoch: 14 [30000/49000 (61%)]\tLoss: 1.544499\n",
      "Train Epoch: 14 [31000/49000 (63%)]\tLoss: 1.539494\n",
      "Train Epoch: 14 [32000/49000 (65%)]\tLoss: 1.548897\n",
      "Train Epoch: 14 [33000/49000 (67%)]\tLoss: 1.547478\n",
      "Train Epoch: 14 [34000/49000 (69%)]\tLoss: 1.597439\n",
      "Train Epoch: 14 [35000/49000 (71%)]\tLoss: 1.548045\n",
      "Train Epoch: 14 [36000/49000 (73%)]\tLoss: 1.531249\n",
      "Train Epoch: 14 [37000/49000 (76%)]\tLoss: 1.559233\n",
      "Train Epoch: 14 [38000/49000 (78%)]\tLoss: 1.537664\n",
      "Train Epoch: 14 [39000/49000 (80%)]\tLoss: 1.530807\n",
      "Train Epoch: 14 [40000/49000 (82%)]\tLoss: 1.559917\n",
      "Train Epoch: 14 [41000/49000 (84%)]\tLoss: 1.540194\n",
      "Train Epoch: 14 [42000/49000 (86%)]\tLoss: 1.539518\n",
      "Train Epoch: 14 [43000/49000 (88%)]\tLoss: 1.559916\n",
      "Train Epoch: 14 [44000/49000 (90%)]\tLoss: 1.564629\n",
      "Train Epoch: 14 [45000/49000 (92%)]\tLoss: 1.532502\n",
      "Train Epoch: 14 [46000/49000 (94%)]\tLoss: 1.525175\n",
      "Train Epoch: 14 [47000/49000 (96%)]\tLoss: 1.601711\n",
      "Train Epoch: 14 [48000/49000 (98%)]\tLoss: 1.554638\n",
      "\n",
      "Test set: Avg. loss: 1.5629, Accuracy: 18929/21000 (90.14%)\n",
      "\n",
      "Train Epoch: 15 [0/49000 (0%)]\tLoss: 1.579926\n",
      "Train Epoch: 15 [1000/49000 (2%)]\tLoss: 1.557417\n",
      "Train Epoch: 15 [2000/49000 (4%)]\tLoss: 1.574698\n",
      "Train Epoch: 15 [3000/49000 (6%)]\tLoss: 1.576722\n",
      "Train Epoch: 15 [4000/49000 (8%)]\tLoss: 1.554428\n",
      "Train Epoch: 15 [5000/49000 (10%)]\tLoss: 1.547549\n",
      "Train Epoch: 15 [6000/49000 (12%)]\tLoss: 1.562845\n",
      "Train Epoch: 15 [7000/49000 (14%)]\tLoss: 1.576934\n",
      "Train Epoch: 15 [8000/49000 (16%)]\tLoss: 1.549959\n",
      "Train Epoch: 15 [9000/49000 (18%)]\tLoss: 1.583782\n",
      "Train Epoch: 15 [10000/49000 (20%)]\tLoss: 1.601967\n",
      "Train Epoch: 15 [11000/49000 (22%)]\tLoss: 1.543976\n",
      "Train Epoch: 15 [12000/49000 (24%)]\tLoss: 1.565841\n",
      "Train Epoch: 15 [13000/49000 (27%)]\tLoss: 1.561949\n",
      "Train Epoch: 15 [14000/49000 (29%)]\tLoss: 1.558514\n",
      "Train Epoch: 15 [15000/49000 (31%)]\tLoss: 1.530587\n",
      "Train Epoch: 15 [16000/49000 (33%)]\tLoss: 1.540997\n",
      "Train Epoch: 15 [17000/49000 (35%)]\tLoss: 1.575139\n",
      "Train Epoch: 15 [18000/49000 (37%)]\tLoss: 1.522439\n",
      "Train Epoch: 15 [19000/49000 (39%)]\tLoss: 1.595596\n",
      "Train Epoch: 15 [20000/49000 (41%)]\tLoss: 1.516676\n",
      "Train Epoch: 15 [21000/49000 (43%)]\tLoss: 1.577671\n",
      "Train Epoch: 15 [22000/49000 (45%)]\tLoss: 1.584781\n",
      "Train Epoch: 15 [23000/49000 (47%)]\tLoss: 1.583394\n",
      "Train Epoch: 15 [24000/49000 (49%)]\tLoss: 1.564421\n",
      "Train Epoch: 15 [25000/49000 (51%)]\tLoss: 1.542950\n",
      "Train Epoch: 15 [26000/49000 (53%)]\tLoss: 1.575720\n",
      "Train Epoch: 15 [27000/49000 (55%)]\tLoss: 1.568881\n",
      "Train Epoch: 15 [28000/49000 (57%)]\tLoss: 1.529550\n",
      "Train Epoch: 15 [29000/49000 (59%)]\tLoss: 1.577488\n",
      "Train Epoch: 15 [30000/49000 (61%)]\tLoss: 1.556817\n",
      "Train Epoch: 15 [31000/49000 (63%)]\tLoss: 1.539673\n",
      "Train Epoch: 15 [32000/49000 (65%)]\tLoss: 1.526965\n",
      "Train Epoch: 15 [33000/49000 (67%)]\tLoss: 1.548828\n",
      "Train Epoch: 15 [34000/49000 (69%)]\tLoss: 1.551300\n",
      "Train Epoch: 15 [35000/49000 (71%)]\tLoss: 1.518851\n",
      "Train Epoch: 15 [36000/49000 (73%)]\tLoss: 1.544150\n",
      "Train Epoch: 15 [37000/49000 (76%)]\tLoss: 1.562553\n",
      "Train Epoch: 15 [38000/49000 (78%)]\tLoss: 1.561923\n",
      "Train Epoch: 15 [39000/49000 (80%)]\tLoss: 1.558418\n",
      "Train Epoch: 15 [40000/49000 (82%)]\tLoss: 1.517583\n",
      "Train Epoch: 15 [41000/49000 (84%)]\tLoss: 1.581879\n",
      "Train Epoch: 15 [42000/49000 (86%)]\tLoss: 1.549263\n",
      "Train Epoch: 15 [43000/49000 (88%)]\tLoss: 1.550190\n",
      "Train Epoch: 15 [44000/49000 (90%)]\tLoss: 1.536518\n",
      "Train Epoch: 15 [45000/49000 (92%)]\tLoss: 1.585413\n",
      "Train Epoch: 15 [46000/49000 (94%)]\tLoss: 1.547121\n",
      "Train Epoch: 15 [47000/49000 (96%)]\tLoss: 1.560189\n",
      "Train Epoch: 15 [48000/49000 (98%)]\tLoss: 1.538336\n",
      "\n",
      "Test set: Avg. loss: 1.5607, Accuracy: 18993/21000 (90.44%)\n",
      "\n",
      "Train Epoch: 16 [0/49000 (0%)]\tLoss: 1.555160\n",
      "Train Epoch: 16 [1000/49000 (2%)]\tLoss: 1.565210\n",
      "Train Epoch: 16 [2000/49000 (4%)]\tLoss: 1.566409\n",
      "Train Epoch: 16 [3000/49000 (6%)]\tLoss: 1.570871\n",
      "Train Epoch: 16 [4000/49000 (8%)]\tLoss: 1.589671\n",
      "Train Epoch: 16 [5000/49000 (10%)]\tLoss: 1.610613\n",
      "Train Epoch: 16 [6000/49000 (12%)]\tLoss: 1.529031\n",
      "Train Epoch: 16 [7000/49000 (14%)]\tLoss: 1.595869\n",
      "Train Epoch: 16 [8000/49000 (16%)]\tLoss: 1.594331\n",
      "Train Epoch: 16 [9000/49000 (18%)]\tLoss: 1.558730\n",
      "Train Epoch: 16 [10000/49000 (20%)]\tLoss: 1.557791\n",
      "Train Epoch: 16 [11000/49000 (22%)]\tLoss: 1.550973\n",
      "Train Epoch: 16 [12000/49000 (24%)]\tLoss: 1.578490\n",
      "Train Epoch: 16 [13000/49000 (27%)]\tLoss: 1.551526\n",
      "Train Epoch: 16 [14000/49000 (29%)]\tLoss: 1.554793\n",
      "Train Epoch: 16 [15000/49000 (31%)]\tLoss: 1.556358\n",
      "Train Epoch: 16 [16000/49000 (33%)]\tLoss: 1.561079\n",
      "Train Epoch: 16 [17000/49000 (35%)]\tLoss: 1.546611\n",
      "Train Epoch: 16 [18000/49000 (37%)]\tLoss: 1.546255\n",
      "Train Epoch: 16 [19000/49000 (39%)]\tLoss: 1.574850\n",
      "Train Epoch: 16 [20000/49000 (41%)]\tLoss: 1.569447\n",
      "Train Epoch: 16 [21000/49000 (43%)]\tLoss: 1.589842\n",
      "Train Epoch: 16 [22000/49000 (45%)]\tLoss: 1.524624\n",
      "Train Epoch: 16 [23000/49000 (47%)]\tLoss: 1.561882\n",
      "Train Epoch: 16 [24000/49000 (49%)]\tLoss: 1.561704\n",
      "Train Epoch: 16 [25000/49000 (51%)]\tLoss: 1.559890\n",
      "Train Epoch: 16 [26000/49000 (53%)]\tLoss: 1.532346\n",
      "Train Epoch: 16 [27000/49000 (55%)]\tLoss: 1.555305\n",
      "Train Epoch: 16 [28000/49000 (57%)]\tLoss: 1.508239\n",
      "Train Epoch: 16 [29000/49000 (59%)]\tLoss: 1.551054\n",
      "Train Epoch: 16 [30000/49000 (61%)]\tLoss: 1.535047\n",
      "Train Epoch: 16 [31000/49000 (63%)]\tLoss: 1.590506\n",
      "Train Epoch: 16 [32000/49000 (65%)]\tLoss: 1.527741\n",
      "Train Epoch: 16 [33000/49000 (67%)]\tLoss: 1.581544\n",
      "Train Epoch: 16 [34000/49000 (69%)]\tLoss: 1.552060\n",
      "Train Epoch: 16 [35000/49000 (71%)]\tLoss: 1.557972\n",
      "Train Epoch: 16 [36000/49000 (73%)]\tLoss: 1.565385\n",
      "Train Epoch: 16 [37000/49000 (76%)]\tLoss: 1.547382\n",
      "Train Epoch: 16 [38000/49000 (78%)]\tLoss: 1.552395\n",
      "Train Epoch: 16 [39000/49000 (80%)]\tLoss: 1.555763\n",
      "Train Epoch: 16 [40000/49000 (82%)]\tLoss: 1.555382\n",
      "Train Epoch: 16 [41000/49000 (84%)]\tLoss: 1.546864\n",
      "Train Epoch: 16 [42000/49000 (86%)]\tLoss: 1.545812\n",
      "Train Epoch: 16 [43000/49000 (88%)]\tLoss: 1.539559\n",
      "Train Epoch: 16 [44000/49000 (90%)]\tLoss: 1.539094\n",
      "Train Epoch: 16 [45000/49000 (92%)]\tLoss: 1.547614\n",
      "Train Epoch: 16 [46000/49000 (94%)]\tLoss: 1.553119\n",
      "Train Epoch: 16 [47000/49000 (96%)]\tLoss: 1.504033\n",
      "Train Epoch: 16 [48000/49000 (98%)]\tLoss: 1.547637\n",
      "\n",
      "Test set: Avg. loss: 1.5604, Accuracy: 19010/21000 (90.52%)\n",
      "\n",
      "Train Epoch: 17 [0/49000 (0%)]\tLoss: 1.560652\n",
      "Train Epoch: 17 [1000/49000 (2%)]\tLoss: 1.569655\n",
      "Train Epoch: 17 [2000/49000 (4%)]\tLoss: 1.582408\n",
      "Train Epoch: 17 [3000/49000 (6%)]\tLoss: 1.545286\n",
      "Train Epoch: 17 [4000/49000 (8%)]\tLoss: 1.561688\n",
      "Train Epoch: 17 [5000/49000 (10%)]\tLoss: 1.565693\n",
      "Train Epoch: 17 [6000/49000 (12%)]\tLoss: 1.550288\n",
      "Train Epoch: 17 [7000/49000 (14%)]\tLoss: 1.524182\n",
      "Train Epoch: 17 [8000/49000 (16%)]\tLoss: 1.536894\n",
      "Train Epoch: 17 [9000/49000 (18%)]\tLoss: 1.538774\n",
      "Train Epoch: 17 [10000/49000 (20%)]\tLoss: 1.526134\n",
      "Train Epoch: 17 [11000/49000 (22%)]\tLoss: 1.551259\n",
      "Train Epoch: 17 [12000/49000 (24%)]\tLoss: 1.583169\n",
      "Train Epoch: 17 [13000/49000 (27%)]\tLoss: 1.559818\n",
      "Train Epoch: 17 [14000/49000 (29%)]\tLoss: 1.513505\n",
      "Train Epoch: 17 [15000/49000 (31%)]\tLoss: 1.556803\n",
      "Train Epoch: 17 [16000/49000 (33%)]\tLoss: 1.536850\n",
      "Train Epoch: 17 [17000/49000 (35%)]\tLoss: 1.555961\n",
      "Train Epoch: 17 [18000/49000 (37%)]\tLoss: 1.519450\n",
      "Train Epoch: 17 [19000/49000 (39%)]\tLoss: 1.551365\n",
      "Train Epoch: 17 [20000/49000 (41%)]\tLoss: 1.530606\n",
      "Train Epoch: 17 [21000/49000 (43%)]\tLoss: 1.538812\n",
      "Train Epoch: 17 [22000/49000 (45%)]\tLoss: 1.548183\n",
      "Train Epoch: 17 [23000/49000 (47%)]\tLoss: 1.570350\n",
      "Train Epoch: 17 [24000/49000 (49%)]\tLoss: 1.549183\n",
      "Train Epoch: 17 [25000/49000 (51%)]\tLoss: 1.548704\n",
      "Train Epoch: 17 [26000/49000 (53%)]\tLoss: 1.583236\n",
      "Train Epoch: 17 [27000/49000 (55%)]\tLoss: 1.553008\n",
      "Train Epoch: 17 [28000/49000 (57%)]\tLoss: 1.544632\n",
      "Train Epoch: 17 [29000/49000 (59%)]\tLoss: 1.556603\n",
      "Train Epoch: 17 [30000/49000 (61%)]\tLoss: 1.583495\n",
      "Train Epoch: 17 [31000/49000 (63%)]\tLoss: 1.560102\n",
      "Train Epoch: 17 [32000/49000 (65%)]\tLoss: 1.562861\n",
      "Train Epoch: 17 [33000/49000 (67%)]\tLoss: 1.550650\n",
      "Train Epoch: 17 [34000/49000 (69%)]\tLoss: 1.532438\n",
      "Train Epoch: 17 [35000/49000 (71%)]\tLoss: 1.515584\n",
      "Train Epoch: 17 [36000/49000 (73%)]\tLoss: 1.538489\n",
      "Train Epoch: 17 [37000/49000 (76%)]\tLoss: 1.523488\n",
      "Train Epoch: 17 [38000/49000 (78%)]\tLoss: 1.546835\n",
      "Train Epoch: 17 [39000/49000 (80%)]\tLoss: 1.590535\n",
      "Train Epoch: 17 [40000/49000 (82%)]\tLoss: 1.554896\n",
      "Train Epoch: 17 [41000/49000 (84%)]\tLoss: 1.505642\n",
      "Train Epoch: 17 [42000/49000 (86%)]\tLoss: 1.520305\n",
      "Train Epoch: 17 [43000/49000 (88%)]\tLoss: 1.583307\n",
      "Train Epoch: 17 [44000/49000 (90%)]\tLoss: 1.579346\n",
      "Train Epoch: 17 [45000/49000 (92%)]\tLoss: 1.554545\n",
      "Train Epoch: 17 [46000/49000 (94%)]\tLoss: 1.564431\n",
      "Train Epoch: 17 [47000/49000 (96%)]\tLoss: 1.563793\n",
      "Train Epoch: 17 [48000/49000 (98%)]\tLoss: 1.556567\n",
      "\n",
      "Test set: Avg. loss: 1.5597, Accuracy: 19037/21000 (90.65%)\n",
      "\n",
      "Train Epoch: 18 [0/49000 (0%)]\tLoss: 1.550042\n",
      "Train Epoch: 18 [1000/49000 (2%)]\tLoss: 1.556258\n",
      "Train Epoch: 18 [2000/49000 (4%)]\tLoss: 1.563016\n",
      "Train Epoch: 18 [3000/49000 (6%)]\tLoss: 1.575662\n",
      "Train Epoch: 18 [4000/49000 (8%)]\tLoss: 1.517086\n",
      "Train Epoch: 18 [5000/49000 (10%)]\tLoss: 1.549002\n",
      "Train Epoch: 18 [6000/49000 (12%)]\tLoss: 1.619856\n",
      "Train Epoch: 18 [7000/49000 (14%)]\tLoss: 1.535747\n",
      "Train Epoch: 18 [8000/49000 (16%)]\tLoss: 1.522242\n",
      "Train Epoch: 18 [9000/49000 (18%)]\tLoss: 1.533747\n",
      "Train Epoch: 18 [10000/49000 (20%)]\tLoss: 1.532338\n",
      "Train Epoch: 18 [11000/49000 (22%)]\tLoss: 1.549604\n",
      "Train Epoch: 18 [12000/49000 (24%)]\tLoss: 1.539223\n",
      "Train Epoch: 18 [13000/49000 (27%)]\tLoss: 1.561900\n",
      "Train Epoch: 18 [14000/49000 (29%)]\tLoss: 1.558628\n",
      "Train Epoch: 18 [15000/49000 (31%)]\tLoss: 1.552070\n",
      "Train Epoch: 18 [16000/49000 (33%)]\tLoss: 1.568996\n",
      "Train Epoch: 18 [17000/49000 (35%)]\tLoss: 1.507180\n",
      "Train Epoch: 18 [18000/49000 (37%)]\tLoss: 1.553935\n",
      "Train Epoch: 18 [19000/49000 (39%)]\tLoss: 1.549857\n",
      "Train Epoch: 18 [20000/49000 (41%)]\tLoss: 1.528080\n",
      "Train Epoch: 18 [21000/49000 (43%)]\tLoss: 1.554634\n",
      "Train Epoch: 18 [22000/49000 (45%)]\tLoss: 1.603961\n",
      "Train Epoch: 18 [23000/49000 (47%)]\tLoss: 1.593545\n",
      "Train Epoch: 18 [24000/49000 (49%)]\tLoss: 1.572402\n",
      "Train Epoch: 18 [25000/49000 (51%)]\tLoss: 1.527463\n",
      "Train Epoch: 18 [26000/49000 (53%)]\tLoss: 1.554152\n",
      "Train Epoch: 18 [27000/49000 (55%)]\tLoss: 1.506730\n",
      "Train Epoch: 18 [28000/49000 (57%)]\tLoss: 1.553808\n",
      "Train Epoch: 18 [29000/49000 (59%)]\tLoss: 1.542621\n",
      "Train Epoch: 18 [30000/49000 (61%)]\tLoss: 1.545429\n",
      "Train Epoch: 18 [31000/49000 (63%)]\tLoss: 1.593745\n",
      "Train Epoch: 18 [32000/49000 (65%)]\tLoss: 1.562556\n",
      "Train Epoch: 18 [33000/49000 (67%)]\tLoss: 1.599283\n",
      "Train Epoch: 18 [34000/49000 (69%)]\tLoss: 1.581308\n",
      "Train Epoch: 18 [35000/49000 (71%)]\tLoss: 1.544095\n",
      "Train Epoch: 18 [36000/49000 (73%)]\tLoss: 1.571077\n",
      "Train Epoch: 18 [37000/49000 (76%)]\tLoss: 1.604644\n",
      "Train Epoch: 18 [38000/49000 (78%)]\tLoss: 1.570432\n",
      "Train Epoch: 18 [39000/49000 (80%)]\tLoss: 1.539398\n",
      "Train Epoch: 18 [40000/49000 (82%)]\tLoss: 1.611177\n",
      "Train Epoch: 18 [41000/49000 (84%)]\tLoss: 1.553989\n",
      "Train Epoch: 18 [42000/49000 (86%)]\tLoss: 1.576813\n",
      "Train Epoch: 18 [43000/49000 (88%)]\tLoss: 1.597223\n",
      "Train Epoch: 18 [44000/49000 (90%)]\tLoss: 1.604644\n",
      "Train Epoch: 18 [45000/49000 (92%)]\tLoss: 1.517870\n",
      "Train Epoch: 18 [46000/49000 (94%)]\tLoss: 1.555627\n",
      "Train Epoch: 18 [47000/49000 (96%)]\tLoss: 1.551394\n",
      "Train Epoch: 18 [48000/49000 (98%)]\tLoss: 1.569867\n",
      "\n",
      "Test set: Avg. loss: 1.5601, Accuracy: 18992/21000 (90.44%)\n",
      "\n",
      "Train Epoch: 19 [0/49000 (0%)]\tLoss: 1.550073\n",
      "Train Epoch: 19 [1000/49000 (2%)]\tLoss: 1.578327\n",
      "Train Epoch: 19 [2000/49000 (4%)]\tLoss: 1.539086\n",
      "Train Epoch: 19 [3000/49000 (6%)]\tLoss: 1.532674\n",
      "Train Epoch: 19 [4000/49000 (8%)]\tLoss: 1.533417\n",
      "Train Epoch: 19 [5000/49000 (10%)]\tLoss: 1.579872\n",
      "Train Epoch: 19 [6000/49000 (12%)]\tLoss: 1.589386\n",
      "Train Epoch: 19 [7000/49000 (14%)]\tLoss: 1.543412\n",
      "Train Epoch: 19 [8000/49000 (16%)]\tLoss: 1.554522\n",
      "Train Epoch: 19 [9000/49000 (18%)]\tLoss: 1.511946\n",
      "Train Epoch: 19 [10000/49000 (20%)]\tLoss: 1.540078\n",
      "Train Epoch: 19 [11000/49000 (22%)]\tLoss: 1.556228\n",
      "Train Epoch: 19 [12000/49000 (24%)]\tLoss: 1.564413\n",
      "Train Epoch: 19 [13000/49000 (27%)]\tLoss: 1.545692\n",
      "Train Epoch: 19 [14000/49000 (29%)]\tLoss: 1.567199\n",
      "Train Epoch: 19 [15000/49000 (31%)]\tLoss: 1.547636\n",
      "Train Epoch: 19 [16000/49000 (33%)]\tLoss: 1.609749\n",
      "Train Epoch: 19 [17000/49000 (35%)]\tLoss: 1.568781\n",
      "Train Epoch: 19 [18000/49000 (37%)]\tLoss: 1.526870\n",
      "Train Epoch: 19 [19000/49000 (39%)]\tLoss: 1.589477\n",
      "Train Epoch: 19 [20000/49000 (41%)]\tLoss: 1.546676\n",
      "Train Epoch: 19 [21000/49000 (43%)]\tLoss: 1.577400\n",
      "Train Epoch: 19 [22000/49000 (45%)]\tLoss: 1.533639\n",
      "Train Epoch: 19 [23000/49000 (47%)]\tLoss: 1.523952\n",
      "Train Epoch: 19 [24000/49000 (49%)]\tLoss: 1.518546\n",
      "Train Epoch: 19 [25000/49000 (51%)]\tLoss: 1.531414\n",
      "Train Epoch: 19 [26000/49000 (53%)]\tLoss: 1.599488\n",
      "Train Epoch: 19 [27000/49000 (55%)]\tLoss: 1.532786\n",
      "Train Epoch: 19 [28000/49000 (57%)]\tLoss: 1.545743\n",
      "Train Epoch: 19 [29000/49000 (59%)]\tLoss: 1.541113\n",
      "Train Epoch: 19 [30000/49000 (61%)]\tLoss: 1.555397\n",
      "Train Epoch: 19 [31000/49000 (63%)]\tLoss: 1.544283\n",
      "Train Epoch: 19 [32000/49000 (65%)]\tLoss: 1.589560\n",
      "Train Epoch: 19 [33000/49000 (67%)]\tLoss: 1.516446\n",
      "Train Epoch: 19 [34000/49000 (69%)]\tLoss: 1.515700\n",
      "Train Epoch: 19 [35000/49000 (71%)]\tLoss: 1.572431\n",
      "Train Epoch: 19 [36000/49000 (73%)]\tLoss: 1.570376\n",
      "Train Epoch: 19 [37000/49000 (76%)]\tLoss: 1.574729\n",
      "Train Epoch: 19 [38000/49000 (78%)]\tLoss: 1.589151\n",
      "Train Epoch: 19 [39000/49000 (80%)]\tLoss: 1.525311\n",
      "Train Epoch: 19 [40000/49000 (82%)]\tLoss: 1.547102\n",
      "Train Epoch: 19 [41000/49000 (84%)]\tLoss: 1.528733\n",
      "Train Epoch: 19 [42000/49000 (86%)]\tLoss: 1.567536\n",
      "Train Epoch: 19 [43000/49000 (88%)]\tLoss: 1.557076\n",
      "Train Epoch: 19 [44000/49000 (90%)]\tLoss: 1.586014\n",
      "Train Epoch: 19 [45000/49000 (92%)]\tLoss: 1.557539\n",
      "Train Epoch: 19 [46000/49000 (94%)]\tLoss: 1.531572\n",
      "Train Epoch: 19 [47000/49000 (96%)]\tLoss: 1.549640\n",
      "Train Epoch: 19 [48000/49000 (98%)]\tLoss: 1.525979\n",
      "\n",
      "Test set: Avg. loss: 1.5610, Accuracy: 18922/21000 (90.10%)\n",
      "\n",
      "Train Epoch: 20 [0/49000 (0%)]\tLoss: 1.550208\n",
      "Train Epoch: 20 [1000/49000 (2%)]\tLoss: 1.545975\n",
      "Train Epoch: 20 [2000/49000 (4%)]\tLoss: 1.563860\n",
      "Train Epoch: 20 [3000/49000 (6%)]\tLoss: 1.567830\n",
      "Train Epoch: 20 [4000/49000 (8%)]\tLoss: 1.549749\n",
      "Train Epoch: 20 [5000/49000 (10%)]\tLoss: 1.526236\n",
      "Train Epoch: 20 [6000/49000 (12%)]\tLoss: 1.537129\n",
      "Train Epoch: 20 [7000/49000 (14%)]\tLoss: 1.563745\n",
      "Train Epoch: 20 [8000/49000 (16%)]\tLoss: 1.583649\n",
      "Train Epoch: 20 [9000/49000 (18%)]\tLoss: 1.570327\n",
      "Train Epoch: 20 [10000/49000 (20%)]\tLoss: 1.559734\n",
      "Train Epoch: 20 [11000/49000 (22%)]\tLoss: 1.570502\n",
      "Train Epoch: 20 [12000/49000 (24%)]\tLoss: 1.493819\n",
      "Train Epoch: 20 [13000/49000 (27%)]\tLoss: 1.514950\n",
      "Train Epoch: 20 [14000/49000 (29%)]\tLoss: 1.551331\n",
      "Train Epoch: 20 [15000/49000 (31%)]\tLoss: 1.537802\n",
      "Train Epoch: 20 [16000/49000 (33%)]\tLoss: 1.572113\n",
      "Train Epoch: 20 [17000/49000 (35%)]\tLoss: 1.556413\n",
      "Train Epoch: 20 [18000/49000 (37%)]\tLoss: 1.519658\n",
      "Train Epoch: 20 [19000/49000 (39%)]\tLoss: 1.529338\n",
      "Train Epoch: 20 [20000/49000 (41%)]\tLoss: 1.549529\n",
      "Train Epoch: 20 [21000/49000 (43%)]\tLoss: 1.552601\n",
      "Train Epoch: 20 [22000/49000 (45%)]\tLoss: 1.525084\n",
      "Train Epoch: 20 [23000/49000 (47%)]\tLoss: 1.553938\n",
      "Train Epoch: 20 [24000/49000 (49%)]\tLoss: 1.559750\n",
      "Train Epoch: 20 [25000/49000 (51%)]\tLoss: 1.550114\n",
      "Train Epoch: 20 [26000/49000 (53%)]\tLoss: 1.538008\n",
      "Train Epoch: 20 [27000/49000 (55%)]\tLoss: 1.562401\n",
      "Train Epoch: 20 [28000/49000 (57%)]\tLoss: 1.539248\n",
      "Train Epoch: 20 [29000/49000 (59%)]\tLoss: 1.545617\n",
      "Train Epoch: 20 [30000/49000 (61%)]\tLoss: 1.523254\n",
      "Train Epoch: 20 [31000/49000 (63%)]\tLoss: 1.571591\n",
      "Train Epoch: 20 [32000/49000 (65%)]\tLoss: 1.530001\n",
      "Train Epoch: 20 [33000/49000 (67%)]\tLoss: 1.555013\n",
      "Train Epoch: 20 [34000/49000 (69%)]\tLoss: 1.562587\n",
      "Train Epoch: 20 [35000/49000 (71%)]\tLoss: 1.568919\n",
      "Train Epoch: 20 [36000/49000 (73%)]\tLoss: 1.565214\n",
      "Train Epoch: 20 [37000/49000 (76%)]\tLoss: 1.534404\n",
      "Train Epoch: 20 [38000/49000 (78%)]\tLoss: 1.556838\n",
      "Train Epoch: 20 [39000/49000 (80%)]\tLoss: 1.531425\n",
      "Train Epoch: 20 [40000/49000 (82%)]\tLoss: 1.575915\n",
      "Train Epoch: 20 [41000/49000 (84%)]\tLoss: 1.559286\n",
      "Train Epoch: 20 [42000/49000 (86%)]\tLoss: 1.535553\n",
      "Train Epoch: 20 [43000/49000 (88%)]\tLoss: 1.572859\n",
      "Train Epoch: 20 [44000/49000 (90%)]\tLoss: 1.522230\n",
      "Train Epoch: 20 [45000/49000 (92%)]\tLoss: 1.545748\n",
      "Train Epoch: 20 [46000/49000 (94%)]\tLoss: 1.522398\n",
      "Train Epoch: 20 [47000/49000 (96%)]\tLoss: 1.599203\n",
      "Train Epoch: 20 [48000/49000 (98%)]\tLoss: 1.532007\n",
      "\n",
      "Test set: Avg. loss: 1.5580, Accuracy: 18991/21000 (90.43%)\n",
      "\n",
      "Train Epoch: 21 [0/49000 (0%)]\tLoss: 1.585344\n",
      "Train Epoch: 21 [1000/49000 (2%)]\tLoss: 1.582029\n",
      "Train Epoch: 21 [2000/49000 (4%)]\tLoss: 1.606597\n",
      "Train Epoch: 21 [3000/49000 (6%)]\tLoss: 1.556473\n",
      "Train Epoch: 21 [4000/49000 (8%)]\tLoss: 1.540123\n",
      "Train Epoch: 21 [5000/49000 (10%)]\tLoss: 1.531963\n",
      "Train Epoch: 21 [6000/49000 (12%)]\tLoss: 1.518999\n",
      "Train Epoch: 21 [7000/49000 (14%)]\tLoss: 1.546849\n",
      "Train Epoch: 21 [8000/49000 (16%)]\tLoss: 1.526888\n",
      "Train Epoch: 21 [9000/49000 (18%)]\tLoss: 1.559798\n",
      "Train Epoch: 21 [10000/49000 (20%)]\tLoss: 1.531585\n",
      "Train Epoch: 21 [11000/49000 (22%)]\tLoss: 1.555810\n",
      "Train Epoch: 21 [12000/49000 (24%)]\tLoss: 1.528457\n",
      "Train Epoch: 21 [13000/49000 (27%)]\tLoss: 1.574512\n",
      "Train Epoch: 21 [14000/49000 (29%)]\tLoss: 1.566611\n",
      "Train Epoch: 21 [15000/49000 (31%)]\tLoss: 1.518928\n",
      "Train Epoch: 21 [16000/49000 (33%)]\tLoss: 1.504067\n",
      "Train Epoch: 21 [17000/49000 (35%)]\tLoss: 1.565871\n",
      "Train Epoch: 21 [18000/49000 (37%)]\tLoss: 1.536814\n",
      "Train Epoch: 21 [19000/49000 (39%)]\tLoss: 1.542445\n",
      "Train Epoch: 21 [20000/49000 (41%)]\tLoss: 1.555945\n",
      "Train Epoch: 21 [21000/49000 (43%)]\tLoss: 1.561707\n",
      "Train Epoch: 21 [22000/49000 (45%)]\tLoss: 1.535573\n",
      "Train Epoch: 21 [23000/49000 (47%)]\tLoss: 1.577352\n",
      "Train Epoch: 21 [24000/49000 (49%)]\tLoss: 1.535622\n",
      "Train Epoch: 21 [25000/49000 (51%)]\tLoss: 1.576326\n",
      "Train Epoch: 21 [26000/49000 (53%)]\tLoss: 1.560164\n",
      "Train Epoch: 21 [27000/49000 (55%)]\tLoss: 1.540760\n",
      "Train Epoch: 21 [28000/49000 (57%)]\tLoss: 1.573077\n",
      "Train Epoch: 21 [29000/49000 (59%)]\tLoss: 1.581339\n",
      "Train Epoch: 21 [30000/49000 (61%)]\tLoss: 1.513063\n",
      "Train Epoch: 21 [31000/49000 (63%)]\tLoss: 1.559578\n",
      "Train Epoch: 21 [32000/49000 (65%)]\tLoss: 1.553654\n",
      "Train Epoch: 21 [33000/49000 (67%)]\tLoss: 1.554268\n",
      "Train Epoch: 21 [34000/49000 (69%)]\tLoss: 1.553575\n",
      "Train Epoch: 21 [35000/49000 (71%)]\tLoss: 1.577651\n",
      "Train Epoch: 21 [36000/49000 (73%)]\tLoss: 1.561781\n",
      "Train Epoch: 21 [37000/49000 (76%)]\tLoss: 1.535867\n",
      "Train Epoch: 21 [38000/49000 (78%)]\tLoss: 1.571866\n",
      "Train Epoch: 21 [39000/49000 (80%)]\tLoss: 1.531230\n",
      "Train Epoch: 21 [40000/49000 (82%)]\tLoss: 1.575130\n",
      "Train Epoch: 21 [41000/49000 (84%)]\tLoss: 1.545868\n",
      "Train Epoch: 21 [42000/49000 (86%)]\tLoss: 1.524539\n",
      "Train Epoch: 21 [43000/49000 (88%)]\tLoss: 1.579915\n",
      "Train Epoch: 21 [44000/49000 (90%)]\tLoss: 1.588680\n",
      "Train Epoch: 21 [45000/49000 (92%)]\tLoss: 1.560977\n",
      "Train Epoch: 21 [46000/49000 (94%)]\tLoss: 1.516278\n",
      "Train Epoch: 21 [47000/49000 (96%)]\tLoss: 1.547912\n",
      "Train Epoch: 21 [48000/49000 (98%)]\tLoss: 1.541771\n",
      "\n",
      "Test set: Avg. loss: 1.5586, Accuracy: 18969/21000 (90.33%)\n",
      "\n",
      "Train Epoch: 22 [0/49000 (0%)]\tLoss: 1.546466\n",
      "Train Epoch: 22 [1000/49000 (2%)]\tLoss: 1.609134\n",
      "Train Epoch: 22 [2000/49000 (4%)]\tLoss: 1.546775\n",
      "Train Epoch: 22 [3000/49000 (6%)]\tLoss: 1.559944\n",
      "Train Epoch: 22 [4000/49000 (8%)]\tLoss: 1.559442\n",
      "Train Epoch: 22 [5000/49000 (10%)]\tLoss: 1.567218\n",
      "Train Epoch: 22 [6000/49000 (12%)]\tLoss: 1.526943\n",
      "Train Epoch: 22 [7000/49000 (14%)]\tLoss: 1.534040\n",
      "Train Epoch: 22 [8000/49000 (16%)]\tLoss: 1.568901\n",
      "Train Epoch: 22 [9000/49000 (18%)]\tLoss: 1.557254\n",
      "Train Epoch: 22 [10000/49000 (20%)]\tLoss: 1.545055\n",
      "Train Epoch: 22 [11000/49000 (22%)]\tLoss: 1.573946\n",
      "Train Epoch: 22 [12000/49000 (24%)]\tLoss: 1.573412\n",
      "Train Epoch: 22 [13000/49000 (27%)]\tLoss: 1.544548\n",
      "Train Epoch: 22 [14000/49000 (29%)]\tLoss: 1.554369\n",
      "Train Epoch: 22 [15000/49000 (31%)]\tLoss: 1.522660\n",
      "Train Epoch: 22 [16000/49000 (33%)]\tLoss: 1.587069\n",
      "Train Epoch: 22 [17000/49000 (35%)]\tLoss: 1.530591\n",
      "Train Epoch: 22 [18000/49000 (37%)]\tLoss: 1.575214\n",
      "Train Epoch: 22 [19000/49000 (39%)]\tLoss: 1.514539\n",
      "Train Epoch: 22 [20000/49000 (41%)]\tLoss: 1.557651\n",
      "Train Epoch: 22 [21000/49000 (43%)]\tLoss: 1.516696\n",
      "Train Epoch: 22 [22000/49000 (45%)]\tLoss: 1.543511\n",
      "Train Epoch: 22 [23000/49000 (47%)]\tLoss: 1.578909\n",
      "Train Epoch: 22 [24000/49000 (49%)]\tLoss: 1.535035\n",
      "Train Epoch: 22 [25000/49000 (51%)]\tLoss: 1.530836\n",
      "Train Epoch: 22 [26000/49000 (53%)]\tLoss: 1.541184\n",
      "Train Epoch: 22 [27000/49000 (55%)]\tLoss: 1.563580\n",
      "Train Epoch: 22 [28000/49000 (57%)]\tLoss: 1.555987\n",
      "Train Epoch: 22 [29000/49000 (59%)]\tLoss: 1.547490\n",
      "Train Epoch: 22 [30000/49000 (61%)]\tLoss: 1.594460\n",
      "Train Epoch: 22 [31000/49000 (63%)]\tLoss: 1.546959\n",
      "Train Epoch: 22 [32000/49000 (65%)]\tLoss: 1.520892\n",
      "Train Epoch: 22 [33000/49000 (67%)]\tLoss: 1.555173\n",
      "Train Epoch: 22 [34000/49000 (69%)]\tLoss: 1.551014\n",
      "Train Epoch: 22 [35000/49000 (71%)]\tLoss: 1.529741\n",
      "Train Epoch: 22 [36000/49000 (73%)]\tLoss: 1.581916\n",
      "Train Epoch: 22 [37000/49000 (76%)]\tLoss: 1.548756\n",
      "Train Epoch: 22 [38000/49000 (78%)]\tLoss: 1.541277\n",
      "Train Epoch: 22 [39000/49000 (80%)]\tLoss: 1.563812\n",
      "Train Epoch: 22 [40000/49000 (82%)]\tLoss: 1.546489\n",
      "Train Epoch: 22 [41000/49000 (84%)]\tLoss: 1.555294\n",
      "Train Epoch: 22 [42000/49000 (86%)]\tLoss: 1.525950\n",
      "Train Epoch: 22 [43000/49000 (88%)]\tLoss: 1.554181\n",
      "Train Epoch: 22 [44000/49000 (90%)]\tLoss: 1.576879\n",
      "Train Epoch: 22 [45000/49000 (92%)]\tLoss: 1.536336\n",
      "Train Epoch: 22 [46000/49000 (94%)]\tLoss: 1.557533\n",
      "Train Epoch: 22 [47000/49000 (96%)]\tLoss: 1.533485\n",
      "Train Epoch: 22 [48000/49000 (98%)]\tLoss: 1.576289\n",
      "\n",
      "Test set: Avg. loss: 1.5571, Accuracy: 19033/21000 (90.63%)\n",
      "\n",
      "Train Epoch: 23 [0/49000 (0%)]\tLoss: 1.553450\n",
      "Train Epoch: 23 [1000/49000 (2%)]\tLoss: 1.561367\n",
      "Train Epoch: 23 [2000/49000 (4%)]\tLoss: 1.571934\n",
      "Train Epoch: 23 [3000/49000 (6%)]\tLoss: 1.538718\n",
      "Train Epoch: 23 [4000/49000 (8%)]\tLoss: 1.596754\n",
      "Train Epoch: 23 [5000/49000 (10%)]\tLoss: 1.545866\n",
      "Train Epoch: 23 [6000/49000 (12%)]\tLoss: 1.550090\n",
      "Train Epoch: 23 [7000/49000 (14%)]\tLoss: 1.571477\n",
      "Train Epoch: 23 [8000/49000 (16%)]\tLoss: 1.569284\n",
      "Train Epoch: 23 [9000/49000 (18%)]\tLoss: 1.542521\n",
      "Train Epoch: 23 [10000/49000 (20%)]\tLoss: 1.536130\n",
      "Train Epoch: 23 [11000/49000 (22%)]\tLoss: 1.534693\n",
      "Train Epoch: 23 [12000/49000 (24%)]\tLoss: 1.565055\n",
      "Train Epoch: 23 [13000/49000 (27%)]\tLoss: 1.564012\n",
      "Train Epoch: 23 [14000/49000 (29%)]\tLoss: 1.537959\n",
      "Train Epoch: 23 [15000/49000 (31%)]\tLoss: 1.538180\n",
      "Train Epoch: 23 [16000/49000 (33%)]\tLoss: 1.599395\n",
      "Train Epoch: 23 [17000/49000 (35%)]\tLoss: 1.572737\n",
      "Train Epoch: 23 [18000/49000 (37%)]\tLoss: 1.525289\n",
      "Train Epoch: 23 [19000/49000 (39%)]\tLoss: 1.568710\n",
      "Train Epoch: 23 [20000/49000 (41%)]\tLoss: 1.580120\n",
      "Train Epoch: 23 [21000/49000 (43%)]\tLoss: 1.529598\n",
      "Train Epoch: 23 [22000/49000 (45%)]\tLoss: 1.595649\n",
      "Train Epoch: 23 [23000/49000 (47%)]\tLoss: 1.566426\n",
      "Train Epoch: 23 [24000/49000 (49%)]\tLoss: 1.586771\n",
      "Train Epoch: 23 [25000/49000 (51%)]\tLoss: 1.569550\n",
      "Train Epoch: 23 [26000/49000 (53%)]\tLoss: 1.540020\n",
      "Train Epoch: 23 [27000/49000 (55%)]\tLoss: 1.513283\n",
      "Train Epoch: 23 [28000/49000 (57%)]\tLoss: 1.530928\n",
      "Train Epoch: 23 [29000/49000 (59%)]\tLoss: 1.549776\n",
      "Train Epoch: 23 [30000/49000 (61%)]\tLoss: 1.528250\n",
      "Train Epoch: 23 [31000/49000 (63%)]\tLoss: 1.545893\n",
      "Train Epoch: 23 [32000/49000 (65%)]\tLoss: 1.518229\n",
      "Train Epoch: 23 [33000/49000 (67%)]\tLoss: 1.541502\n",
      "Train Epoch: 23 [34000/49000 (69%)]\tLoss: 1.542949\n",
      "Train Epoch: 23 [35000/49000 (71%)]\tLoss: 1.532363\n",
      "Train Epoch: 23 [36000/49000 (73%)]\tLoss: 1.523793\n",
      "Train Epoch: 23 [37000/49000 (76%)]\tLoss: 1.526561\n",
      "Train Epoch: 23 [38000/49000 (78%)]\tLoss: 1.542043\n",
      "Train Epoch: 23 [39000/49000 (80%)]\tLoss: 1.511934\n",
      "Train Epoch: 23 [40000/49000 (82%)]\tLoss: 1.556169\n",
      "Train Epoch: 23 [41000/49000 (84%)]\tLoss: 1.518380\n",
      "Train Epoch: 23 [42000/49000 (86%)]\tLoss: 1.514676\n",
      "Train Epoch: 23 [43000/49000 (88%)]\tLoss: 1.584501\n",
      "Train Epoch: 23 [44000/49000 (90%)]\tLoss: 1.547390\n",
      "Train Epoch: 23 [45000/49000 (92%)]\tLoss: 1.555492\n",
      "Train Epoch: 23 [46000/49000 (94%)]\tLoss: 1.546752\n",
      "Train Epoch: 23 [47000/49000 (96%)]\tLoss: 1.562081\n",
      "Train Epoch: 23 [48000/49000 (98%)]\tLoss: 1.559595\n",
      "\n",
      "Test set: Avg. loss: 1.5569, Accuracy: 19019/21000 (90.57%)\n",
      "\n",
      "Train Epoch: 24 [0/49000 (0%)]\tLoss: 1.531693\n",
      "Train Epoch: 24 [1000/49000 (2%)]\tLoss: 1.521130\n",
      "Train Epoch: 24 [2000/49000 (4%)]\tLoss: 1.546577\n",
      "Train Epoch: 24 [3000/49000 (6%)]\tLoss: 1.561791\n",
      "Train Epoch: 24 [4000/49000 (8%)]\tLoss: 1.551342\n",
      "Train Epoch: 24 [5000/49000 (10%)]\tLoss: 1.534752\n",
      "Train Epoch: 24 [6000/49000 (12%)]\tLoss: 1.566432\n",
      "Train Epoch: 24 [7000/49000 (14%)]\tLoss: 1.597815\n",
      "Train Epoch: 24 [8000/49000 (16%)]\tLoss: 1.583671\n",
      "Train Epoch: 24 [9000/49000 (18%)]\tLoss: 1.543131\n",
      "Train Epoch: 24 [10000/49000 (20%)]\tLoss: 1.550729\n",
      "Train Epoch: 24 [11000/49000 (22%)]\tLoss: 1.546221\n",
      "Train Epoch: 24 [12000/49000 (24%)]\tLoss: 1.541241\n",
      "Train Epoch: 24 [13000/49000 (27%)]\tLoss: 1.573403\n",
      "Train Epoch: 24 [14000/49000 (29%)]\tLoss: 1.543082\n",
      "Train Epoch: 24 [15000/49000 (31%)]\tLoss: 1.545069\n",
      "Train Epoch: 24 [16000/49000 (33%)]\tLoss: 1.568864\n",
      "Train Epoch: 24 [17000/49000 (35%)]\tLoss: 1.557929\n",
      "Train Epoch: 24 [18000/49000 (37%)]\tLoss: 1.523574\n",
      "Train Epoch: 24 [19000/49000 (39%)]\tLoss: 1.559205\n",
      "Train Epoch: 24 [20000/49000 (41%)]\tLoss: 1.542080\n",
      "Train Epoch: 24 [21000/49000 (43%)]\tLoss: 1.533038\n",
      "Train Epoch: 24 [22000/49000 (45%)]\tLoss: 1.547108\n",
      "Train Epoch: 24 [23000/49000 (47%)]\tLoss: 1.540620\n",
      "Train Epoch: 24 [24000/49000 (49%)]\tLoss: 1.527360\n",
      "Train Epoch: 24 [25000/49000 (51%)]\tLoss: 1.539357\n",
      "Train Epoch: 24 [26000/49000 (53%)]\tLoss: 1.569698\n",
      "Train Epoch: 24 [27000/49000 (55%)]\tLoss: 1.591951\n",
      "Train Epoch: 24 [28000/49000 (57%)]\tLoss: 1.548597\n",
      "Train Epoch: 24 [29000/49000 (59%)]\tLoss: 1.527938\n",
      "Train Epoch: 24 [30000/49000 (61%)]\tLoss: 1.561514\n",
      "Train Epoch: 24 [31000/49000 (63%)]\tLoss: 1.533791\n",
      "Train Epoch: 24 [32000/49000 (65%)]\tLoss: 1.549955\n",
      "Train Epoch: 24 [33000/49000 (67%)]\tLoss: 1.560675\n",
      "Train Epoch: 24 [34000/49000 (69%)]\tLoss: 1.569669\n",
      "Train Epoch: 24 [35000/49000 (71%)]\tLoss: 1.570581\n",
      "Train Epoch: 24 [36000/49000 (73%)]\tLoss: 1.548484\n",
      "Train Epoch: 24 [37000/49000 (76%)]\tLoss: 1.529151\n",
      "Train Epoch: 24 [38000/49000 (78%)]\tLoss: 1.537316\n",
      "Train Epoch: 24 [39000/49000 (80%)]\tLoss: 1.548734\n",
      "Train Epoch: 24 [40000/49000 (82%)]\tLoss: 1.512351\n",
      "Train Epoch: 24 [41000/49000 (84%)]\tLoss: 1.558515\n",
      "Train Epoch: 24 [42000/49000 (86%)]\tLoss: 1.587546\n",
      "Train Epoch: 24 [43000/49000 (88%)]\tLoss: 1.536376\n",
      "Train Epoch: 24 [44000/49000 (90%)]\tLoss: 1.517365\n",
      "Train Epoch: 24 [45000/49000 (92%)]\tLoss: 1.538172\n",
      "Train Epoch: 24 [46000/49000 (94%)]\tLoss: 1.569671\n",
      "Train Epoch: 24 [47000/49000 (96%)]\tLoss: 1.541041\n",
      "Train Epoch: 24 [48000/49000 (98%)]\tLoss: 1.554170\n",
      "\n",
      "Test set: Avg. loss: 1.5568, Accuracy: 19021/21000 (90.58%)\n",
      "\n",
      "Train Epoch: 25 [0/49000 (0%)]\tLoss: 1.542600\n",
      "Train Epoch: 25 [1000/49000 (2%)]\tLoss: 1.527818\n",
      "Train Epoch: 25 [2000/49000 (4%)]\tLoss: 1.536984\n",
      "Train Epoch: 25 [3000/49000 (6%)]\tLoss: 1.531604\n",
      "Train Epoch: 25 [4000/49000 (8%)]\tLoss: 1.539888\n",
      "Train Epoch: 25 [5000/49000 (10%)]\tLoss: 1.531212\n",
      "Train Epoch: 25 [6000/49000 (12%)]\tLoss: 1.589852\n",
      "Train Epoch: 25 [7000/49000 (14%)]\tLoss: 1.534500\n",
      "Train Epoch: 25 [8000/49000 (16%)]\tLoss: 1.621937\n",
      "Train Epoch: 25 [9000/49000 (18%)]\tLoss: 1.542837\n",
      "Train Epoch: 25 [10000/49000 (20%)]\tLoss: 1.555343\n",
      "Train Epoch: 25 [11000/49000 (22%)]\tLoss: 1.635442\n",
      "Train Epoch: 25 [12000/49000 (24%)]\tLoss: 1.526518\n",
      "Train Epoch: 25 [13000/49000 (27%)]\tLoss: 1.530827\n",
      "Train Epoch: 25 [14000/49000 (29%)]\tLoss: 1.542582\n",
      "Train Epoch: 25 [15000/49000 (31%)]\tLoss: 1.559747\n",
      "Train Epoch: 25 [16000/49000 (33%)]\tLoss: 1.551518\n",
      "Train Epoch: 25 [17000/49000 (35%)]\tLoss: 1.543705\n",
      "Train Epoch: 25 [18000/49000 (37%)]\tLoss: 1.532085\n",
      "Train Epoch: 25 [19000/49000 (39%)]\tLoss: 1.526722\n",
      "Train Epoch: 25 [20000/49000 (41%)]\tLoss: 1.516559\n",
      "Train Epoch: 25 [21000/49000 (43%)]\tLoss: 1.551647\n",
      "Train Epoch: 25 [22000/49000 (45%)]\tLoss: 1.562457\n",
      "Train Epoch: 25 [23000/49000 (47%)]\tLoss: 1.553875\n",
      "Train Epoch: 25 [24000/49000 (49%)]\tLoss: 1.535065\n",
      "Train Epoch: 25 [25000/49000 (51%)]\tLoss: 1.547950\n",
      "Train Epoch: 25 [26000/49000 (53%)]\tLoss: 1.568197\n",
      "Train Epoch: 25 [27000/49000 (55%)]\tLoss: 1.578871\n",
      "Train Epoch: 25 [28000/49000 (57%)]\tLoss: 1.536223\n",
      "Train Epoch: 25 [29000/49000 (59%)]\tLoss: 1.542502\n",
      "Train Epoch: 25 [30000/49000 (61%)]\tLoss: 1.523966\n",
      "Train Epoch: 25 [31000/49000 (63%)]\tLoss: 1.579148\n",
      "Train Epoch: 25 [32000/49000 (65%)]\tLoss: 1.519557\n",
      "Train Epoch: 25 [33000/49000 (67%)]\tLoss: 1.563053\n",
      "Train Epoch: 25 [34000/49000 (69%)]\tLoss: 1.561626\n",
      "Train Epoch: 25 [35000/49000 (71%)]\tLoss: 1.556929\n",
      "Train Epoch: 25 [36000/49000 (73%)]\tLoss: 1.589565\n",
      "Train Epoch: 25 [37000/49000 (76%)]\tLoss: 1.555990\n",
      "Train Epoch: 25 [38000/49000 (78%)]\tLoss: 1.554090\n",
      "Train Epoch: 25 [39000/49000 (80%)]\tLoss: 1.583400\n",
      "Train Epoch: 25 [40000/49000 (82%)]\tLoss: 1.554872\n",
      "Train Epoch: 25 [41000/49000 (84%)]\tLoss: 1.559606\n",
      "Train Epoch: 25 [42000/49000 (86%)]\tLoss: 1.586959\n",
      "Train Epoch: 25 [43000/49000 (88%)]\tLoss: 1.546378\n",
      "Train Epoch: 25 [44000/49000 (90%)]\tLoss: 1.539113\n",
      "Train Epoch: 25 [45000/49000 (92%)]\tLoss: 1.549073\n",
      "Train Epoch: 25 [46000/49000 (94%)]\tLoss: 1.551195\n",
      "Train Epoch: 25 [47000/49000 (96%)]\tLoss: 1.541229\n",
      "Train Epoch: 25 [48000/49000 (98%)]\tLoss: 1.577038\n",
      "\n",
      "Test set: Avg. loss: 1.5577, Accuracy: 19032/21000 (90.63%)\n",
      "\n",
      "Train Epoch: 26 [0/49000 (0%)]\tLoss: 1.565013\n",
      "Train Epoch: 26 [1000/49000 (2%)]\tLoss: 1.550729\n",
      "Train Epoch: 26 [2000/49000 (4%)]\tLoss: 1.580862\n",
      "Train Epoch: 26 [3000/49000 (6%)]\tLoss: 1.570966\n",
      "Train Epoch: 26 [4000/49000 (8%)]\tLoss: 1.555198\n",
      "Train Epoch: 26 [5000/49000 (10%)]\tLoss: 1.557088\n",
      "Train Epoch: 26 [6000/49000 (12%)]\tLoss: 1.555149\n",
      "Train Epoch: 26 [7000/49000 (14%)]\tLoss: 1.579459\n",
      "Train Epoch: 26 [8000/49000 (16%)]\tLoss: 1.612540\n",
      "Train Epoch: 26 [9000/49000 (18%)]\tLoss: 1.535011\n",
      "Train Epoch: 26 [10000/49000 (20%)]\tLoss: 1.579035\n",
      "Train Epoch: 26 [11000/49000 (22%)]\tLoss: 1.533803\n",
      "Train Epoch: 26 [12000/49000 (24%)]\tLoss: 1.571422\n",
      "Train Epoch: 26 [13000/49000 (27%)]\tLoss: 1.543040\n",
      "Train Epoch: 26 [14000/49000 (29%)]\tLoss: 1.566621\n",
      "Train Epoch: 26 [15000/49000 (31%)]\tLoss: 1.533027\n",
      "Train Epoch: 26 [16000/49000 (33%)]\tLoss: 1.535607\n",
      "Train Epoch: 26 [17000/49000 (35%)]\tLoss: 1.567599\n",
      "Train Epoch: 26 [18000/49000 (37%)]\tLoss: 1.549158\n",
      "Train Epoch: 26 [19000/49000 (39%)]\tLoss: 1.555618\n",
      "Train Epoch: 26 [20000/49000 (41%)]\tLoss: 1.520781\n",
      "Train Epoch: 26 [21000/49000 (43%)]\tLoss: 1.537509\n",
      "Train Epoch: 26 [22000/49000 (45%)]\tLoss: 1.511910\n",
      "Train Epoch: 26 [23000/49000 (47%)]\tLoss: 1.586234\n",
      "Train Epoch: 26 [24000/49000 (49%)]\tLoss: 1.575407\n",
      "Train Epoch: 26 [25000/49000 (51%)]\tLoss: 1.555308\n",
      "Train Epoch: 26 [26000/49000 (53%)]\tLoss: 1.571246\n",
      "Train Epoch: 26 [27000/49000 (55%)]\tLoss: 1.506998\n",
      "Train Epoch: 26 [28000/49000 (57%)]\tLoss: 1.572978\n",
      "Train Epoch: 26 [29000/49000 (59%)]\tLoss: 1.579674\n",
      "Train Epoch: 26 [30000/49000 (61%)]\tLoss: 1.533765\n",
      "Train Epoch: 26 [31000/49000 (63%)]\tLoss: 1.525197\n",
      "Train Epoch: 26 [32000/49000 (65%)]\tLoss: 1.539677\n",
      "Train Epoch: 26 [33000/49000 (67%)]\tLoss: 1.527760\n",
      "Train Epoch: 26 [34000/49000 (69%)]\tLoss: 1.520224\n",
      "Train Epoch: 26 [35000/49000 (71%)]\tLoss: 1.555501\n",
      "Train Epoch: 26 [36000/49000 (73%)]\tLoss: 1.512268\n",
      "Train Epoch: 26 [37000/49000 (76%)]\tLoss: 1.538254\n",
      "Train Epoch: 26 [38000/49000 (78%)]\tLoss: 1.550450\n",
      "Train Epoch: 26 [39000/49000 (80%)]\tLoss: 1.570359\n",
      "Train Epoch: 26 [40000/49000 (82%)]\tLoss: 1.524209\n",
      "Train Epoch: 26 [41000/49000 (84%)]\tLoss: 1.546679\n",
      "Train Epoch: 26 [42000/49000 (86%)]\tLoss: 1.516405\n",
      "Train Epoch: 26 [43000/49000 (88%)]\tLoss: 1.572080\n",
      "Train Epoch: 26 [44000/49000 (90%)]\tLoss: 1.580906\n",
      "Train Epoch: 26 [45000/49000 (92%)]\tLoss: 1.534659\n",
      "Train Epoch: 26 [46000/49000 (94%)]\tLoss: 1.543382\n",
      "Train Epoch: 26 [47000/49000 (96%)]\tLoss: 1.556722\n",
      "Train Epoch: 26 [48000/49000 (98%)]\tLoss: 1.498724\n",
      "\n",
      "Test set: Avg. loss: 1.5580, Accuracy: 18999/21000 (90.47%)\n",
      "\n",
      "Train Epoch: 27 [0/49000 (0%)]\tLoss: 1.531225\n",
      "Train Epoch: 27 [1000/49000 (2%)]\tLoss: 1.566999\n",
      "Train Epoch: 27 [2000/49000 (4%)]\tLoss: 1.540860\n",
      "Train Epoch: 27 [3000/49000 (6%)]\tLoss: 1.557639\n",
      "Train Epoch: 27 [4000/49000 (8%)]\tLoss: 1.523489\n",
      "Train Epoch: 27 [5000/49000 (10%)]\tLoss: 1.563002\n",
      "Train Epoch: 27 [6000/49000 (12%)]\tLoss: 1.549058\n",
      "Train Epoch: 27 [7000/49000 (14%)]\tLoss: 1.558334\n",
      "Train Epoch: 27 [8000/49000 (16%)]\tLoss: 1.547775\n",
      "Train Epoch: 27 [9000/49000 (18%)]\tLoss: 1.547184\n",
      "Train Epoch: 27 [10000/49000 (20%)]\tLoss: 1.573299\n",
      "Train Epoch: 27 [11000/49000 (22%)]\tLoss: 1.580919\n",
      "Train Epoch: 27 [12000/49000 (24%)]\tLoss: 1.561113\n",
      "Train Epoch: 27 [13000/49000 (27%)]\tLoss: 1.587375\n",
      "Train Epoch: 27 [14000/49000 (29%)]\tLoss: 1.537043\n",
      "Train Epoch: 27 [15000/49000 (31%)]\tLoss: 1.550730\n",
      "Train Epoch: 27 [16000/49000 (33%)]\tLoss: 1.532736\n",
      "Train Epoch: 27 [17000/49000 (35%)]\tLoss: 1.590886\n",
      "Train Epoch: 27 [18000/49000 (37%)]\tLoss: 1.548047\n",
      "Train Epoch: 27 [19000/49000 (39%)]\tLoss: 1.549318\n",
      "Train Epoch: 27 [20000/49000 (41%)]\tLoss: 1.560875\n",
      "Train Epoch: 27 [21000/49000 (43%)]\tLoss: 1.547004\n",
      "Train Epoch: 27 [22000/49000 (45%)]\tLoss: 1.505100\n",
      "Train Epoch: 27 [23000/49000 (47%)]\tLoss: 1.520085\n",
      "Train Epoch: 27 [24000/49000 (49%)]\tLoss: 1.541563\n",
      "Train Epoch: 27 [25000/49000 (51%)]\tLoss: 1.557254\n",
      "Train Epoch: 27 [26000/49000 (53%)]\tLoss: 1.580257\n",
      "Train Epoch: 27 [27000/49000 (55%)]\tLoss: 1.572162\n",
      "Train Epoch: 27 [28000/49000 (57%)]\tLoss: 1.518918\n",
      "Train Epoch: 27 [29000/49000 (59%)]\tLoss: 1.542278\n",
      "Train Epoch: 27 [30000/49000 (61%)]\tLoss: 1.587347\n",
      "Train Epoch: 27 [31000/49000 (63%)]\tLoss: 1.530164\n",
      "Train Epoch: 27 [32000/49000 (65%)]\tLoss: 1.538845\n",
      "Train Epoch: 27 [33000/49000 (67%)]\tLoss: 1.514064\n",
      "Train Epoch: 27 [34000/49000 (69%)]\tLoss: 1.517982\n",
      "Train Epoch: 27 [35000/49000 (71%)]\tLoss: 1.555366\n",
      "Train Epoch: 27 [36000/49000 (73%)]\tLoss: 1.520189\n",
      "Train Epoch: 27 [37000/49000 (76%)]\tLoss: 1.562694\n",
      "Train Epoch: 27 [38000/49000 (78%)]\tLoss: 1.557932\n",
      "Train Epoch: 27 [39000/49000 (80%)]\tLoss: 1.604992\n",
      "Train Epoch: 27 [40000/49000 (82%)]\tLoss: 1.611855\n",
      "Train Epoch: 27 [41000/49000 (84%)]\tLoss: 1.554799\n",
      "Train Epoch: 27 [42000/49000 (86%)]\tLoss: 1.552618\n",
      "Train Epoch: 27 [43000/49000 (88%)]\tLoss: 1.532400\n",
      "Train Epoch: 27 [44000/49000 (90%)]\tLoss: 1.584571\n",
      "Train Epoch: 27 [45000/49000 (92%)]\tLoss: 1.570721\n",
      "Train Epoch: 27 [46000/49000 (94%)]\tLoss: 1.602977\n",
      "Train Epoch: 27 [47000/49000 (96%)]\tLoss: 1.575662\n",
      "Train Epoch: 27 [48000/49000 (98%)]\tLoss: 1.558127\n",
      "\n",
      "Test set: Avg. loss: 1.5556, Accuracy: 19055/21000 (90.74%)\n",
      "\n",
      "Train Epoch: 28 [0/49000 (0%)]\tLoss: 1.581376\n",
      "Train Epoch: 28 [1000/49000 (2%)]\tLoss: 1.520566\n",
      "Train Epoch: 28 [2000/49000 (4%)]\tLoss: 1.538208\n",
      "Train Epoch: 28 [3000/49000 (6%)]\tLoss: 1.526167\n",
      "Train Epoch: 28 [4000/49000 (8%)]\tLoss: 1.573669\n",
      "Train Epoch: 28 [5000/49000 (10%)]\tLoss: 1.551946\n",
      "Train Epoch: 28 [6000/49000 (12%)]\tLoss: 1.533269\n",
      "Train Epoch: 28 [7000/49000 (14%)]\tLoss: 1.563102\n",
      "Train Epoch: 28 [8000/49000 (16%)]\tLoss: 1.520101\n",
      "Train Epoch: 28 [9000/49000 (18%)]\tLoss: 1.546843\n",
      "Train Epoch: 28 [10000/49000 (20%)]\tLoss: 1.535918\n",
      "Train Epoch: 28 [11000/49000 (22%)]\tLoss: 1.556823\n",
      "Train Epoch: 28 [12000/49000 (24%)]\tLoss: 1.569670\n",
      "Train Epoch: 28 [13000/49000 (27%)]\tLoss: 1.578617\n",
      "Train Epoch: 28 [14000/49000 (29%)]\tLoss: 1.549537\n",
      "Train Epoch: 28 [15000/49000 (31%)]\tLoss: 1.573228\n",
      "Train Epoch: 28 [16000/49000 (33%)]\tLoss: 1.560162\n",
      "Train Epoch: 28 [17000/49000 (35%)]\tLoss: 1.514391\n",
      "Train Epoch: 28 [18000/49000 (37%)]\tLoss: 1.560585\n",
      "Train Epoch: 28 [19000/49000 (39%)]\tLoss: 1.525522\n",
      "Train Epoch: 28 [20000/49000 (41%)]\tLoss: 1.525113\n",
      "Train Epoch: 28 [21000/49000 (43%)]\tLoss: 1.556400\n",
      "Train Epoch: 28 [22000/49000 (45%)]\tLoss: 1.524386\n",
      "Train Epoch: 28 [23000/49000 (47%)]\tLoss: 1.566646\n",
      "Train Epoch: 28 [24000/49000 (49%)]\tLoss: 1.540873\n",
      "Train Epoch: 28 [25000/49000 (51%)]\tLoss: 1.557928\n",
      "Train Epoch: 28 [26000/49000 (53%)]\tLoss: 1.568078\n",
      "Train Epoch: 28 [27000/49000 (55%)]\tLoss: 1.554864\n",
      "Train Epoch: 28 [28000/49000 (57%)]\tLoss: 1.570389\n",
      "Train Epoch: 28 [29000/49000 (59%)]\tLoss: 1.564056\n",
      "Train Epoch: 28 [30000/49000 (61%)]\tLoss: 1.548755\n",
      "Train Epoch: 28 [31000/49000 (63%)]\tLoss: 1.574670\n",
      "Train Epoch: 28 [32000/49000 (65%)]\tLoss: 1.558201\n",
      "Train Epoch: 28 [33000/49000 (67%)]\tLoss: 1.582441\n",
      "Train Epoch: 28 [34000/49000 (69%)]\tLoss: 1.546842\n",
      "Train Epoch: 28 [35000/49000 (71%)]\tLoss: 1.559227\n",
      "Train Epoch: 28 [36000/49000 (73%)]\tLoss: 1.625875\n",
      "Train Epoch: 28 [37000/49000 (76%)]\tLoss: 1.585465\n",
      "Train Epoch: 28 [38000/49000 (78%)]\tLoss: 1.561171\n",
      "Train Epoch: 28 [39000/49000 (80%)]\tLoss: 1.551245\n",
      "Train Epoch: 28 [40000/49000 (82%)]\tLoss: 1.555145\n",
      "Train Epoch: 28 [41000/49000 (84%)]\tLoss: 1.538634\n",
      "Train Epoch: 28 [42000/49000 (86%)]\tLoss: 1.558374\n",
      "Train Epoch: 28 [43000/49000 (88%)]\tLoss: 1.512627\n",
      "Train Epoch: 28 [44000/49000 (90%)]\tLoss: 1.527481\n",
      "Train Epoch: 28 [45000/49000 (92%)]\tLoss: 1.561331\n",
      "Train Epoch: 28 [46000/49000 (94%)]\tLoss: 1.530653\n",
      "Train Epoch: 28 [47000/49000 (96%)]\tLoss: 1.536983\n",
      "Train Epoch: 28 [48000/49000 (98%)]\tLoss: 1.536498\n",
      "\n",
      "Test set: Avg. loss: 1.5555, Accuracy: 19061/21000 (90.77%)\n",
      "\n",
      "Train Epoch: 29 [0/49000 (0%)]\tLoss: 1.574875\n",
      "Train Epoch: 29 [1000/49000 (2%)]\tLoss: 1.547541\n",
      "Train Epoch: 29 [2000/49000 (4%)]\tLoss: 1.536439\n",
      "Train Epoch: 29 [3000/49000 (6%)]\tLoss: 1.544734\n",
      "Train Epoch: 29 [4000/49000 (8%)]\tLoss: 1.535351\n",
      "Train Epoch: 29 [5000/49000 (10%)]\tLoss: 1.529389\n",
      "Train Epoch: 29 [6000/49000 (12%)]\tLoss: 1.567211\n",
      "Train Epoch: 29 [7000/49000 (14%)]\tLoss: 1.541793\n",
      "Train Epoch: 29 [8000/49000 (16%)]\tLoss: 1.507994\n",
      "Train Epoch: 29 [9000/49000 (18%)]\tLoss: 1.513263\n",
      "Train Epoch: 29 [10000/49000 (20%)]\tLoss: 1.525119\n",
      "Train Epoch: 29 [11000/49000 (22%)]\tLoss: 1.556450\n",
      "Train Epoch: 29 [12000/49000 (24%)]\tLoss: 1.559147\n",
      "Train Epoch: 29 [13000/49000 (27%)]\tLoss: 1.588958\n",
      "Train Epoch: 29 [14000/49000 (29%)]\tLoss: 1.563448\n",
      "Train Epoch: 29 [15000/49000 (31%)]\tLoss: 1.571868\n",
      "Train Epoch: 29 [16000/49000 (33%)]\tLoss: 1.549603\n",
      "Train Epoch: 29 [17000/49000 (35%)]\tLoss: 1.571476\n",
      "Train Epoch: 29 [18000/49000 (37%)]\tLoss: 1.556134\n",
      "Train Epoch: 29 [19000/49000 (39%)]\tLoss: 1.535004\n",
      "Train Epoch: 29 [20000/49000 (41%)]\tLoss: 1.532167\n",
      "Train Epoch: 29 [21000/49000 (43%)]\tLoss: 1.537090\n",
      "Train Epoch: 29 [22000/49000 (45%)]\tLoss: 1.555642\n",
      "Train Epoch: 29 [23000/49000 (47%)]\tLoss: 1.541120\n",
      "Train Epoch: 29 [24000/49000 (49%)]\tLoss: 1.538636\n",
      "Train Epoch: 29 [25000/49000 (51%)]\tLoss: 1.522663\n",
      "Train Epoch: 29 [26000/49000 (53%)]\tLoss: 1.530911\n",
      "Train Epoch: 29 [27000/49000 (55%)]\tLoss: 1.578576\n",
      "Train Epoch: 29 [28000/49000 (57%)]\tLoss: 1.539888\n",
      "Train Epoch: 29 [29000/49000 (59%)]\tLoss: 1.561012\n",
      "Train Epoch: 29 [30000/49000 (61%)]\tLoss: 1.566312\n",
      "Train Epoch: 29 [31000/49000 (63%)]\tLoss: 1.519001\n",
      "Train Epoch: 29 [32000/49000 (65%)]\tLoss: 1.552833\n",
      "Train Epoch: 29 [33000/49000 (67%)]\tLoss: 1.539144\n",
      "Train Epoch: 29 [34000/49000 (69%)]\tLoss: 1.522591\n",
      "Train Epoch: 29 [35000/49000 (71%)]\tLoss: 1.551533\n",
      "Train Epoch: 29 [36000/49000 (73%)]\tLoss: 1.570646\n",
      "Train Epoch: 29 [37000/49000 (76%)]\tLoss: 1.577770\n",
      "Train Epoch: 29 [38000/49000 (78%)]\tLoss: 1.590504\n",
      "Train Epoch: 29 [39000/49000 (80%)]\tLoss: 1.517777\n",
      "Train Epoch: 29 [40000/49000 (82%)]\tLoss: 1.515579\n",
      "Train Epoch: 29 [41000/49000 (84%)]\tLoss: 1.574901\n",
      "Train Epoch: 29 [42000/49000 (86%)]\tLoss: 1.525981\n",
      "Train Epoch: 29 [43000/49000 (88%)]\tLoss: 1.541752\n",
      "Train Epoch: 29 [44000/49000 (90%)]\tLoss: 1.537497\n",
      "Train Epoch: 29 [45000/49000 (92%)]\tLoss: 1.579556\n",
      "Train Epoch: 29 [46000/49000 (94%)]\tLoss: 1.530402\n",
      "Train Epoch: 29 [47000/49000 (96%)]\tLoss: 1.559539\n",
      "Train Epoch: 29 [48000/49000 (98%)]\tLoss: 1.553951\n",
      "\n",
      "Test set: Avg. loss: 1.5565, Accuracy: 19058/21000 (90.75%)\n",
      "\n",
      "Train Epoch: 30 [0/49000 (0%)]\tLoss: 1.565038\n",
      "Train Epoch: 30 [1000/49000 (2%)]\tLoss: 1.582291\n",
      "Train Epoch: 30 [2000/49000 (4%)]\tLoss: 1.552167\n",
      "Train Epoch: 30 [3000/49000 (6%)]\tLoss: 1.529281\n",
      "Train Epoch: 30 [4000/49000 (8%)]\tLoss: 1.529960\n",
      "Train Epoch: 30 [5000/49000 (10%)]\tLoss: 1.549222\n",
      "Train Epoch: 30 [6000/49000 (12%)]\tLoss: 1.603400\n",
      "Train Epoch: 30 [7000/49000 (14%)]\tLoss: 1.567782\n",
      "Train Epoch: 30 [8000/49000 (16%)]\tLoss: 1.556249\n",
      "Train Epoch: 30 [9000/49000 (18%)]\tLoss: 1.545959\n",
      "Train Epoch: 30 [10000/49000 (20%)]\tLoss: 1.524175\n",
      "Train Epoch: 30 [11000/49000 (22%)]\tLoss: 1.565662\n",
      "Train Epoch: 30 [12000/49000 (24%)]\tLoss: 1.556619\n",
      "Train Epoch: 30 [13000/49000 (27%)]\tLoss: 1.579185\n",
      "Train Epoch: 30 [14000/49000 (29%)]\tLoss: 1.532851\n",
      "Train Epoch: 30 [15000/49000 (31%)]\tLoss: 1.548032\n",
      "Train Epoch: 30 [16000/49000 (33%)]\tLoss: 1.534840\n",
      "Train Epoch: 30 [17000/49000 (35%)]\tLoss: 1.553167\n",
      "Train Epoch: 30 [18000/49000 (37%)]\tLoss: 1.533719\n",
      "Train Epoch: 30 [19000/49000 (39%)]\tLoss: 1.604730\n",
      "Train Epoch: 30 [20000/49000 (41%)]\tLoss: 1.559535\n",
      "Train Epoch: 30 [21000/49000 (43%)]\tLoss: 1.513106\n",
      "Train Epoch: 30 [22000/49000 (45%)]\tLoss: 1.524432\n",
      "Train Epoch: 30 [23000/49000 (47%)]\tLoss: 1.546218\n",
      "Train Epoch: 30 [24000/49000 (49%)]\tLoss: 1.526763\n",
      "Train Epoch: 30 [25000/49000 (51%)]\tLoss: 1.569004\n",
      "Train Epoch: 30 [26000/49000 (53%)]\tLoss: 1.511914\n",
      "Train Epoch: 30 [27000/49000 (55%)]\tLoss: 1.573348\n",
      "Train Epoch: 30 [28000/49000 (57%)]\tLoss: 1.610952\n",
      "Train Epoch: 30 [29000/49000 (59%)]\tLoss: 1.572626\n",
      "Train Epoch: 30 [30000/49000 (61%)]\tLoss: 1.524663\n",
      "Train Epoch: 30 [31000/49000 (63%)]\tLoss: 1.528142\n",
      "Train Epoch: 30 [32000/49000 (65%)]\tLoss: 1.525901\n",
      "Train Epoch: 30 [33000/49000 (67%)]\tLoss: 1.557200\n",
      "Train Epoch: 30 [34000/49000 (69%)]\tLoss: 1.584312\n",
      "Train Epoch: 30 [35000/49000 (71%)]\tLoss: 1.524417\n",
      "Train Epoch: 30 [36000/49000 (73%)]\tLoss: 1.578378\n",
      "Train Epoch: 30 [37000/49000 (76%)]\tLoss: 1.532423\n",
      "Train Epoch: 30 [38000/49000 (78%)]\tLoss: 1.524532\n",
      "Train Epoch: 30 [39000/49000 (80%)]\tLoss: 1.537566\n",
      "Train Epoch: 30 [40000/49000 (82%)]\tLoss: 1.544903\n",
      "Train Epoch: 30 [41000/49000 (84%)]\tLoss: 1.538158\n",
      "Train Epoch: 30 [42000/49000 (86%)]\tLoss: 1.537853\n",
      "Train Epoch: 30 [43000/49000 (88%)]\tLoss: 1.528690\n",
      "Train Epoch: 30 [44000/49000 (90%)]\tLoss: 1.536305\n",
      "Train Epoch: 30 [45000/49000 (92%)]\tLoss: 1.533159\n",
      "Train Epoch: 30 [46000/49000 (94%)]\tLoss: 1.556645\n",
      "Train Epoch: 30 [47000/49000 (96%)]\tLoss: 1.537001\n",
      "Train Epoch: 30 [48000/49000 (98%)]\tLoss: 1.551377\n",
      "\n",
      "Test set: Avg. loss: 1.5548, Accuracy: 19053/21000 (90.73%)\n",
      "\n",
      "Train Epoch: 31 [0/49000 (0%)]\tLoss: 1.553842\n",
      "Train Epoch: 31 [1000/49000 (2%)]\tLoss: 1.532225\n",
      "Train Epoch: 31 [2000/49000 (4%)]\tLoss: 1.571939\n",
      "Train Epoch: 31 [3000/49000 (6%)]\tLoss: 1.565054\n",
      "Train Epoch: 31 [4000/49000 (8%)]\tLoss: 1.550674\n",
      "Train Epoch: 31 [5000/49000 (10%)]\tLoss: 1.533067\n",
      "Train Epoch: 31 [6000/49000 (12%)]\tLoss: 1.561254\n",
      "Train Epoch: 31 [7000/49000 (14%)]\tLoss: 1.519603\n",
      "Train Epoch: 31 [8000/49000 (16%)]\tLoss: 1.565879\n",
      "Train Epoch: 31 [9000/49000 (18%)]\tLoss: 1.546483\n",
      "Train Epoch: 31 [10000/49000 (20%)]\tLoss: 1.559241\n",
      "Train Epoch: 31 [11000/49000 (22%)]\tLoss: 1.547955\n",
      "Train Epoch: 31 [12000/49000 (24%)]\tLoss: 1.531384\n",
      "Train Epoch: 31 [13000/49000 (27%)]\tLoss: 1.538286\n",
      "Train Epoch: 31 [14000/49000 (29%)]\tLoss: 1.531882\n",
      "Train Epoch: 31 [15000/49000 (31%)]\tLoss: 1.538479\n",
      "Train Epoch: 31 [16000/49000 (33%)]\tLoss: 1.544002\n",
      "Train Epoch: 31 [17000/49000 (35%)]\tLoss: 1.603140\n",
      "Train Epoch: 31 [18000/49000 (37%)]\tLoss: 1.531117\n",
      "Train Epoch: 31 [19000/49000 (39%)]\tLoss: 1.536468\n",
      "Train Epoch: 31 [20000/49000 (41%)]\tLoss: 1.529872\n",
      "Train Epoch: 31 [21000/49000 (43%)]\tLoss: 1.544659\n",
      "Train Epoch: 31 [22000/49000 (45%)]\tLoss: 1.538572\n",
      "Train Epoch: 31 [23000/49000 (47%)]\tLoss: 1.552841\n",
      "Train Epoch: 31 [24000/49000 (49%)]\tLoss: 1.552386\n",
      "Train Epoch: 31 [25000/49000 (51%)]\tLoss: 1.581457\n",
      "Train Epoch: 31 [26000/49000 (53%)]\tLoss: 1.570834\n",
      "Train Epoch: 31 [27000/49000 (55%)]\tLoss: 1.551763\n",
      "Train Epoch: 31 [28000/49000 (57%)]\tLoss: 1.549460\n",
      "Train Epoch: 31 [29000/49000 (59%)]\tLoss: 1.558294\n",
      "Train Epoch: 31 [30000/49000 (61%)]\tLoss: 1.561834\n",
      "Train Epoch: 31 [31000/49000 (63%)]\tLoss: 1.539817\n",
      "Train Epoch: 31 [32000/49000 (65%)]\tLoss: 1.547220\n",
      "Train Epoch: 31 [33000/49000 (67%)]\tLoss: 1.579559\n",
      "Train Epoch: 31 [34000/49000 (69%)]\tLoss: 1.575520\n",
      "Train Epoch: 31 [35000/49000 (71%)]\tLoss: 1.514233\n",
      "Train Epoch: 31 [36000/49000 (73%)]\tLoss: 1.546018\n",
      "Train Epoch: 31 [37000/49000 (76%)]\tLoss: 1.547622\n",
      "Train Epoch: 31 [38000/49000 (78%)]\tLoss: 1.591308\n",
      "Train Epoch: 31 [39000/49000 (80%)]\tLoss: 1.560182\n",
      "Train Epoch: 31 [40000/49000 (82%)]\tLoss: 1.541270\n",
      "Train Epoch: 31 [41000/49000 (84%)]\tLoss: 1.539296\n",
      "Train Epoch: 31 [42000/49000 (86%)]\tLoss: 1.577299\n",
      "Train Epoch: 31 [43000/49000 (88%)]\tLoss: 1.549231\n",
      "Train Epoch: 31 [44000/49000 (90%)]\tLoss: 1.520300\n",
      "Train Epoch: 31 [45000/49000 (92%)]\tLoss: 1.573749\n",
      "Train Epoch: 31 [46000/49000 (94%)]\tLoss: 1.538452\n",
      "Train Epoch: 31 [47000/49000 (96%)]\tLoss: 1.545402\n",
      "Train Epoch: 31 [48000/49000 (98%)]\tLoss: 1.531855\n",
      "\n",
      "Test set: Avg. loss: 1.5548, Accuracy: 19079/21000 (90.85%)\n",
      "\n",
      "Train Epoch: 32 [0/49000 (0%)]\tLoss: 1.554088\n",
      "Train Epoch: 32 [1000/49000 (2%)]\tLoss: 1.538816\n",
      "Train Epoch: 32 [2000/49000 (4%)]\tLoss: 1.542350\n",
      "Train Epoch: 32 [3000/49000 (6%)]\tLoss: 1.548173\n",
      "Train Epoch: 32 [4000/49000 (8%)]\tLoss: 1.536725\n",
      "Train Epoch: 32 [5000/49000 (10%)]\tLoss: 1.551695\n",
      "Train Epoch: 32 [6000/49000 (12%)]\tLoss: 1.546683\n",
      "Train Epoch: 32 [7000/49000 (14%)]\tLoss: 1.576990\n",
      "Train Epoch: 32 [8000/49000 (16%)]\tLoss: 1.558457\n",
      "Train Epoch: 32 [9000/49000 (18%)]\tLoss: 1.510158\n",
      "Train Epoch: 32 [10000/49000 (20%)]\tLoss: 1.542797\n",
      "Train Epoch: 32 [11000/49000 (22%)]\tLoss: 1.529583\n",
      "Train Epoch: 32 [12000/49000 (24%)]\tLoss: 1.550708\n",
      "Train Epoch: 32 [13000/49000 (27%)]\tLoss: 1.531228\n",
      "Train Epoch: 32 [14000/49000 (29%)]\tLoss: 1.523498\n",
      "Train Epoch: 32 [15000/49000 (31%)]\tLoss: 1.557674\n",
      "Train Epoch: 32 [16000/49000 (33%)]\tLoss: 1.551830\n",
      "Train Epoch: 32 [17000/49000 (35%)]\tLoss: 1.544746\n",
      "Train Epoch: 32 [18000/49000 (37%)]\tLoss: 1.557176\n",
      "Train Epoch: 32 [19000/49000 (39%)]\tLoss: 1.568196\n",
      "Train Epoch: 32 [20000/49000 (41%)]\tLoss: 1.542130\n",
      "Train Epoch: 32 [21000/49000 (43%)]\tLoss: 1.509384\n",
      "Train Epoch: 32 [22000/49000 (45%)]\tLoss: 1.555490\n",
      "Train Epoch: 32 [23000/49000 (47%)]\tLoss: 1.561516\n",
      "Train Epoch: 32 [24000/49000 (49%)]\tLoss: 1.523142\n",
      "Train Epoch: 32 [25000/49000 (51%)]\tLoss: 1.520731\n",
      "Train Epoch: 32 [26000/49000 (53%)]\tLoss: 1.547041\n",
      "Train Epoch: 32 [27000/49000 (55%)]\tLoss: 1.547787\n",
      "Train Epoch: 32 [28000/49000 (57%)]\tLoss: 1.560034\n",
      "Train Epoch: 32 [29000/49000 (59%)]\tLoss: 1.592174\n",
      "Train Epoch: 32 [30000/49000 (61%)]\tLoss: 1.526741\n",
      "Train Epoch: 32 [31000/49000 (63%)]\tLoss: 1.535239\n",
      "Train Epoch: 32 [32000/49000 (65%)]\tLoss: 1.512137\n",
      "Train Epoch: 32 [33000/49000 (67%)]\tLoss: 1.528331\n",
      "Train Epoch: 32 [34000/49000 (69%)]\tLoss: 1.584786\n",
      "Train Epoch: 32 [35000/49000 (71%)]\tLoss: 1.528330\n",
      "Train Epoch: 32 [36000/49000 (73%)]\tLoss: 1.555437\n",
      "Train Epoch: 32 [37000/49000 (76%)]\tLoss: 1.536889\n",
      "Train Epoch: 32 [38000/49000 (78%)]\tLoss: 1.500436\n",
      "Train Epoch: 32 [39000/49000 (80%)]\tLoss: 1.533515\n",
      "Train Epoch: 32 [40000/49000 (82%)]\tLoss: 1.548872\n",
      "Train Epoch: 32 [41000/49000 (84%)]\tLoss: 1.541338\n",
      "Train Epoch: 32 [42000/49000 (86%)]\tLoss: 1.521949\n",
      "Train Epoch: 32 [43000/49000 (88%)]\tLoss: 1.519229\n",
      "Train Epoch: 32 [44000/49000 (90%)]\tLoss: 1.518861\n",
      "Train Epoch: 32 [45000/49000 (92%)]\tLoss: 1.568115\n",
      "Train Epoch: 32 [46000/49000 (94%)]\tLoss: 1.540439\n",
      "Train Epoch: 32 [47000/49000 (96%)]\tLoss: 1.538399\n",
      "Train Epoch: 32 [48000/49000 (98%)]\tLoss: 1.557186\n",
      "\n",
      "Test set: Avg. loss: 1.5568, Accuracy: 18990/21000 (90.43%)\n",
      "\n",
      "Train Epoch: 33 [0/49000 (0%)]\tLoss: 1.555596\n",
      "Train Epoch: 33 [1000/49000 (2%)]\tLoss: 1.538327\n",
      "Train Epoch: 33 [2000/49000 (4%)]\tLoss: 1.528514\n",
      "Train Epoch: 33 [3000/49000 (6%)]\tLoss: 1.574083\n",
      "Train Epoch: 33 [4000/49000 (8%)]\tLoss: 1.523118\n",
      "Train Epoch: 33 [5000/49000 (10%)]\tLoss: 1.555528\n",
      "Train Epoch: 33 [6000/49000 (12%)]\tLoss: 1.515000\n",
      "Train Epoch: 33 [7000/49000 (14%)]\tLoss: 1.548277\n",
      "Train Epoch: 33 [8000/49000 (16%)]\tLoss: 1.552329\n",
      "Train Epoch: 33 [9000/49000 (18%)]\tLoss: 1.562493\n",
      "Train Epoch: 33 [10000/49000 (20%)]\tLoss: 1.572992\n",
      "Train Epoch: 33 [11000/49000 (22%)]\tLoss: 1.527166\n",
      "Train Epoch: 33 [12000/49000 (24%)]\tLoss: 1.538921\n",
      "Train Epoch: 33 [13000/49000 (27%)]\tLoss: 1.515139\n",
      "Train Epoch: 33 [14000/49000 (29%)]\tLoss: 1.550243\n",
      "Train Epoch: 33 [15000/49000 (31%)]\tLoss: 1.544810\n",
      "Train Epoch: 33 [16000/49000 (33%)]\tLoss: 1.524311\n",
      "Train Epoch: 33 [17000/49000 (35%)]\tLoss: 1.513485\n",
      "Train Epoch: 33 [18000/49000 (37%)]\tLoss: 1.518079\n",
      "Train Epoch: 33 [19000/49000 (39%)]\tLoss: 1.527569\n",
      "Train Epoch: 33 [20000/49000 (41%)]\tLoss: 1.542384\n",
      "Train Epoch: 33 [21000/49000 (43%)]\tLoss: 1.514577\n",
      "Train Epoch: 33 [22000/49000 (45%)]\tLoss: 1.546930\n",
      "Train Epoch: 33 [23000/49000 (47%)]\tLoss: 1.515947\n",
      "Train Epoch: 33 [24000/49000 (49%)]\tLoss: 1.585970\n",
      "Train Epoch: 33 [25000/49000 (51%)]\tLoss: 1.576906\n",
      "Train Epoch: 33 [26000/49000 (53%)]\tLoss: 1.581250\n",
      "Train Epoch: 33 [27000/49000 (55%)]\tLoss: 1.529615\n",
      "Train Epoch: 33 [28000/49000 (57%)]\tLoss: 1.596572\n",
      "Train Epoch: 33 [29000/49000 (59%)]\tLoss: 1.588145\n",
      "Train Epoch: 33 [30000/49000 (61%)]\tLoss: 1.522977\n",
      "Train Epoch: 33 [31000/49000 (63%)]\tLoss: 1.587463\n",
      "Train Epoch: 33 [32000/49000 (65%)]\tLoss: 1.559720\n",
      "Train Epoch: 33 [33000/49000 (67%)]\tLoss: 1.539172\n",
      "Train Epoch: 33 [34000/49000 (69%)]\tLoss: 1.520676\n",
      "Train Epoch: 33 [35000/49000 (71%)]\tLoss: 1.514066\n",
      "Train Epoch: 33 [36000/49000 (73%)]\tLoss: 1.538582\n",
      "Train Epoch: 33 [37000/49000 (76%)]\tLoss: 1.510180\n",
      "Train Epoch: 33 [38000/49000 (78%)]\tLoss: 1.522926\n",
      "Train Epoch: 33 [39000/49000 (80%)]\tLoss: 1.578769\n",
      "Train Epoch: 33 [40000/49000 (82%)]\tLoss: 1.578967\n",
      "Train Epoch: 33 [41000/49000 (84%)]\tLoss: 1.523811\n",
      "Train Epoch: 33 [42000/49000 (86%)]\tLoss: 1.572996\n",
      "Train Epoch: 33 [43000/49000 (88%)]\tLoss: 1.597273\n",
      "Train Epoch: 33 [44000/49000 (90%)]\tLoss: 1.542753\n",
      "Train Epoch: 33 [45000/49000 (92%)]\tLoss: 1.522761\n",
      "Train Epoch: 33 [46000/49000 (94%)]\tLoss: 1.520996\n",
      "Train Epoch: 33 [47000/49000 (96%)]\tLoss: 1.536408\n",
      "Train Epoch: 33 [48000/49000 (98%)]\tLoss: 1.563561\n",
      "\n",
      "Test set: Avg. loss: 1.5554, Accuracy: 19047/21000 (90.70%)\n",
      "\n",
      "Train Epoch: 34 [0/49000 (0%)]\tLoss: 1.538454\n",
      "Train Epoch: 34 [1000/49000 (2%)]\tLoss: 1.567647\n",
      "Train Epoch: 34 [2000/49000 (4%)]\tLoss: 1.522292\n",
      "Train Epoch: 34 [3000/49000 (6%)]\tLoss: 1.531295\n",
      "Train Epoch: 34 [4000/49000 (8%)]\tLoss: 1.560248\n",
      "Train Epoch: 34 [5000/49000 (10%)]\tLoss: 1.542513\n",
      "Train Epoch: 34 [6000/49000 (12%)]\tLoss: 1.549261\n",
      "Train Epoch: 34 [7000/49000 (14%)]\tLoss: 1.545046\n",
      "Train Epoch: 34 [8000/49000 (16%)]\tLoss: 1.545515\n",
      "Train Epoch: 34 [9000/49000 (18%)]\tLoss: 1.563086\n",
      "Train Epoch: 34 [10000/49000 (20%)]\tLoss: 1.546066\n",
      "Train Epoch: 34 [11000/49000 (22%)]\tLoss: 1.557306\n",
      "Train Epoch: 34 [12000/49000 (24%)]\tLoss: 1.533083\n",
      "Train Epoch: 34 [13000/49000 (27%)]\tLoss: 1.541982\n",
      "Train Epoch: 34 [14000/49000 (29%)]\tLoss: 1.505050\n",
      "Train Epoch: 34 [15000/49000 (31%)]\tLoss: 1.532175\n",
      "Train Epoch: 34 [16000/49000 (33%)]\tLoss: 1.532750\n",
      "Train Epoch: 34 [17000/49000 (35%)]\tLoss: 1.552258\n",
      "Train Epoch: 34 [18000/49000 (37%)]\tLoss: 1.559415\n",
      "Train Epoch: 34 [19000/49000 (39%)]\tLoss: 1.532421\n",
      "Train Epoch: 34 [20000/49000 (41%)]\tLoss: 1.628509\n",
      "Train Epoch: 34 [21000/49000 (43%)]\tLoss: 1.587665\n",
      "Train Epoch: 34 [22000/49000 (45%)]\tLoss: 1.532326\n",
      "Train Epoch: 34 [23000/49000 (47%)]\tLoss: 1.515916\n",
      "Train Epoch: 34 [24000/49000 (49%)]\tLoss: 1.560332\n",
      "Train Epoch: 34 [25000/49000 (51%)]\tLoss: 1.569589\n",
      "Train Epoch: 34 [26000/49000 (53%)]\tLoss: 1.566660\n",
      "Train Epoch: 34 [27000/49000 (55%)]\tLoss: 1.582924\n",
      "Train Epoch: 34 [28000/49000 (57%)]\tLoss: 1.557929\n",
      "Train Epoch: 34 [29000/49000 (59%)]\tLoss: 1.505587\n",
      "Train Epoch: 34 [30000/49000 (61%)]\tLoss: 1.539567\n",
      "Train Epoch: 34 [31000/49000 (63%)]\tLoss: 1.555647\n",
      "Train Epoch: 34 [32000/49000 (65%)]\tLoss: 1.556596\n",
      "Train Epoch: 34 [33000/49000 (67%)]\tLoss: 1.545859\n",
      "Train Epoch: 34 [34000/49000 (69%)]\tLoss: 1.539204\n",
      "Train Epoch: 34 [35000/49000 (71%)]\tLoss: 1.538781\n",
      "Train Epoch: 34 [36000/49000 (73%)]\tLoss: 1.552763\n",
      "Train Epoch: 34 [37000/49000 (76%)]\tLoss: 1.535822\n",
      "Train Epoch: 34 [38000/49000 (78%)]\tLoss: 1.557945\n",
      "Train Epoch: 34 [39000/49000 (80%)]\tLoss: 1.541841\n",
      "Train Epoch: 34 [40000/49000 (82%)]\tLoss: 1.550920\n",
      "Train Epoch: 34 [41000/49000 (84%)]\tLoss: 1.523607\n",
      "Train Epoch: 34 [42000/49000 (86%)]\tLoss: 1.555904\n",
      "Train Epoch: 34 [43000/49000 (88%)]\tLoss: 1.524573\n",
      "Train Epoch: 34 [44000/49000 (90%)]\tLoss: 1.570661\n",
      "Train Epoch: 34 [45000/49000 (92%)]\tLoss: 1.534636\n",
      "Train Epoch: 34 [46000/49000 (94%)]\tLoss: 1.547868\n",
      "Train Epoch: 34 [47000/49000 (96%)]\tLoss: 1.570082\n",
      "Train Epoch: 34 [48000/49000 (98%)]\tLoss: 1.572711\n",
      "\n",
      "Test set: Avg. loss: 1.5543, Accuracy: 19035/21000 (90.64%)\n",
      "\n",
      "Train Epoch: 35 [0/49000 (0%)]\tLoss: 1.531575\n",
      "Train Epoch: 35 [1000/49000 (2%)]\tLoss: 1.514893\n",
      "Train Epoch: 35 [2000/49000 (4%)]\tLoss: 1.566631\n",
      "Train Epoch: 35 [3000/49000 (6%)]\tLoss: 1.524921\n",
      "Train Epoch: 35 [4000/49000 (8%)]\tLoss: 1.563866\n",
      "Train Epoch: 35 [5000/49000 (10%)]\tLoss: 1.540866\n",
      "Train Epoch: 35 [6000/49000 (12%)]\tLoss: 1.538139\n",
      "Train Epoch: 35 [7000/49000 (14%)]\tLoss: 1.541197\n",
      "Train Epoch: 35 [8000/49000 (16%)]\tLoss: 1.559634\n",
      "Train Epoch: 35 [9000/49000 (18%)]\tLoss: 1.541074\n",
      "Train Epoch: 35 [10000/49000 (20%)]\tLoss: 1.522281\n",
      "Train Epoch: 35 [11000/49000 (22%)]\tLoss: 1.536823\n",
      "Train Epoch: 35 [12000/49000 (24%)]\tLoss: 1.559536\n",
      "Train Epoch: 35 [13000/49000 (27%)]\tLoss: 1.545756\n",
      "Train Epoch: 35 [14000/49000 (29%)]\tLoss: 1.542904\n",
      "Train Epoch: 35 [15000/49000 (31%)]\tLoss: 1.551559\n",
      "Train Epoch: 35 [16000/49000 (33%)]\tLoss: 1.503501\n",
      "Train Epoch: 35 [17000/49000 (35%)]\tLoss: 1.526163\n",
      "Train Epoch: 35 [18000/49000 (37%)]\tLoss: 1.595439\n",
      "Train Epoch: 35 [19000/49000 (39%)]\tLoss: 1.531535\n",
      "Train Epoch: 35 [20000/49000 (41%)]\tLoss: 1.523916\n",
      "Train Epoch: 35 [21000/49000 (43%)]\tLoss: 1.535666\n",
      "Train Epoch: 35 [22000/49000 (45%)]\tLoss: 1.539598\n",
      "Train Epoch: 35 [23000/49000 (47%)]\tLoss: 1.519177\n",
      "Train Epoch: 35 [24000/49000 (49%)]\tLoss: 1.568709\n",
      "Train Epoch: 35 [25000/49000 (51%)]\tLoss: 1.545452\n",
      "Train Epoch: 35 [26000/49000 (53%)]\tLoss: 1.553115\n",
      "Train Epoch: 35 [27000/49000 (55%)]\tLoss: 1.572318\n",
      "Train Epoch: 35 [28000/49000 (57%)]\tLoss: 1.536922\n",
      "Train Epoch: 35 [29000/49000 (59%)]\tLoss: 1.547915\n",
      "Train Epoch: 35 [30000/49000 (61%)]\tLoss: 1.567675\n",
      "Train Epoch: 35 [31000/49000 (63%)]\tLoss: 1.532696\n",
      "Train Epoch: 35 [32000/49000 (65%)]\tLoss: 1.551083\n",
      "Train Epoch: 35 [33000/49000 (67%)]\tLoss: 1.537805\n",
      "Train Epoch: 35 [34000/49000 (69%)]\tLoss: 1.538530\n",
      "Train Epoch: 35 [35000/49000 (71%)]\tLoss: 1.528450\n",
      "Train Epoch: 35 [36000/49000 (73%)]\tLoss: 1.520766\n",
      "Train Epoch: 35 [37000/49000 (76%)]\tLoss: 1.540899\n",
      "Train Epoch: 35 [38000/49000 (78%)]\tLoss: 1.518706\n",
      "Train Epoch: 35 [39000/49000 (80%)]\tLoss: 1.536098\n",
      "Train Epoch: 35 [40000/49000 (82%)]\tLoss: 1.527400\n",
      "Train Epoch: 35 [41000/49000 (84%)]\tLoss: 1.573240\n",
      "Train Epoch: 35 [42000/49000 (86%)]\tLoss: 1.533331\n",
      "Train Epoch: 35 [43000/49000 (88%)]\tLoss: 1.552995\n",
      "Train Epoch: 35 [44000/49000 (90%)]\tLoss: 1.541749\n",
      "Train Epoch: 35 [45000/49000 (92%)]\tLoss: 1.612745\n",
      "Train Epoch: 35 [46000/49000 (94%)]\tLoss: 1.540183\n",
      "Train Epoch: 35 [47000/49000 (96%)]\tLoss: 1.578205\n",
      "Train Epoch: 35 [48000/49000 (98%)]\tLoss: 1.568753\n",
      "\n",
      "Test set: Avg. loss: 1.5553, Accuracy: 19098/21000 (90.94%)\n",
      "\n",
      "Train Epoch: 36 [0/49000 (0%)]\tLoss: 1.546322\n",
      "Train Epoch: 36 [1000/49000 (2%)]\tLoss: 1.584117\n",
      "Train Epoch: 36 [2000/49000 (4%)]\tLoss: 1.536361\n",
      "Train Epoch: 36 [3000/49000 (6%)]\tLoss: 1.555959\n",
      "Train Epoch: 36 [4000/49000 (8%)]\tLoss: 1.525399\n",
      "Train Epoch: 36 [5000/49000 (10%)]\tLoss: 1.538921\n",
      "Train Epoch: 36 [6000/49000 (12%)]\tLoss: 1.503190\n",
      "Train Epoch: 36 [7000/49000 (14%)]\tLoss: 1.559139\n",
      "Train Epoch: 36 [8000/49000 (16%)]\tLoss: 1.572193\n",
      "Train Epoch: 36 [9000/49000 (18%)]\tLoss: 1.549656\n",
      "Train Epoch: 36 [10000/49000 (20%)]\tLoss: 1.516939\n",
      "Train Epoch: 36 [11000/49000 (22%)]\tLoss: 1.539589\n",
      "Train Epoch: 36 [12000/49000 (24%)]\tLoss: 1.516008\n",
      "Train Epoch: 36 [13000/49000 (27%)]\tLoss: 1.535051\n",
      "Train Epoch: 36 [14000/49000 (29%)]\tLoss: 1.521891\n",
      "Train Epoch: 36 [15000/49000 (31%)]\tLoss: 1.546357\n",
      "Train Epoch: 36 [16000/49000 (33%)]\tLoss: 1.534579\n",
      "Train Epoch: 36 [17000/49000 (35%)]\tLoss: 1.518006\n",
      "Train Epoch: 36 [18000/49000 (37%)]\tLoss: 1.549237\n",
      "Train Epoch: 36 [19000/49000 (39%)]\tLoss: 1.537989\n",
      "Train Epoch: 36 [20000/49000 (41%)]\tLoss: 1.582988\n",
      "Train Epoch: 36 [21000/49000 (43%)]\tLoss: 1.540906\n",
      "Train Epoch: 36 [22000/49000 (45%)]\tLoss: 1.577050\n",
      "Train Epoch: 36 [23000/49000 (47%)]\tLoss: 1.558033\n",
      "Train Epoch: 36 [24000/49000 (49%)]\tLoss: 1.538487\n",
      "Train Epoch: 36 [25000/49000 (51%)]\tLoss: 1.560710\n",
      "Train Epoch: 36 [26000/49000 (53%)]\tLoss: 1.533924\n",
      "Train Epoch: 36 [27000/49000 (55%)]\tLoss: 1.548586\n",
      "Train Epoch: 36 [28000/49000 (57%)]\tLoss: 1.527113\n",
      "Train Epoch: 36 [29000/49000 (59%)]\tLoss: 1.567170\n",
      "Train Epoch: 36 [30000/49000 (61%)]\tLoss: 1.523058\n",
      "Train Epoch: 36 [31000/49000 (63%)]\tLoss: 1.555954\n",
      "Train Epoch: 36 [32000/49000 (65%)]\tLoss: 1.548127\n",
      "Train Epoch: 36 [33000/49000 (67%)]\tLoss: 1.533155\n",
      "Train Epoch: 36 [34000/49000 (69%)]\tLoss: 1.533929\n",
      "Train Epoch: 36 [35000/49000 (71%)]\tLoss: 1.574283\n",
      "Train Epoch: 36 [36000/49000 (73%)]\tLoss: 1.565763\n",
      "Train Epoch: 36 [37000/49000 (76%)]\tLoss: 1.542386\n",
      "Train Epoch: 36 [38000/49000 (78%)]\tLoss: 1.510464\n",
      "Train Epoch: 36 [39000/49000 (80%)]\tLoss: 1.575182\n",
      "Train Epoch: 36 [40000/49000 (82%)]\tLoss: 1.521223\n",
      "Train Epoch: 36 [41000/49000 (84%)]\tLoss: 1.543481\n",
      "Train Epoch: 36 [42000/49000 (86%)]\tLoss: 1.554787\n",
      "Train Epoch: 36 [43000/49000 (88%)]\tLoss: 1.515961\n",
      "Train Epoch: 36 [44000/49000 (90%)]\tLoss: 1.547429\n",
      "Train Epoch: 36 [45000/49000 (92%)]\tLoss: 1.510353\n",
      "Train Epoch: 36 [46000/49000 (94%)]\tLoss: 1.512418\n",
      "Train Epoch: 36 [47000/49000 (96%)]\tLoss: 1.535957\n",
      "Train Epoch: 36 [48000/49000 (98%)]\tLoss: 1.558176\n",
      "\n",
      "Test set: Avg. loss: 1.5537, Accuracy: 19061/21000 (90.77%)\n",
      "\n",
      "Train Epoch: 37 [0/49000 (0%)]\tLoss: 1.549183\n",
      "Train Epoch: 37 [1000/49000 (2%)]\tLoss: 1.537698\n",
      "Train Epoch: 37 [2000/49000 (4%)]\tLoss: 1.538635\n",
      "Train Epoch: 37 [3000/49000 (6%)]\tLoss: 1.532327\n",
      "Train Epoch: 37 [4000/49000 (8%)]\tLoss: 1.543046\n",
      "Train Epoch: 37 [5000/49000 (10%)]\tLoss: 1.554920\n",
      "Train Epoch: 37 [6000/49000 (12%)]\tLoss: 1.532549\n",
      "Train Epoch: 37 [7000/49000 (14%)]\tLoss: 1.567403\n",
      "Train Epoch: 37 [8000/49000 (16%)]\tLoss: 1.539528\n",
      "Train Epoch: 37 [9000/49000 (18%)]\tLoss: 1.553041\n",
      "Train Epoch: 37 [10000/49000 (20%)]\tLoss: 1.593149\n",
      "Train Epoch: 37 [11000/49000 (22%)]\tLoss: 1.577580\n",
      "Train Epoch: 37 [12000/49000 (24%)]\tLoss: 1.563247\n",
      "Train Epoch: 37 [13000/49000 (27%)]\tLoss: 1.546759\n",
      "Train Epoch: 37 [14000/49000 (29%)]\tLoss: 1.522828\n",
      "Train Epoch: 37 [15000/49000 (31%)]\tLoss: 1.531508\n",
      "Train Epoch: 37 [16000/49000 (33%)]\tLoss: 1.550317\n",
      "Train Epoch: 37 [17000/49000 (35%)]\tLoss: 1.530627\n",
      "Train Epoch: 37 [18000/49000 (37%)]\tLoss: 1.546859\n",
      "Train Epoch: 37 [19000/49000 (39%)]\tLoss: 1.578773\n",
      "Train Epoch: 37 [20000/49000 (41%)]\tLoss: 1.540153\n",
      "Train Epoch: 37 [21000/49000 (43%)]\tLoss: 1.541876\n",
      "Train Epoch: 37 [22000/49000 (45%)]\tLoss: 1.582487\n",
      "Train Epoch: 37 [23000/49000 (47%)]\tLoss: 1.566818\n",
      "Train Epoch: 37 [24000/49000 (49%)]\tLoss: 1.543272\n",
      "Train Epoch: 37 [25000/49000 (51%)]\tLoss: 1.542225\n",
      "Train Epoch: 37 [26000/49000 (53%)]\tLoss: 1.557397\n",
      "Train Epoch: 37 [27000/49000 (55%)]\tLoss: 1.544029\n",
      "Train Epoch: 37 [28000/49000 (57%)]\tLoss: 1.511855\n",
      "Train Epoch: 37 [29000/49000 (59%)]\tLoss: 1.546998\n",
      "Train Epoch: 37 [30000/49000 (61%)]\tLoss: 1.529547\n",
      "Train Epoch: 37 [31000/49000 (63%)]\tLoss: 1.516507\n",
      "Train Epoch: 37 [32000/49000 (65%)]\tLoss: 1.567811\n",
      "Train Epoch: 37 [33000/49000 (67%)]\tLoss: 1.537642\n",
      "Train Epoch: 37 [34000/49000 (69%)]\tLoss: 1.545490\n",
      "Train Epoch: 37 [35000/49000 (71%)]\tLoss: 1.576198\n",
      "Train Epoch: 37 [36000/49000 (73%)]\tLoss: 1.542722\n",
      "Train Epoch: 37 [37000/49000 (76%)]\tLoss: 1.518704\n",
      "Train Epoch: 37 [38000/49000 (78%)]\tLoss: 1.532288\n",
      "Train Epoch: 37 [39000/49000 (80%)]\tLoss: 1.551144\n",
      "Train Epoch: 37 [40000/49000 (82%)]\tLoss: 1.525715\n",
      "Train Epoch: 37 [41000/49000 (84%)]\tLoss: 1.520236\n",
      "Train Epoch: 37 [42000/49000 (86%)]\tLoss: 1.542863\n",
      "Train Epoch: 37 [43000/49000 (88%)]\tLoss: 1.535047\n",
      "Train Epoch: 37 [44000/49000 (90%)]\tLoss: 1.583604\n",
      "Train Epoch: 37 [45000/49000 (92%)]\tLoss: 1.547143\n",
      "Train Epoch: 37 [46000/49000 (94%)]\tLoss: 1.591285\n",
      "Train Epoch: 37 [47000/49000 (96%)]\tLoss: 1.530666\n",
      "Train Epoch: 37 [48000/49000 (98%)]\tLoss: 1.546216\n",
      "\n",
      "Test set: Avg. loss: 1.5543, Accuracy: 19046/21000 (90.70%)\n",
      "\n",
      "Train Epoch: 38 [0/49000 (0%)]\tLoss: 1.559130\n",
      "Train Epoch: 38 [1000/49000 (2%)]\tLoss: 1.518478\n",
      "Train Epoch: 38 [2000/49000 (4%)]\tLoss: 1.525520\n",
      "Train Epoch: 38 [3000/49000 (6%)]\tLoss: 1.543956\n",
      "Train Epoch: 38 [4000/49000 (8%)]\tLoss: 1.558456\n",
      "Train Epoch: 38 [5000/49000 (10%)]\tLoss: 1.562515\n",
      "Train Epoch: 38 [6000/49000 (12%)]\tLoss: 1.575026\n",
      "Train Epoch: 38 [7000/49000 (14%)]\tLoss: 1.544232\n",
      "Train Epoch: 38 [8000/49000 (16%)]\tLoss: 1.572578\n",
      "Train Epoch: 38 [9000/49000 (18%)]\tLoss: 1.517467\n",
      "Train Epoch: 38 [10000/49000 (20%)]\tLoss: 1.550794\n",
      "Train Epoch: 38 [11000/49000 (22%)]\tLoss: 1.581893\n",
      "Train Epoch: 38 [12000/49000 (24%)]\tLoss: 1.547302\n",
      "Train Epoch: 38 [13000/49000 (27%)]\tLoss: 1.558299\n",
      "Train Epoch: 38 [14000/49000 (29%)]\tLoss: 1.554627\n",
      "Train Epoch: 38 [15000/49000 (31%)]\tLoss: 1.538574\n",
      "Train Epoch: 38 [16000/49000 (33%)]\tLoss: 1.563266\n",
      "Train Epoch: 38 [17000/49000 (35%)]\tLoss: 1.560548\n",
      "Train Epoch: 38 [18000/49000 (37%)]\tLoss: 1.548242\n",
      "Train Epoch: 38 [19000/49000 (39%)]\tLoss: 1.570120\n",
      "Train Epoch: 38 [20000/49000 (41%)]\tLoss: 1.521435\n",
      "Train Epoch: 38 [21000/49000 (43%)]\tLoss: 1.530012\n",
      "Train Epoch: 38 [22000/49000 (45%)]\tLoss: 1.541409\n",
      "Train Epoch: 38 [23000/49000 (47%)]\tLoss: 1.551503\n",
      "Train Epoch: 38 [24000/49000 (49%)]\tLoss: 1.531718\n",
      "Train Epoch: 38 [25000/49000 (51%)]\tLoss: 1.534971\n",
      "Train Epoch: 38 [26000/49000 (53%)]\tLoss: 1.576278\n",
      "Train Epoch: 38 [27000/49000 (55%)]\tLoss: 1.526561\n",
      "Train Epoch: 38 [28000/49000 (57%)]\tLoss: 1.540796\n",
      "Train Epoch: 38 [29000/49000 (59%)]\tLoss: 1.553002\n",
      "Train Epoch: 38 [30000/49000 (61%)]\tLoss: 1.569965\n",
      "Train Epoch: 38 [31000/49000 (63%)]\tLoss: 1.524908\n",
      "Train Epoch: 38 [32000/49000 (65%)]\tLoss: 1.547254\n",
      "Train Epoch: 38 [33000/49000 (67%)]\tLoss: 1.567027\n",
      "Train Epoch: 38 [34000/49000 (69%)]\tLoss: 1.537326\n",
      "Train Epoch: 38 [35000/49000 (71%)]\tLoss: 1.542074\n",
      "Train Epoch: 38 [36000/49000 (73%)]\tLoss: 1.581243\n",
      "Train Epoch: 38 [37000/49000 (76%)]\tLoss: 1.521773\n",
      "Train Epoch: 38 [38000/49000 (78%)]\tLoss: 1.584049\n",
      "Train Epoch: 38 [39000/49000 (80%)]\tLoss: 1.554640\n",
      "Train Epoch: 38 [40000/49000 (82%)]\tLoss: 1.565354\n",
      "Train Epoch: 38 [41000/49000 (84%)]\tLoss: 1.561632\n",
      "Train Epoch: 38 [42000/49000 (86%)]\tLoss: 1.570383\n",
      "Train Epoch: 38 [43000/49000 (88%)]\tLoss: 1.583836\n",
      "Train Epoch: 38 [44000/49000 (90%)]\tLoss: 1.540679\n",
      "Train Epoch: 38 [45000/49000 (92%)]\tLoss: 1.534266\n",
      "Train Epoch: 38 [46000/49000 (94%)]\tLoss: 1.531414\n",
      "Train Epoch: 38 [47000/49000 (96%)]\tLoss: 1.579014\n",
      "Train Epoch: 38 [48000/49000 (98%)]\tLoss: 1.547326\n",
      "\n",
      "Test set: Avg. loss: 1.5534, Accuracy: 19093/21000 (90.92%)\n",
      "\n",
      "Train Epoch: 39 [0/49000 (0%)]\tLoss: 1.558683\n",
      "Train Epoch: 39 [1000/49000 (2%)]\tLoss: 1.540327\n",
      "Train Epoch: 39 [2000/49000 (4%)]\tLoss: 1.541221\n",
      "Train Epoch: 39 [3000/49000 (6%)]\tLoss: 1.531569\n",
      "Train Epoch: 39 [4000/49000 (8%)]\tLoss: 1.555669\n",
      "Train Epoch: 39 [5000/49000 (10%)]\tLoss: 1.530696\n",
      "Train Epoch: 39 [6000/49000 (12%)]\tLoss: 1.542053\n",
      "Train Epoch: 39 [7000/49000 (14%)]\tLoss: 1.512378\n",
      "Train Epoch: 39 [8000/49000 (16%)]\tLoss: 1.575383\n",
      "Train Epoch: 39 [9000/49000 (18%)]\tLoss: 1.542608\n",
      "Train Epoch: 39 [10000/49000 (20%)]\tLoss: 1.563462\n",
      "Train Epoch: 39 [11000/49000 (22%)]\tLoss: 1.555077\n",
      "Train Epoch: 39 [12000/49000 (24%)]\tLoss: 1.575201\n",
      "Train Epoch: 39 [13000/49000 (27%)]\tLoss: 1.550540\n",
      "Train Epoch: 39 [14000/49000 (29%)]\tLoss: 1.556839\n",
      "Train Epoch: 39 [15000/49000 (31%)]\tLoss: 1.554603\n",
      "Train Epoch: 39 [16000/49000 (33%)]\tLoss: 1.560611\n",
      "Train Epoch: 39 [17000/49000 (35%)]\tLoss: 1.527491\n",
      "Train Epoch: 39 [18000/49000 (37%)]\tLoss: 1.518979\n",
      "Train Epoch: 39 [19000/49000 (39%)]\tLoss: 1.530070\n",
      "Train Epoch: 39 [20000/49000 (41%)]\tLoss: 1.588783\n",
      "Train Epoch: 39 [21000/49000 (43%)]\tLoss: 1.531221\n",
      "Train Epoch: 39 [22000/49000 (45%)]\tLoss: 1.522316\n",
      "Train Epoch: 39 [23000/49000 (47%)]\tLoss: 1.559974\n",
      "Train Epoch: 39 [24000/49000 (49%)]\tLoss: 1.531960\n",
      "Train Epoch: 39 [25000/49000 (51%)]\tLoss: 1.530540\n",
      "Train Epoch: 39 [26000/49000 (53%)]\tLoss: 1.560317\n",
      "Train Epoch: 39 [27000/49000 (55%)]\tLoss: 1.558910\n",
      "Train Epoch: 39 [28000/49000 (57%)]\tLoss: 1.553593\n",
      "Train Epoch: 39 [29000/49000 (59%)]\tLoss: 1.560409\n",
      "Train Epoch: 39 [30000/49000 (61%)]\tLoss: 1.510554\n",
      "Train Epoch: 39 [31000/49000 (63%)]\tLoss: 1.547224\n",
      "Train Epoch: 39 [32000/49000 (65%)]\tLoss: 1.512882\n",
      "Train Epoch: 39 [33000/49000 (67%)]\tLoss: 1.564716\n",
      "Train Epoch: 39 [34000/49000 (69%)]\tLoss: 1.555016\n",
      "Train Epoch: 39 [35000/49000 (71%)]\tLoss: 1.549098\n",
      "Train Epoch: 39 [36000/49000 (73%)]\tLoss: 1.562627\n",
      "Train Epoch: 39 [37000/49000 (76%)]\tLoss: 1.578774\n",
      "Train Epoch: 39 [38000/49000 (78%)]\tLoss: 1.537164\n",
      "Train Epoch: 39 [39000/49000 (80%)]\tLoss: 1.495021\n",
      "Train Epoch: 39 [40000/49000 (82%)]\tLoss: 1.554091\n",
      "Train Epoch: 39 [41000/49000 (84%)]\tLoss: 1.535487\n",
      "Train Epoch: 39 [42000/49000 (86%)]\tLoss: 1.521599\n",
      "Train Epoch: 39 [43000/49000 (88%)]\tLoss: 1.528566\n",
      "Train Epoch: 39 [44000/49000 (90%)]\tLoss: 1.560918\n",
      "Train Epoch: 39 [45000/49000 (92%)]\tLoss: 1.543126\n",
      "Train Epoch: 39 [46000/49000 (94%)]\tLoss: 1.571764\n",
      "Train Epoch: 39 [47000/49000 (96%)]\tLoss: 1.528297\n",
      "Train Epoch: 39 [48000/49000 (98%)]\tLoss: 1.557313\n",
      "\n",
      "Test set: Avg. loss: 1.5530, Accuracy: 19079/21000 (90.85%)\n",
      "\n",
      "Train Epoch: 40 [0/49000 (0%)]\tLoss: 1.530256\n",
      "Train Epoch: 40 [1000/49000 (2%)]\tLoss: 1.594107\n",
      "Train Epoch: 40 [2000/49000 (4%)]\tLoss: 1.535966\n",
      "Train Epoch: 40 [3000/49000 (6%)]\tLoss: 1.572918\n",
      "Train Epoch: 40 [4000/49000 (8%)]\tLoss: 1.538528\n",
      "Train Epoch: 40 [5000/49000 (10%)]\tLoss: 1.579473\n",
      "Train Epoch: 40 [6000/49000 (12%)]\tLoss: 1.556142\n",
      "Train Epoch: 40 [7000/49000 (14%)]\tLoss: 1.554791\n",
      "Train Epoch: 40 [8000/49000 (16%)]\tLoss: 1.553835\n",
      "Train Epoch: 40 [9000/49000 (18%)]\tLoss: 1.535598\n",
      "Train Epoch: 40 [10000/49000 (20%)]\tLoss: 1.522184\n",
      "Train Epoch: 40 [11000/49000 (22%)]\tLoss: 1.521783\n",
      "Train Epoch: 40 [12000/49000 (24%)]\tLoss: 1.504020\n",
      "Train Epoch: 40 [13000/49000 (27%)]\tLoss: 1.525277\n",
      "Train Epoch: 40 [14000/49000 (29%)]\tLoss: 1.554668\n",
      "Train Epoch: 40 [15000/49000 (31%)]\tLoss: 1.570016\n",
      "Train Epoch: 40 [16000/49000 (33%)]\tLoss: 1.535049\n",
      "Train Epoch: 40 [17000/49000 (35%)]\tLoss: 1.557620\n",
      "Train Epoch: 40 [18000/49000 (37%)]\tLoss: 1.560241\n",
      "Train Epoch: 40 [19000/49000 (39%)]\tLoss: 1.509084\n",
      "Train Epoch: 40 [20000/49000 (41%)]\tLoss: 1.542435\n",
      "Train Epoch: 40 [21000/49000 (43%)]\tLoss: 1.552034\n",
      "Train Epoch: 40 [22000/49000 (45%)]\tLoss: 1.556055\n",
      "Train Epoch: 40 [23000/49000 (47%)]\tLoss: 1.579498\n",
      "Train Epoch: 40 [24000/49000 (49%)]\tLoss: 1.529498\n",
      "Train Epoch: 40 [25000/49000 (51%)]\tLoss: 1.575645\n",
      "Train Epoch: 40 [26000/49000 (53%)]\tLoss: 1.555869\n",
      "Train Epoch: 40 [27000/49000 (55%)]\tLoss: 1.537668\n",
      "Train Epoch: 40 [28000/49000 (57%)]\tLoss: 1.537452\n",
      "Train Epoch: 40 [29000/49000 (59%)]\tLoss: 1.546564\n",
      "Train Epoch: 40 [30000/49000 (61%)]\tLoss: 1.525743\n",
      "Train Epoch: 40 [31000/49000 (63%)]\tLoss: 1.563355\n",
      "Train Epoch: 40 [32000/49000 (65%)]\tLoss: 1.543251\n",
      "Train Epoch: 40 [33000/49000 (67%)]\tLoss: 1.509205\n",
      "Train Epoch: 40 [34000/49000 (69%)]\tLoss: 1.537094\n",
      "Train Epoch: 40 [35000/49000 (71%)]\tLoss: 1.544200\n",
      "Train Epoch: 40 [36000/49000 (73%)]\tLoss: 1.534168\n",
      "Train Epoch: 40 [37000/49000 (76%)]\tLoss: 1.520357\n",
      "Train Epoch: 40 [38000/49000 (78%)]\tLoss: 1.543441\n",
      "Train Epoch: 40 [39000/49000 (80%)]\tLoss: 1.525881\n",
      "Train Epoch: 40 [40000/49000 (82%)]\tLoss: 1.551626\n",
      "Train Epoch: 40 [41000/49000 (84%)]\tLoss: 1.529883\n",
      "Train Epoch: 40 [42000/49000 (86%)]\tLoss: 1.554425\n",
      "Train Epoch: 40 [43000/49000 (88%)]\tLoss: 1.572755\n",
      "Train Epoch: 40 [44000/49000 (90%)]\tLoss: 1.586861\n",
      "Train Epoch: 40 [45000/49000 (92%)]\tLoss: 1.534354\n",
      "Train Epoch: 40 [46000/49000 (94%)]\tLoss: 1.520949\n",
      "Train Epoch: 40 [47000/49000 (96%)]\tLoss: 1.545896\n",
      "Train Epoch: 40 [48000/49000 (98%)]\tLoss: 1.573790\n",
      "\n",
      "Test set: Avg. loss: 1.5527, Accuracy: 19077/21000 (90.84%)\n",
      "\n",
      "Train Epoch: 41 [0/49000 (0%)]\tLoss: 1.515344\n",
      "Train Epoch: 41 [1000/49000 (2%)]\tLoss: 1.544553\n",
      "Train Epoch: 41 [2000/49000 (4%)]\tLoss: 1.560732\n",
      "Train Epoch: 41 [3000/49000 (6%)]\tLoss: 1.553725\n",
      "Train Epoch: 41 [4000/49000 (8%)]\tLoss: 1.559747\n",
      "Train Epoch: 41 [5000/49000 (10%)]\tLoss: 1.570146\n",
      "Train Epoch: 41 [6000/49000 (12%)]\tLoss: 1.516793\n",
      "Train Epoch: 41 [7000/49000 (14%)]\tLoss: 1.575486\n",
      "Train Epoch: 41 [8000/49000 (16%)]\tLoss: 1.517398\n",
      "Train Epoch: 41 [9000/49000 (18%)]\tLoss: 1.541426\n",
      "Train Epoch: 41 [10000/49000 (20%)]\tLoss: 1.545240\n",
      "Train Epoch: 41 [11000/49000 (22%)]\tLoss: 1.552094\n",
      "Train Epoch: 41 [12000/49000 (24%)]\tLoss: 1.523083\n",
      "Train Epoch: 41 [13000/49000 (27%)]\tLoss: 1.535885\n",
      "Train Epoch: 41 [14000/49000 (29%)]\tLoss: 1.503185\n",
      "Train Epoch: 41 [15000/49000 (31%)]\tLoss: 1.560120\n",
      "Train Epoch: 41 [16000/49000 (33%)]\tLoss: 1.545811\n",
      "Train Epoch: 41 [17000/49000 (35%)]\tLoss: 1.580485\n",
      "Train Epoch: 41 [18000/49000 (37%)]\tLoss: 1.562870\n",
      "Train Epoch: 41 [19000/49000 (39%)]\tLoss: 1.524768\n",
      "Train Epoch: 41 [20000/49000 (41%)]\tLoss: 1.572497\n",
      "Train Epoch: 41 [21000/49000 (43%)]\tLoss: 1.525671\n",
      "Train Epoch: 41 [22000/49000 (45%)]\tLoss: 1.590172\n",
      "Train Epoch: 41 [23000/49000 (47%)]\tLoss: 1.530848\n",
      "Train Epoch: 41 [24000/49000 (49%)]\tLoss: 1.517583\n",
      "Train Epoch: 41 [25000/49000 (51%)]\tLoss: 1.543761\n",
      "Train Epoch: 41 [26000/49000 (53%)]\tLoss: 1.588755\n",
      "Train Epoch: 41 [27000/49000 (55%)]\tLoss: 1.562035\n",
      "Train Epoch: 41 [28000/49000 (57%)]\tLoss: 1.567284\n",
      "Train Epoch: 41 [29000/49000 (59%)]\tLoss: 1.544160\n",
      "Train Epoch: 41 [30000/49000 (61%)]\tLoss: 1.539339\n",
      "Train Epoch: 41 [31000/49000 (63%)]\tLoss: 1.556119\n",
      "Train Epoch: 41 [32000/49000 (65%)]\tLoss: 1.568715\n",
      "Train Epoch: 41 [33000/49000 (67%)]\tLoss: 1.563790\n",
      "Train Epoch: 41 [34000/49000 (69%)]\tLoss: 1.582565\n",
      "Train Epoch: 41 [35000/49000 (71%)]\tLoss: 1.543221\n",
      "Train Epoch: 41 [36000/49000 (73%)]\tLoss: 1.549084\n",
      "Train Epoch: 41 [37000/49000 (76%)]\tLoss: 1.507511\n",
      "Train Epoch: 41 [38000/49000 (78%)]\tLoss: 1.530603\n",
      "Train Epoch: 41 [39000/49000 (80%)]\tLoss: 1.540588\n",
      "Train Epoch: 41 [40000/49000 (82%)]\tLoss: 1.540360\n",
      "Train Epoch: 41 [41000/49000 (84%)]\tLoss: 1.576755\n",
      "Train Epoch: 41 [42000/49000 (86%)]\tLoss: 1.582000\n",
      "Train Epoch: 41 [43000/49000 (88%)]\tLoss: 1.542384\n",
      "Train Epoch: 41 [44000/49000 (90%)]\tLoss: 1.582539\n",
      "Train Epoch: 41 [45000/49000 (92%)]\tLoss: 1.565987\n",
      "Train Epoch: 41 [46000/49000 (94%)]\tLoss: 1.507380\n",
      "Train Epoch: 41 [47000/49000 (96%)]\tLoss: 1.545701\n",
      "Train Epoch: 41 [48000/49000 (98%)]\tLoss: 1.534610\n",
      "\n",
      "Test set: Avg. loss: 1.5528, Accuracy: 19065/21000 (90.79%)\n",
      "\n",
      "Train Epoch: 42 [0/49000 (0%)]\tLoss: 1.548646\n",
      "Train Epoch: 42 [1000/49000 (2%)]\tLoss: 1.534353\n",
      "Train Epoch: 42 [2000/49000 (4%)]\tLoss: 1.534066\n",
      "Train Epoch: 42 [3000/49000 (6%)]\tLoss: 1.534341\n",
      "Train Epoch: 42 [4000/49000 (8%)]\tLoss: 1.540006\n",
      "Train Epoch: 42 [5000/49000 (10%)]\tLoss: 1.563596\n",
      "Train Epoch: 42 [6000/49000 (12%)]\tLoss: 1.542046\n",
      "Train Epoch: 42 [7000/49000 (14%)]\tLoss: 1.555063\n",
      "Train Epoch: 42 [8000/49000 (16%)]\tLoss: 1.517231\n",
      "Train Epoch: 42 [9000/49000 (18%)]\tLoss: 1.544639\n",
      "Train Epoch: 42 [10000/49000 (20%)]\tLoss: 1.574465\n",
      "Train Epoch: 42 [11000/49000 (22%)]\tLoss: 1.564442\n",
      "Train Epoch: 42 [12000/49000 (24%)]\tLoss: 1.512614\n",
      "Train Epoch: 42 [13000/49000 (27%)]\tLoss: 1.542781\n",
      "Train Epoch: 42 [14000/49000 (29%)]\tLoss: 1.528913\n",
      "Train Epoch: 42 [15000/49000 (31%)]\tLoss: 1.538052\n",
      "Train Epoch: 42 [16000/49000 (33%)]\tLoss: 1.501453\n",
      "Train Epoch: 42 [17000/49000 (35%)]\tLoss: 1.565205\n",
      "Train Epoch: 42 [18000/49000 (37%)]\tLoss: 1.527793\n",
      "Train Epoch: 42 [19000/49000 (39%)]\tLoss: 1.535568\n",
      "Train Epoch: 42 [20000/49000 (41%)]\tLoss: 1.543006\n",
      "Train Epoch: 42 [21000/49000 (43%)]\tLoss: 1.535971\n",
      "Train Epoch: 42 [22000/49000 (45%)]\tLoss: 1.543336\n",
      "Train Epoch: 42 [23000/49000 (47%)]\tLoss: 1.572874\n",
      "Train Epoch: 42 [24000/49000 (49%)]\tLoss: 1.496713\n",
      "Train Epoch: 42 [25000/49000 (51%)]\tLoss: 1.537263\n",
      "Train Epoch: 42 [26000/49000 (53%)]\tLoss: 1.599229\n",
      "Train Epoch: 42 [27000/49000 (55%)]\tLoss: 1.523273\n",
      "Train Epoch: 42 [28000/49000 (57%)]\tLoss: 1.519844\n",
      "Train Epoch: 42 [29000/49000 (59%)]\tLoss: 1.520694\n",
      "Train Epoch: 42 [30000/49000 (61%)]\tLoss: 1.545539\n",
      "Train Epoch: 42 [31000/49000 (63%)]\tLoss: 1.530618\n",
      "Train Epoch: 42 [32000/49000 (65%)]\tLoss: 1.487276\n",
      "Train Epoch: 42 [33000/49000 (67%)]\tLoss: 1.543926\n",
      "Train Epoch: 42 [34000/49000 (69%)]\tLoss: 1.556960\n",
      "Train Epoch: 42 [35000/49000 (71%)]\tLoss: 1.563202\n",
      "Train Epoch: 42 [36000/49000 (73%)]\tLoss: 1.558433\n",
      "Train Epoch: 42 [37000/49000 (76%)]\tLoss: 1.549098\n",
      "Train Epoch: 42 [38000/49000 (78%)]\tLoss: 1.549379\n",
      "Train Epoch: 42 [39000/49000 (80%)]\tLoss: 1.529812\n",
      "Train Epoch: 42 [40000/49000 (82%)]\tLoss: 1.537635\n",
      "Train Epoch: 42 [41000/49000 (84%)]\tLoss: 1.581265\n",
      "Train Epoch: 42 [42000/49000 (86%)]\tLoss: 1.570168\n",
      "Train Epoch: 42 [43000/49000 (88%)]\tLoss: 1.573703\n",
      "Train Epoch: 42 [44000/49000 (90%)]\tLoss: 1.590650\n",
      "Train Epoch: 42 [45000/49000 (92%)]\tLoss: 1.571640\n",
      "Train Epoch: 42 [46000/49000 (94%)]\tLoss: 1.551828\n",
      "Train Epoch: 42 [47000/49000 (96%)]\tLoss: 1.544041\n",
      "Train Epoch: 42 [48000/49000 (98%)]\tLoss: 1.543169\n",
      "\n",
      "Test set: Avg. loss: 1.5523, Accuracy: 19114/21000 (91.02%)\n",
      "\n",
      "Train Epoch: 43 [0/49000 (0%)]\tLoss: 1.560320\n",
      "Train Epoch: 43 [1000/49000 (2%)]\tLoss: 1.529640\n",
      "Train Epoch: 43 [2000/49000 (4%)]\tLoss: 1.567587\n",
      "Train Epoch: 43 [3000/49000 (6%)]\tLoss: 1.529695\n",
      "Train Epoch: 43 [4000/49000 (8%)]\tLoss: 1.532792\n",
      "Train Epoch: 43 [5000/49000 (10%)]\tLoss: 1.540766\n",
      "Train Epoch: 43 [6000/49000 (12%)]\tLoss: 1.545013\n",
      "Train Epoch: 43 [7000/49000 (14%)]\tLoss: 1.585090\n",
      "Train Epoch: 43 [8000/49000 (16%)]\tLoss: 1.521324\n",
      "Train Epoch: 43 [9000/49000 (18%)]\tLoss: 1.540398\n",
      "Train Epoch: 43 [10000/49000 (20%)]\tLoss: 1.564427\n",
      "Train Epoch: 43 [11000/49000 (22%)]\tLoss: 1.529858\n",
      "Train Epoch: 43 [12000/49000 (24%)]\tLoss: 1.584480\n",
      "Train Epoch: 43 [13000/49000 (27%)]\tLoss: 1.550560\n",
      "Train Epoch: 43 [14000/49000 (29%)]\tLoss: 1.555311\n",
      "Train Epoch: 43 [15000/49000 (31%)]\tLoss: 1.556566\n",
      "Train Epoch: 43 [16000/49000 (33%)]\tLoss: 1.553131\n",
      "Train Epoch: 43 [17000/49000 (35%)]\tLoss: 1.519789\n",
      "Train Epoch: 43 [18000/49000 (37%)]\tLoss: 1.530149\n",
      "Train Epoch: 43 [19000/49000 (39%)]\tLoss: 1.532109\n",
      "Train Epoch: 43 [20000/49000 (41%)]\tLoss: 1.545147\n",
      "Train Epoch: 43 [21000/49000 (43%)]\tLoss: 1.550405\n",
      "Train Epoch: 43 [22000/49000 (45%)]\tLoss: 1.570666\n",
      "Train Epoch: 43 [23000/49000 (47%)]\tLoss: 1.566276\n",
      "Train Epoch: 43 [24000/49000 (49%)]\tLoss: 1.517971\n",
      "Train Epoch: 43 [25000/49000 (51%)]\tLoss: 1.535653\n",
      "Train Epoch: 43 [26000/49000 (53%)]\tLoss: 1.533142\n",
      "Train Epoch: 43 [27000/49000 (55%)]\tLoss: 1.517788\n",
      "Train Epoch: 43 [28000/49000 (57%)]\tLoss: 1.569261\n",
      "Train Epoch: 43 [29000/49000 (59%)]\tLoss: 1.512985\n",
      "Train Epoch: 43 [30000/49000 (61%)]\tLoss: 1.585842\n",
      "Train Epoch: 43 [31000/49000 (63%)]\tLoss: 1.553069\n",
      "Train Epoch: 43 [32000/49000 (65%)]\tLoss: 1.560266\n",
      "Train Epoch: 43 [33000/49000 (67%)]\tLoss: 1.540002\n",
      "Train Epoch: 43 [34000/49000 (69%)]\tLoss: 1.534171\n",
      "Train Epoch: 43 [35000/49000 (71%)]\tLoss: 1.524995\n",
      "Train Epoch: 43 [36000/49000 (73%)]\tLoss: 1.537111\n",
      "Train Epoch: 43 [37000/49000 (76%)]\tLoss: 1.500903\n",
      "Train Epoch: 43 [38000/49000 (78%)]\tLoss: 1.561156\n",
      "Train Epoch: 43 [39000/49000 (80%)]\tLoss: 1.577589\n",
      "Train Epoch: 43 [40000/49000 (82%)]\tLoss: 1.577982\n",
      "Train Epoch: 43 [41000/49000 (84%)]\tLoss: 1.555632\n",
      "Train Epoch: 43 [42000/49000 (86%)]\tLoss: 1.513374\n",
      "Train Epoch: 43 [43000/49000 (88%)]\tLoss: 1.529171\n",
      "Train Epoch: 43 [44000/49000 (90%)]\tLoss: 1.523475\n",
      "Train Epoch: 43 [45000/49000 (92%)]\tLoss: 1.519801\n",
      "Train Epoch: 43 [46000/49000 (94%)]\tLoss: 1.548322\n",
      "Train Epoch: 43 [47000/49000 (96%)]\tLoss: 1.534745\n",
      "Train Epoch: 43 [48000/49000 (98%)]\tLoss: 1.530982\n",
      "\n",
      "Test set: Avg. loss: 1.5552, Accuracy: 19131/21000 (91.10%)\n",
      "\n",
      "Train Epoch: 44 [0/49000 (0%)]\tLoss: 1.523604\n",
      "Train Epoch: 44 [1000/49000 (2%)]\tLoss: 1.529546\n",
      "Train Epoch: 44 [2000/49000 (4%)]\tLoss: 1.555326\n",
      "Train Epoch: 44 [3000/49000 (6%)]\tLoss: 1.525595\n",
      "Train Epoch: 44 [4000/49000 (8%)]\tLoss: 1.516365\n",
      "Train Epoch: 44 [5000/49000 (10%)]\tLoss: 1.534449\n",
      "Train Epoch: 44 [6000/49000 (12%)]\tLoss: 1.512285\n",
      "Train Epoch: 44 [7000/49000 (14%)]\tLoss: 1.513389\n",
      "Train Epoch: 44 [8000/49000 (16%)]\tLoss: 1.564430\n",
      "Train Epoch: 44 [9000/49000 (18%)]\tLoss: 1.540903\n",
      "Train Epoch: 44 [10000/49000 (20%)]\tLoss: 1.510970\n",
      "Train Epoch: 44 [11000/49000 (22%)]\tLoss: 1.559748\n",
      "Train Epoch: 44 [12000/49000 (24%)]\tLoss: 1.556102\n",
      "Train Epoch: 44 [13000/49000 (27%)]\tLoss: 1.517469\n",
      "Train Epoch: 44 [14000/49000 (29%)]\tLoss: 1.558457\n",
      "Train Epoch: 44 [15000/49000 (31%)]\tLoss: 1.547955\n",
      "Train Epoch: 44 [16000/49000 (33%)]\tLoss: 1.573144\n",
      "Train Epoch: 44 [17000/49000 (35%)]\tLoss: 1.562197\n",
      "Train Epoch: 44 [18000/49000 (37%)]\tLoss: 1.538371\n",
      "Train Epoch: 44 [19000/49000 (39%)]\tLoss: 1.555687\n",
      "Train Epoch: 44 [20000/49000 (41%)]\tLoss: 1.552202\n",
      "Train Epoch: 44 [21000/49000 (43%)]\tLoss: 1.572814\n",
      "Train Epoch: 44 [22000/49000 (45%)]\tLoss: 1.569048\n",
      "Train Epoch: 44 [23000/49000 (47%)]\tLoss: 1.552502\n",
      "Train Epoch: 44 [24000/49000 (49%)]\tLoss: 1.518262\n",
      "Train Epoch: 44 [25000/49000 (51%)]\tLoss: 1.603544\n",
      "Train Epoch: 44 [26000/49000 (53%)]\tLoss: 1.523935\n",
      "Train Epoch: 44 [27000/49000 (55%)]\tLoss: 1.554415\n",
      "Train Epoch: 44 [28000/49000 (57%)]\tLoss: 1.547076\n",
      "Train Epoch: 44 [29000/49000 (59%)]\tLoss: 1.542279\n",
      "Train Epoch: 44 [30000/49000 (61%)]\tLoss: 1.527351\n",
      "Train Epoch: 44 [31000/49000 (63%)]\tLoss: 1.542610\n",
      "Train Epoch: 44 [32000/49000 (65%)]\tLoss: 1.510002\n",
      "Train Epoch: 44 [33000/49000 (67%)]\tLoss: 1.564864\n",
      "Train Epoch: 44 [34000/49000 (69%)]\tLoss: 1.557739\n",
      "Train Epoch: 44 [35000/49000 (71%)]\tLoss: 1.518395\n",
      "Train Epoch: 44 [36000/49000 (73%)]\tLoss: 1.537673\n",
      "Train Epoch: 44 [37000/49000 (76%)]\tLoss: 1.585348\n",
      "Train Epoch: 44 [38000/49000 (78%)]\tLoss: 1.567610\n",
      "Train Epoch: 44 [39000/49000 (80%)]\tLoss: 1.571089\n",
      "Train Epoch: 44 [40000/49000 (82%)]\tLoss: 1.525476\n",
      "Train Epoch: 44 [41000/49000 (84%)]\tLoss: 1.538195\n",
      "Train Epoch: 44 [42000/49000 (86%)]\tLoss: 1.568214\n",
      "Train Epoch: 44 [43000/49000 (88%)]\tLoss: 1.534279\n",
      "Train Epoch: 44 [44000/49000 (90%)]\tLoss: 1.545560\n",
      "Train Epoch: 44 [45000/49000 (92%)]\tLoss: 1.563089\n",
      "Train Epoch: 44 [46000/49000 (94%)]\tLoss: 1.546197\n",
      "Train Epoch: 44 [47000/49000 (96%)]\tLoss: 1.530981\n",
      "Train Epoch: 44 [48000/49000 (98%)]\tLoss: 1.519004\n",
      "\n",
      "Test set: Avg. loss: 1.5532, Accuracy: 19115/21000 (91.02%)\n",
      "\n",
      "Train Epoch: 45 [0/49000 (0%)]\tLoss: 1.522291\n",
      "Train Epoch: 45 [1000/49000 (2%)]\tLoss: 1.575534\n",
      "Train Epoch: 45 [2000/49000 (4%)]\tLoss: 1.575843\n",
      "Train Epoch: 45 [3000/49000 (6%)]\tLoss: 1.549500\n",
      "Train Epoch: 45 [4000/49000 (8%)]\tLoss: 1.537629\n",
      "Train Epoch: 45 [5000/49000 (10%)]\tLoss: 1.562077\n",
      "Train Epoch: 45 [6000/49000 (12%)]\tLoss: 1.522193\n",
      "Train Epoch: 45 [7000/49000 (14%)]\tLoss: 1.556974\n",
      "Train Epoch: 45 [8000/49000 (16%)]\tLoss: 1.536506\n",
      "Train Epoch: 45 [9000/49000 (18%)]\tLoss: 1.533196\n",
      "Train Epoch: 45 [10000/49000 (20%)]\tLoss: 1.543261\n",
      "Train Epoch: 45 [11000/49000 (22%)]\tLoss: 1.553411\n",
      "Train Epoch: 45 [12000/49000 (24%)]\tLoss: 1.555132\n",
      "Train Epoch: 45 [13000/49000 (27%)]\tLoss: 1.524601\n",
      "Train Epoch: 45 [14000/49000 (29%)]\tLoss: 1.549280\n",
      "Train Epoch: 45 [15000/49000 (31%)]\tLoss: 1.541263\n",
      "Train Epoch: 45 [16000/49000 (33%)]\tLoss: 1.510466\n",
      "Train Epoch: 45 [17000/49000 (35%)]\tLoss: 1.541586\n",
      "Train Epoch: 45 [18000/49000 (37%)]\tLoss: 1.548025\n",
      "Train Epoch: 45 [19000/49000 (39%)]\tLoss: 1.568864\n",
      "Train Epoch: 45 [20000/49000 (41%)]\tLoss: 1.577760\n",
      "Train Epoch: 45 [21000/49000 (43%)]\tLoss: 1.553085\n",
      "Train Epoch: 45 [22000/49000 (45%)]\tLoss: 1.529910\n",
      "Train Epoch: 45 [23000/49000 (47%)]\tLoss: 1.519156\n",
      "Train Epoch: 45 [24000/49000 (49%)]\tLoss: 1.518134\n",
      "Train Epoch: 45 [25000/49000 (51%)]\tLoss: 1.577448\n",
      "Train Epoch: 45 [26000/49000 (53%)]\tLoss: 1.520920\n",
      "Train Epoch: 45 [27000/49000 (55%)]\tLoss: 1.551791\n",
      "Train Epoch: 45 [28000/49000 (57%)]\tLoss: 1.517289\n",
      "Train Epoch: 45 [29000/49000 (59%)]\tLoss: 1.548922\n",
      "Train Epoch: 45 [30000/49000 (61%)]\tLoss: 1.523268\n",
      "Train Epoch: 45 [31000/49000 (63%)]\tLoss: 1.563856\n",
      "Train Epoch: 45 [32000/49000 (65%)]\tLoss: 1.573401\n",
      "Train Epoch: 45 [33000/49000 (67%)]\tLoss: 1.517121\n",
      "Train Epoch: 45 [34000/49000 (69%)]\tLoss: 1.501456\n",
      "Train Epoch: 45 [35000/49000 (71%)]\tLoss: 1.533187\n",
      "Train Epoch: 45 [36000/49000 (73%)]\tLoss: 1.564211\n",
      "Train Epoch: 45 [37000/49000 (76%)]\tLoss: 1.574517\n",
      "Train Epoch: 45 [38000/49000 (78%)]\tLoss: 1.561403\n",
      "Train Epoch: 45 [39000/49000 (80%)]\tLoss: 1.541324\n",
      "Train Epoch: 45 [40000/49000 (82%)]\tLoss: 1.518301\n",
      "Train Epoch: 45 [41000/49000 (84%)]\tLoss: 1.498491\n",
      "Train Epoch: 45 [42000/49000 (86%)]\tLoss: 1.550860\n",
      "Train Epoch: 45 [43000/49000 (88%)]\tLoss: 1.544088\n",
      "Train Epoch: 45 [44000/49000 (90%)]\tLoss: 1.526588\n",
      "Train Epoch: 45 [45000/49000 (92%)]\tLoss: 1.543457\n",
      "Train Epoch: 45 [46000/49000 (94%)]\tLoss: 1.528880\n",
      "Train Epoch: 45 [47000/49000 (96%)]\tLoss: 1.553258\n",
      "Train Epoch: 45 [48000/49000 (98%)]\tLoss: 1.565312\n",
      "\n",
      "Test set: Avg. loss: 1.5523, Accuracy: 19096/21000 (90.93%)\n",
      "\n",
      "Train Epoch: 46 [0/49000 (0%)]\tLoss: 1.534729\n",
      "Train Epoch: 46 [1000/49000 (2%)]\tLoss: 1.514796\n",
      "Train Epoch: 46 [2000/49000 (4%)]\tLoss: 1.599475\n",
      "Train Epoch: 46 [3000/49000 (6%)]\tLoss: 1.569176\n",
      "Train Epoch: 46 [4000/49000 (8%)]\tLoss: 1.563764\n",
      "Train Epoch: 46 [5000/49000 (10%)]\tLoss: 1.529699\n",
      "Train Epoch: 46 [6000/49000 (12%)]\tLoss: 1.591822\n",
      "Train Epoch: 46 [7000/49000 (14%)]\tLoss: 1.515259\n",
      "Train Epoch: 46 [8000/49000 (16%)]\tLoss: 1.529736\n",
      "Train Epoch: 46 [9000/49000 (18%)]\tLoss: 1.547591\n",
      "Train Epoch: 46 [10000/49000 (20%)]\tLoss: 1.539265\n",
      "Train Epoch: 46 [11000/49000 (22%)]\tLoss: 1.563461\n",
      "Train Epoch: 46 [12000/49000 (24%)]\tLoss: 1.517858\n",
      "Train Epoch: 46 [13000/49000 (27%)]\tLoss: 1.569544\n",
      "Train Epoch: 46 [14000/49000 (29%)]\tLoss: 1.522145\n",
      "Train Epoch: 46 [15000/49000 (31%)]\tLoss: 1.537272\n",
      "Train Epoch: 46 [16000/49000 (33%)]\tLoss: 1.544770\n",
      "Train Epoch: 46 [17000/49000 (35%)]\tLoss: 1.553461\n",
      "Train Epoch: 46 [18000/49000 (37%)]\tLoss: 1.526992\n",
      "Train Epoch: 46 [19000/49000 (39%)]\tLoss: 1.540439\n",
      "Train Epoch: 46 [20000/49000 (41%)]\tLoss: 1.516812\n",
      "Train Epoch: 46 [21000/49000 (43%)]\tLoss: 1.596232\n",
      "Train Epoch: 46 [22000/49000 (45%)]\tLoss: 1.504561\n",
      "Train Epoch: 46 [23000/49000 (47%)]\tLoss: 1.514876\n",
      "Train Epoch: 46 [24000/49000 (49%)]\tLoss: 1.558946\n",
      "Train Epoch: 46 [25000/49000 (51%)]\tLoss: 1.558097\n",
      "Train Epoch: 46 [26000/49000 (53%)]\tLoss: 1.540692\n",
      "Train Epoch: 46 [27000/49000 (55%)]\tLoss: 1.558273\n",
      "Train Epoch: 46 [28000/49000 (57%)]\tLoss: 1.502499\n",
      "Train Epoch: 46 [29000/49000 (59%)]\tLoss: 1.514199\n",
      "Train Epoch: 46 [30000/49000 (61%)]\tLoss: 1.513643\n",
      "Train Epoch: 46 [31000/49000 (63%)]\tLoss: 1.565262\n",
      "Train Epoch: 46 [32000/49000 (65%)]\tLoss: 1.532724\n",
      "Train Epoch: 46 [33000/49000 (67%)]\tLoss: 1.538263\n",
      "Train Epoch: 46 [34000/49000 (69%)]\tLoss: 1.522936\n",
      "Train Epoch: 46 [35000/49000 (71%)]\tLoss: 1.523028\n",
      "Train Epoch: 46 [36000/49000 (73%)]\tLoss: 1.547495\n",
      "Train Epoch: 46 [37000/49000 (76%)]\tLoss: 1.559078\n",
      "Train Epoch: 46 [38000/49000 (78%)]\tLoss: 1.513262\n",
      "Train Epoch: 46 [39000/49000 (80%)]\tLoss: 1.527661\n",
      "Train Epoch: 46 [40000/49000 (82%)]\tLoss: 1.523345\n",
      "Train Epoch: 46 [41000/49000 (84%)]\tLoss: 1.508725\n",
      "Train Epoch: 46 [42000/49000 (86%)]\tLoss: 1.537516\n",
      "Train Epoch: 46 [43000/49000 (88%)]\tLoss: 1.535200\n",
      "Train Epoch: 46 [44000/49000 (90%)]\tLoss: 1.555883\n",
      "Train Epoch: 46 [45000/49000 (92%)]\tLoss: 1.555053\n",
      "Train Epoch: 46 [46000/49000 (94%)]\tLoss: 1.533009\n",
      "Train Epoch: 46 [47000/49000 (96%)]\tLoss: 1.599526\n",
      "Train Epoch: 46 [48000/49000 (98%)]\tLoss: 1.557305\n",
      "\n",
      "Test set: Avg. loss: 1.5526, Accuracy: 19099/21000 (90.95%)\n",
      "\n",
      "Train Epoch: 47 [0/49000 (0%)]\tLoss: 1.504682\n",
      "Train Epoch: 47 [1000/49000 (2%)]\tLoss: 1.558179\n",
      "Train Epoch: 47 [2000/49000 (4%)]\tLoss: 1.506259\n",
      "Train Epoch: 47 [3000/49000 (6%)]\tLoss: 1.534923\n",
      "Train Epoch: 47 [4000/49000 (8%)]\tLoss: 1.550768\n",
      "Train Epoch: 47 [5000/49000 (10%)]\tLoss: 1.574753\n",
      "Train Epoch: 47 [6000/49000 (12%)]\tLoss: 1.562609\n",
      "Train Epoch: 47 [7000/49000 (14%)]\tLoss: 1.589451\n",
      "Train Epoch: 47 [8000/49000 (16%)]\tLoss: 1.572535\n",
      "Train Epoch: 47 [9000/49000 (18%)]\tLoss: 1.562106\n",
      "Train Epoch: 47 [10000/49000 (20%)]\tLoss: 1.546232\n",
      "Train Epoch: 47 [11000/49000 (22%)]\tLoss: 1.529708\n",
      "Train Epoch: 47 [12000/49000 (24%)]\tLoss: 1.543417\n",
      "Train Epoch: 47 [13000/49000 (27%)]\tLoss: 1.551145\n",
      "Train Epoch: 47 [14000/49000 (29%)]\tLoss: 1.526798\n",
      "Train Epoch: 47 [15000/49000 (31%)]\tLoss: 1.534203\n",
      "Train Epoch: 47 [16000/49000 (33%)]\tLoss: 1.555624\n",
      "Train Epoch: 47 [17000/49000 (35%)]\tLoss: 1.604826\n",
      "Train Epoch: 47 [18000/49000 (37%)]\tLoss: 1.532728\n",
      "Train Epoch: 47 [19000/49000 (39%)]\tLoss: 1.570399\n",
      "Train Epoch: 47 [20000/49000 (41%)]\tLoss: 1.536979\n",
      "Train Epoch: 47 [21000/49000 (43%)]\tLoss: 1.541466\n",
      "Train Epoch: 47 [22000/49000 (45%)]\tLoss: 1.567711\n",
      "Train Epoch: 47 [23000/49000 (47%)]\tLoss: 1.534095\n",
      "Train Epoch: 47 [24000/49000 (49%)]\tLoss: 1.519165\n",
      "Train Epoch: 47 [25000/49000 (51%)]\tLoss: 1.555361\n",
      "Train Epoch: 47 [26000/49000 (53%)]\tLoss: 1.557051\n",
      "Train Epoch: 47 [27000/49000 (55%)]\tLoss: 1.547301\n",
      "Train Epoch: 47 [28000/49000 (57%)]\tLoss: 1.548443\n",
      "Train Epoch: 47 [29000/49000 (59%)]\tLoss: 1.552038\n",
      "Train Epoch: 47 [30000/49000 (61%)]\tLoss: 1.511042\n",
      "Train Epoch: 47 [31000/49000 (63%)]\tLoss: 1.563106\n",
      "Train Epoch: 47 [32000/49000 (65%)]\tLoss: 1.559503\n",
      "Train Epoch: 47 [33000/49000 (67%)]\tLoss: 1.547175\n",
      "Train Epoch: 47 [34000/49000 (69%)]\tLoss: 1.507945\n",
      "Train Epoch: 47 [35000/49000 (71%)]\tLoss: 1.549091\n",
      "Train Epoch: 47 [36000/49000 (73%)]\tLoss: 1.511965\n",
      "Train Epoch: 47 [37000/49000 (76%)]\tLoss: 1.585856\n",
      "Train Epoch: 47 [38000/49000 (78%)]\tLoss: 1.549147\n",
      "Train Epoch: 47 [39000/49000 (80%)]\tLoss: 1.513013\n",
      "Train Epoch: 47 [40000/49000 (82%)]\tLoss: 1.584736\n",
      "Train Epoch: 47 [41000/49000 (84%)]\tLoss: 1.544615\n",
      "Train Epoch: 47 [42000/49000 (86%)]\tLoss: 1.521796\n",
      "Train Epoch: 47 [43000/49000 (88%)]\tLoss: 1.526716\n",
      "Train Epoch: 47 [44000/49000 (90%)]\tLoss: 1.557808\n",
      "Train Epoch: 47 [45000/49000 (92%)]\tLoss: 1.583489\n",
      "Train Epoch: 47 [46000/49000 (94%)]\tLoss: 1.543879\n",
      "Train Epoch: 47 [47000/49000 (96%)]\tLoss: 1.520589\n",
      "Train Epoch: 47 [48000/49000 (98%)]\tLoss: 1.520327\n",
      "\n",
      "Test set: Avg. loss: 1.5528, Accuracy: 19109/21000 (91.00%)\n",
      "\n",
      "Train Epoch: 48 [0/49000 (0%)]\tLoss: 1.514532\n",
      "Train Epoch: 48 [1000/49000 (2%)]\tLoss: 1.556262\n",
      "Train Epoch: 48 [2000/49000 (4%)]\tLoss: 1.560490\n",
      "Train Epoch: 48 [3000/49000 (6%)]\tLoss: 1.545412\n",
      "Train Epoch: 48 [4000/49000 (8%)]\tLoss: 1.501155\n",
      "Train Epoch: 48 [5000/49000 (10%)]\tLoss: 1.564266\n",
      "Train Epoch: 48 [6000/49000 (12%)]\tLoss: 1.515709\n",
      "Train Epoch: 48 [7000/49000 (14%)]\tLoss: 1.588792\n",
      "Train Epoch: 48 [8000/49000 (16%)]\tLoss: 1.559362\n",
      "Train Epoch: 48 [9000/49000 (18%)]\tLoss: 1.550197\n",
      "Train Epoch: 48 [10000/49000 (20%)]\tLoss: 1.513291\n",
      "Train Epoch: 48 [11000/49000 (22%)]\tLoss: 1.546151\n",
      "Train Epoch: 48 [12000/49000 (24%)]\tLoss: 1.547379\n",
      "Train Epoch: 48 [13000/49000 (27%)]\tLoss: 1.544676\n",
      "Train Epoch: 48 [14000/49000 (29%)]\tLoss: 1.543737\n",
      "Train Epoch: 48 [15000/49000 (31%)]\tLoss: 1.551569\n",
      "Train Epoch: 48 [16000/49000 (33%)]\tLoss: 1.533307\n",
      "Train Epoch: 48 [17000/49000 (35%)]\tLoss: 1.534715\n",
      "Train Epoch: 48 [18000/49000 (37%)]\tLoss: 1.498692\n",
      "Train Epoch: 48 [19000/49000 (39%)]\tLoss: 1.546368\n",
      "Train Epoch: 48 [20000/49000 (41%)]\tLoss: 1.537495\n",
      "Train Epoch: 48 [21000/49000 (43%)]\tLoss: 1.590098\n",
      "Train Epoch: 48 [22000/49000 (45%)]\tLoss: 1.536194\n",
      "Train Epoch: 48 [23000/49000 (47%)]\tLoss: 1.513999\n",
      "Train Epoch: 48 [24000/49000 (49%)]\tLoss: 1.514782\n",
      "Train Epoch: 48 [25000/49000 (51%)]\tLoss: 1.525619\n",
      "Train Epoch: 48 [26000/49000 (53%)]\tLoss: 1.530825\n",
      "Train Epoch: 48 [27000/49000 (55%)]\tLoss: 1.520333\n",
      "Train Epoch: 48 [28000/49000 (57%)]\tLoss: 1.538484\n",
      "Train Epoch: 48 [29000/49000 (59%)]\tLoss: 1.521794\n",
      "Train Epoch: 48 [30000/49000 (61%)]\tLoss: 1.523383\n",
      "Train Epoch: 48 [31000/49000 (63%)]\tLoss: 1.521401\n",
      "Train Epoch: 48 [32000/49000 (65%)]\tLoss: 1.573764\n",
      "Train Epoch: 48 [33000/49000 (67%)]\tLoss: 1.529114\n",
      "Train Epoch: 48 [34000/49000 (69%)]\tLoss: 1.549742\n",
      "Train Epoch: 48 [35000/49000 (71%)]\tLoss: 1.577000\n",
      "Train Epoch: 48 [36000/49000 (73%)]\tLoss: 1.534776\n",
      "Train Epoch: 48 [37000/49000 (76%)]\tLoss: 1.531497\n",
      "Train Epoch: 48 [38000/49000 (78%)]\tLoss: 1.542211\n",
      "Train Epoch: 48 [39000/49000 (80%)]\tLoss: 1.577010\n",
      "Train Epoch: 48 [40000/49000 (82%)]\tLoss: 1.550256\n",
      "Train Epoch: 48 [41000/49000 (84%)]\tLoss: 1.546113\n",
      "Train Epoch: 48 [42000/49000 (86%)]\tLoss: 1.529734\n",
      "Train Epoch: 48 [43000/49000 (88%)]\tLoss: 1.541429\n",
      "Train Epoch: 48 [44000/49000 (90%)]\tLoss: 1.537295\n",
      "Train Epoch: 48 [45000/49000 (92%)]\tLoss: 1.536440\n",
      "Train Epoch: 48 [46000/49000 (94%)]\tLoss: 1.559212\n",
      "Train Epoch: 48 [47000/49000 (96%)]\tLoss: 1.505126\n",
      "Train Epoch: 48 [48000/49000 (98%)]\tLoss: 1.553013\n",
      "\n",
      "Test set: Avg. loss: 1.5518, Accuracy: 19064/21000 (90.78%)\n",
      "\n",
      "Train Epoch: 49 [0/49000 (0%)]\tLoss: 1.567559\n",
      "Train Epoch: 49 [1000/49000 (2%)]\tLoss: 1.519334\n",
      "Train Epoch: 49 [2000/49000 (4%)]\tLoss: 1.522946\n",
      "Train Epoch: 49 [3000/49000 (6%)]\tLoss: 1.537226\n",
      "Train Epoch: 49 [4000/49000 (8%)]\tLoss: 1.522050\n",
      "Train Epoch: 49 [5000/49000 (10%)]\tLoss: 1.545564\n",
      "Train Epoch: 49 [6000/49000 (12%)]\tLoss: 1.532057\n",
      "Train Epoch: 49 [7000/49000 (14%)]\tLoss: 1.520772\n",
      "Train Epoch: 49 [8000/49000 (16%)]\tLoss: 1.544123\n",
      "Train Epoch: 49 [9000/49000 (18%)]\tLoss: 1.529890\n",
      "Train Epoch: 49 [10000/49000 (20%)]\tLoss: 1.559837\n",
      "Train Epoch: 49 [11000/49000 (22%)]\tLoss: 1.513121\n",
      "Train Epoch: 49 [12000/49000 (24%)]\tLoss: 1.519753\n",
      "Train Epoch: 49 [13000/49000 (27%)]\tLoss: 1.525042\n",
      "Train Epoch: 49 [14000/49000 (29%)]\tLoss: 1.527924\n",
      "Train Epoch: 49 [15000/49000 (31%)]\tLoss: 1.543154\n",
      "Train Epoch: 49 [16000/49000 (33%)]\tLoss: 1.563974\n",
      "Train Epoch: 49 [17000/49000 (35%)]\tLoss: 1.540317\n",
      "Train Epoch: 49 [18000/49000 (37%)]\tLoss: 1.526011\n",
      "Train Epoch: 49 [19000/49000 (39%)]\tLoss: 1.598787\n",
      "Train Epoch: 49 [20000/49000 (41%)]\tLoss: 1.518248\n",
      "Train Epoch: 49 [21000/49000 (43%)]\tLoss: 1.531794\n",
      "Train Epoch: 49 [22000/49000 (45%)]\tLoss: 1.525432\n",
      "Train Epoch: 49 [23000/49000 (47%)]\tLoss: 1.598786\n",
      "Train Epoch: 49 [24000/49000 (49%)]\tLoss: 1.539769\n",
      "Train Epoch: 49 [25000/49000 (51%)]\tLoss: 1.527659\n",
      "Train Epoch: 49 [26000/49000 (53%)]\tLoss: 1.560478\n",
      "Train Epoch: 49 [27000/49000 (55%)]\tLoss: 1.562851\n",
      "Train Epoch: 49 [28000/49000 (57%)]\tLoss: 1.531313\n",
      "Train Epoch: 49 [29000/49000 (59%)]\tLoss: 1.517675\n",
      "Train Epoch: 49 [30000/49000 (61%)]\tLoss: 1.525057\n",
      "Train Epoch: 49 [31000/49000 (63%)]\tLoss: 1.530489\n",
      "Train Epoch: 49 [32000/49000 (65%)]\tLoss: 1.537202\n",
      "Train Epoch: 49 [33000/49000 (67%)]\tLoss: 1.532024\n",
      "Train Epoch: 49 [34000/49000 (69%)]\tLoss: 1.531842\n",
      "Train Epoch: 49 [35000/49000 (71%)]\tLoss: 1.528105\n",
      "Train Epoch: 49 [36000/49000 (73%)]\tLoss: 1.545213\n",
      "Train Epoch: 49 [37000/49000 (76%)]\tLoss: 1.560157\n",
      "Train Epoch: 49 [38000/49000 (78%)]\tLoss: 1.516185\n",
      "Train Epoch: 49 [39000/49000 (80%)]\tLoss: 1.538152\n",
      "Train Epoch: 49 [40000/49000 (82%)]\tLoss: 1.526982\n",
      "Train Epoch: 49 [41000/49000 (84%)]\tLoss: 1.597385\n",
      "Train Epoch: 49 [42000/49000 (86%)]\tLoss: 1.536515\n",
      "Train Epoch: 49 [43000/49000 (88%)]\tLoss: 1.522317\n",
      "Train Epoch: 49 [44000/49000 (90%)]\tLoss: 1.560543\n",
      "Train Epoch: 49 [45000/49000 (92%)]\tLoss: 1.525741\n",
      "Train Epoch: 49 [46000/49000 (94%)]\tLoss: 1.587863\n",
      "Train Epoch: 49 [47000/49000 (96%)]\tLoss: 1.542331\n",
      "Train Epoch: 49 [48000/49000 (98%)]\tLoss: 1.587947\n",
      "\n",
      "Test set: Avg. loss: 1.5517, Accuracy: 19114/21000 (91.02%)\n",
      "\n",
      "Train Epoch: 50 [0/49000 (0%)]\tLoss: 1.539443\n",
      "Train Epoch: 50 [1000/49000 (2%)]\tLoss: 1.571889\n",
      "Train Epoch: 50 [2000/49000 (4%)]\tLoss: 1.553807\n",
      "Train Epoch: 50 [3000/49000 (6%)]\tLoss: 1.572268\n",
      "Train Epoch: 50 [4000/49000 (8%)]\tLoss: 1.581200\n",
      "Train Epoch: 50 [5000/49000 (10%)]\tLoss: 1.546886\n",
      "Train Epoch: 50 [6000/49000 (12%)]\tLoss: 1.553862\n",
      "Train Epoch: 50 [7000/49000 (14%)]\tLoss: 1.549717\n",
      "Train Epoch: 50 [8000/49000 (16%)]\tLoss: 1.516386\n",
      "Train Epoch: 50 [9000/49000 (18%)]\tLoss: 1.545931\n",
      "Train Epoch: 50 [10000/49000 (20%)]\tLoss: 1.523285\n",
      "Train Epoch: 50 [11000/49000 (22%)]\tLoss: 1.549030\n",
      "Train Epoch: 50 [12000/49000 (24%)]\tLoss: 1.511061\n",
      "Train Epoch: 50 [13000/49000 (27%)]\tLoss: 1.515327\n",
      "Train Epoch: 50 [14000/49000 (29%)]\tLoss: 1.561352\n",
      "Train Epoch: 50 [15000/49000 (31%)]\tLoss: 1.546089\n",
      "Train Epoch: 50 [16000/49000 (33%)]\tLoss: 1.526391\n",
      "Train Epoch: 50 [17000/49000 (35%)]\tLoss: 1.533150\n",
      "Train Epoch: 50 [18000/49000 (37%)]\tLoss: 1.536189\n",
      "Train Epoch: 50 [19000/49000 (39%)]\tLoss: 1.515192\n",
      "Train Epoch: 50 [20000/49000 (41%)]\tLoss: 1.539664\n",
      "Train Epoch: 50 [21000/49000 (43%)]\tLoss: 1.552635\n",
      "Train Epoch: 50 [22000/49000 (45%)]\tLoss: 1.564967\n",
      "Train Epoch: 50 [23000/49000 (47%)]\tLoss: 1.534330\n",
      "Train Epoch: 50 [24000/49000 (49%)]\tLoss: 1.524363\n",
      "Train Epoch: 50 [25000/49000 (51%)]\tLoss: 1.544150\n",
      "Train Epoch: 50 [26000/49000 (53%)]\tLoss: 1.536620\n",
      "Train Epoch: 50 [27000/49000 (55%)]\tLoss: 1.544027\n",
      "Train Epoch: 50 [28000/49000 (57%)]\tLoss: 1.524300\n",
      "Train Epoch: 50 [29000/49000 (59%)]\tLoss: 1.568066\n",
      "Train Epoch: 50 [30000/49000 (61%)]\tLoss: 1.557951\n",
      "Train Epoch: 50 [31000/49000 (63%)]\tLoss: 1.524513\n",
      "Train Epoch: 50 [32000/49000 (65%)]\tLoss: 1.518355\n",
      "Train Epoch: 50 [33000/49000 (67%)]\tLoss: 1.530110\n",
      "Train Epoch: 50 [34000/49000 (69%)]\tLoss: 1.548473\n",
      "Train Epoch: 50 [35000/49000 (71%)]\tLoss: 1.531808\n",
      "Train Epoch: 50 [36000/49000 (73%)]\tLoss: 1.536911\n",
      "Train Epoch: 50 [37000/49000 (76%)]\tLoss: 1.555556\n",
      "Train Epoch: 50 [38000/49000 (78%)]\tLoss: 1.523077\n",
      "Train Epoch: 50 [39000/49000 (80%)]\tLoss: 1.532004\n",
      "Train Epoch: 50 [40000/49000 (82%)]\tLoss: 1.569273\n",
      "Train Epoch: 50 [41000/49000 (84%)]\tLoss: 1.544747\n",
      "Train Epoch: 50 [42000/49000 (86%)]\tLoss: 1.514290\n",
      "Train Epoch: 50 [43000/49000 (88%)]\tLoss: 1.537969\n",
      "Train Epoch: 50 [44000/49000 (90%)]\tLoss: 1.533925\n",
      "Train Epoch: 50 [45000/49000 (92%)]\tLoss: 1.547399\n",
      "Train Epoch: 50 [46000/49000 (94%)]\tLoss: 1.518434\n",
      "Train Epoch: 50 [47000/49000 (96%)]\tLoss: 1.506554\n",
      "Train Epoch: 50 [48000/49000 (98%)]\tLoss: 1.532716\n",
      "\n",
      "Test set: Avg. loss: 1.5517, Accuracy: 19095/21000 (90.93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc_torch = test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test_acc_torch = test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ----------------------------------Pytorch Full ANN  end------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the neural networks performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = [test_acc_torch, test_acc_1_layer_ANN*100, test_acc_ANN*100]\n",
    "models_names = [\"torch\", \"ANN 1 layer\", \"ANN 2 layers\"]\n",
    "plt.bar(models_names, results)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('model')\n",
    "plt.ylim(90, 94.5)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}