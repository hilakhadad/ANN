{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 3 - Part A - ANN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 11 - Implementing a Multi-layer Artificial Neural Network from Scratch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtaining and preparing the MNIST dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n",
    "\n",
    "- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 examples)\n",
    "- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n",
    "- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 examples)\n",
    "- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Normalize to [-1, 1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = ((X / 255.) - .5) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize the first digit of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split into training, validation, and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_test:(21000, 784)\n",
      "shape of y_test:(21000,)\n",
      "shape of X_train:(44000, 784)\n",
      "shape of y_train:(44000,)\n",
      "shape of X_valid:(5000, 784)\n",
      "shape of y_valid:(5000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=123, stratify=y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)\n",
    "\n",
    "print(f\"shape of X_test:{X_test.shape}\")\n",
    "print(f\"shape of y_test:{y_test.shape}\")\n",
    "print(f\"shape of X_train:{X_train.shape}\")\n",
    "print(f\"shape of y_train:{y_train.shape}\")\n",
    "print(f\"shape of X_valid:{X_valid.shape}\")\n",
    "print(f\"shape of y_valid:{y_valid.shape}\")\n",
    "\n",
    "# optional to free up some memory by deleting non-used arrays:\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implementing a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):                                        \n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "\n",
    "class NeuralNetMLP:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_hidden2, num_classes, random_seed=123):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # hidden\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        \n",
    "        self.weight_h = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "\n",
    "        self.weight_h2 = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden2, num_hidden))\n",
    "        self.bias_h2 = np.zeros(num_hidden2)\n",
    "        \n",
    "        # output\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_classes, num_hidden2))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        a_h = sigmoid(z_h)\n",
    "\n",
    "        # Hidden layer2\n",
    "        # input dim: [n_examples, n_hidden] dot [n_hidden2, n_hidden].T\n",
    "        # output dim: [n_examples, n_hidden2]\n",
    "        z_h2 = np.dot(a_h, self.weight_h2.T) + self.bias_h2\n",
    "        a_h2 = sigmoid(z_h2)\n",
    "\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden2] dot [n_classes, n_hidden2].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h2, self.weight_out.T) + self.bias_out\n",
    "        a_out = sigmoid(z_out)\n",
    "        return a_h, a_h2, a_out\n",
    "\n",
    "    def backward(self, x, a_h, a_h2, a_out, y):\n",
    "    \n",
    "        #########################\n",
    "        ### Output layer weights\n",
    "        #########################\n",
    "        \n",
    "        # onehot encoding\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "\n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## for convenient re-use\n",
    "        \n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = a_out * (1. - a_out) # sigmoid derivative\n",
    "\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule) placeholder\"\n",
    "\n",
    "        # gradient for output weights\n",
    "        \n",
    "        # [n_examples, n_hidden2]\n",
    "        d_z_out__dw_out = a_h2\n",
    "\n",
    "        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden2]\n",
    "        # output dim: [n_classes, n_hidden2]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "\n",
    "\n",
    "        #################################\n",
    "        # Part 2: dLoss/dHiddenWeights2\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct2 * dHiddenAct2/dHiddenNet2 * dHiddenNet2/dWeight2\n",
    "\n",
    "        # [n_classes, n_hidden2]\n",
    "        d_z_out__a_h2 = self.weight_out\n",
    "\n",
    "        # output dim: [n_examples, n_hidden2]\n",
    "        d_loss__a_h2 = np.dot(delta_out, d_z_out__a_h2)\n",
    "\n",
    "        # [n_examples, n_hidden2]\n",
    "        d_a_h__d_z_h2 = a_h2 * (1. - a_h2) # sigmoid derivative\n",
    "\n",
    "        delta_h2 = d_loss__a_h2 * d_a_h__d_z_h2\n",
    "\n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_h__d_w_h2 = a_h\n",
    "\n",
    "        # output dim: [n_hidden2, n_hidden]\n",
    "        d_loss__d_w_h2 = np.dot((d_loss__a_h2 * d_a_h__d_z_h2).T, d_z_h__d_w_h2)\n",
    "        d_loss__d_b_h2 = np.sum((d_loss__a_h2 * d_a_h__d_z_h2), axis=0)\n",
    "\n",
    "        #################################        \n",
    "        # Part 3: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "        \n",
    "        # [n_hidden2, n_hidden]\n",
    "        d_z_out__a_h = self.weight_h2\n",
    "        \n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_h2, d_z_out__a_h)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative\n",
    "        \n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        \n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out,\n",
    "                d_loss__d_w_h2, d_loss__d_b_h2,\n",
    "                d_loss__d_w_h, d_loss__d_b_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNetMLP(num_features=28*28,\n",
    "                     num_hidden=50,\n",
    "                     num_hidden2=50,\n",
    "                     num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Coding the neural network training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Defining data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "        \n",
    "# iterate over training epochs\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # iterate over minibatches\n",
    "    minibatch_gen = minibatch_generator(\n",
    "        X_train, y_train, minibatch_size)\n",
    "    \n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "        \n",
    "    break\n",
    "    \n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Defining a function to compute the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation MSE: 0.2\n",
      "Initial validation accuracy: 9.0%\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets) \n",
    "\n",
    "\n",
    "_,_, probas = model.forward(X_valid)\n",
    "mse = mse_loss(y_valid, probas)\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(y_valid, predicted_labels)\n",
    "\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _,_, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/i\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial valid MSE: 0.3\n",
      "Initial valid accuracy: 9.0%\n"
     ]
    }
   ],
   "source": [
    "mse, acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # iterate over minibatches\n",
    "        minibatch_gen = minibatch_generator(\n",
    "            X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "            \n",
    "            #### Compute outputs ####\n",
    "            a_h, a_h2, a_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Compute gradients ####\n",
    "            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h2, d_loss__d_b_h2, d_loss__d_w_h, d_loss__d_b_h = \\\n",
    "                model.backward(X_train_mini, a_h, a_h2, a_out, y_train_mini)\n",
    "\n",
    "            #### Update weights ####\n",
    "            model.weight_h -= learning_rate * d_loss__d_w_h\n",
    "            model.bias_h -= learning_rate * d_loss__d_b_h\n",
    "            model.weight_h2 -= learning_rate * d_loss__d_w_h2\n",
    "            model.bias_h2 -= learning_rate * d_loss__d_b_h2\n",
    "            model.weight_out -= learning_rate * d_loss__d_w_out\n",
    "            model.bias_out -= learning_rate * d_loss__d_b_out\n",
    "        \n",
    "        #### Epoch Logging ####        \n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Train MSE: 0.09 | Train Acc: 20.81% | Valid Acc: 20.74%\n",
      "Epoch: 002/050 | Train MSE: 0.09 | Train Acc: 26.82% | Valid Acc: 26.68%\n",
      "Epoch: 003/050 | Train MSE: 0.08 | Train Acc: 31.01% | Valid Acc: 30.58%\n",
      "Epoch: 004/050 | Train MSE: 0.07 | Train Acc: 50.75% | Valid Acc: 50.86%\n",
      "Epoch: 005/050 | Train MSE: 0.06 | Train Acc: 62.88% | Valid Acc: 63.08%\n",
      "Epoch: 006/050 | Train MSE: 0.05 | Train Acc: 70.61% | Valid Acc: 70.60%\n",
      "Epoch: 007/050 | Train MSE: 0.04 | Train Acc: 79.21% | Valid Acc: 79.32%\n",
      "Epoch: 008/050 | Train MSE: 0.03 | Train Acc: 84.49% | Valid Acc: 84.82%\n",
      "Epoch: 009/050 | Train MSE: 0.03 | Train Acc: 86.76% | Valid Acc: 87.00%\n",
      "Epoch: 010/050 | Train MSE: 0.02 | Train Acc: 87.76% | Valid Acc: 87.92%\n",
      "Epoch: 011/050 | Train MSE: 0.02 | Train Acc: 88.45% | Valid Acc: 88.42%\n",
      "Epoch: 012/050 | Train MSE: 0.02 | Train Acc: 89.08% | Valid Acc: 88.86%\n",
      "Epoch: 013/050 | Train MSE: 0.02 | Train Acc: 89.69% | Valid Acc: 89.58%\n",
      "Epoch: 014/050 | Train MSE: 0.02 | Train Acc: 90.04% | Valid Acc: 89.88%\n",
      "Epoch: 015/050 | Train MSE: 0.02 | Train Acc: 90.39% | Valid Acc: 89.84%\n",
      "Epoch: 016/050 | Train MSE: 0.02 | Train Acc: 90.70% | Valid Acc: 90.34%\n",
      "Epoch: 017/050 | Train MSE: 0.02 | Train Acc: 91.10% | Valid Acc: 90.58%\n",
      "Epoch: 018/050 | Train MSE: 0.02 | Train Acc: 91.34% | Valid Acc: 90.72%\n",
      "Epoch: 019/050 | Train MSE: 0.01 | Train Acc: 91.50% | Valid Acc: 90.98%\n",
      "Epoch: 020/050 | Train MSE: 0.01 | Train Acc: 91.77% | Valid Acc: 91.18%\n",
      "Epoch: 021/050 | Train MSE: 0.01 | Train Acc: 92.00% | Valid Acc: 91.38%\n",
      "Epoch: 022/050 | Train MSE: 0.01 | Train Acc: 92.15% | Valid Acc: 91.36%\n",
      "Epoch: 023/050 | Train MSE: 0.01 | Train Acc: 92.31% | Valid Acc: 91.86%\n",
      "Epoch: 024/050 | Train MSE: 0.01 | Train Acc: 92.59% | Valid Acc: 92.20%\n",
      "Epoch: 025/050 | Train MSE: 0.01 | Train Acc: 92.74% | Valid Acc: 92.14%\n",
      "Epoch: 026/050 | Train MSE: 0.01 | Train Acc: 92.89% | Valid Acc: 92.40%\n",
      "Epoch: 027/050 | Train MSE: 0.01 | Train Acc: 93.05% | Valid Acc: 92.62%\n",
      "Epoch: 028/050 | Train MSE: 0.01 | Train Acc: 93.26% | Valid Acc: 92.64%\n",
      "Epoch: 029/050 | Train MSE: 0.01 | Train Acc: 93.42% | Valid Acc: 92.84%\n",
      "Epoch: 030/050 | Train MSE: 0.01 | Train Acc: 93.56% | Valid Acc: 93.02%\n",
      "Epoch: 031/050 | Train MSE: 0.01 | Train Acc: 93.62% | Valid Acc: 93.26%\n",
      "Epoch: 032/050 | Train MSE: 0.01 | Train Acc: 93.76% | Valid Acc: 93.22%\n",
      "Epoch: 033/050 | Train MSE: 0.01 | Train Acc: 93.85% | Valid Acc: 93.16%\n",
      "Epoch: 034/050 | Train MSE: 0.01 | Train Acc: 93.97% | Valid Acc: 93.42%\n",
      "Epoch: 035/050 | Train MSE: 0.01 | Train Acc: 94.08% | Valid Acc: 93.56%\n",
      "Epoch: 036/050 | Train MSE: 0.01 | Train Acc: 94.19% | Valid Acc: 93.58%\n",
      "Epoch: 037/050 | Train MSE: 0.01 | Train Acc: 94.19% | Valid Acc: 93.48%\n",
      "Epoch: 038/050 | Train MSE: 0.01 | Train Acc: 94.46% | Valid Acc: 93.76%\n",
      "Epoch: 039/050 | Train MSE: 0.01 | Train Acc: 94.51% | Valid Acc: 93.78%\n",
      "Epoch: 040/050 | Train MSE: 0.01 | Train Acc: 94.67% | Valid Acc: 93.80%\n",
      "Epoch: 041/050 | Train MSE: 0.01 | Train Acc: 94.71% | Valid Acc: 93.96%\n",
      "Epoch: 042/050 | Train MSE: 0.01 | Train Acc: 94.83% | Valid Acc: 93.90%\n",
      "Epoch: 043/050 | Train MSE: 0.01 | Train Acc: 94.89% | Valid Acc: 94.10%\n",
      "Epoch: 044/050 | Train MSE: 0.01 | Train Acc: 94.93% | Valid Acc: 94.14%\n",
      "Epoch: 045/050 | Train MSE: 0.01 | Train Acc: 95.09% | Valid Acc: 94.22%\n",
      "Epoch: 046/050 | Train MSE: 0.01 | Train Acc: 95.16% | Valid Acc: 94.18%\n",
      "Epoch: 047/050 | Train MSE: 0.01 | Train Acc: 95.25% | Valid Acc: 94.30%\n",
      "Epoch: 048/050 | Train MSE: 0.01 | Train Acc: 95.32% | Valid Acc: 94.40%\n",
      "Epoch: 049/050 | Train MSE: 0.01 | Train Acc: 95.35% | Valid Acc: 94.58%\n",
      "Epoch: 050/050 | Train MSE: 0.01 | Train Acc: 95.43% | Valid Acc: 94.66%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123) # for the training set shuffling\n",
    "\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc = train(\n",
    "    model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 94.42%\n"
     ]
    }
   ],
   "source": [
    "test_mse_ANN, test_acc_ANN = compute_mse_and_acc(model, X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc_ANN*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ----------------------------------Pytorch Full ANN  start------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, photos, labels, transform=None, target_transform=None):\n",
    "        self.img_labels = labels\n",
    "        self.img_dir = photos\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img_dir[idx]\n",
    "        label = self.img_labels[idx]\n",
    "        image = torch.from_numpy(image).float()\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size_train = 100\n",
    "batch_size_test = 100\n",
    "learning_rate = 0.1\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  CustomImageDataset(X_temp, y_temp),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  CustomImageDataset(X_test, y_test),\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 50)\n",
    "        self.final = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(self.final(x))\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  test_acc_torch = 100. * correct / len(test_loader.dataset)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    test_acc_torch))\n",
    "  return test_acc_torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hilak\\AppData\\Local\\Temp\\ipykernel_9256\\2640556148.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3031, Accuracy: 3162/21000 (15.06%)\n",
      "\n",
      "Train Epoch: 1 [0/49000 (0%)]\tLoss: 2.308971\n",
      "Train Epoch: 1 [1000/49000 (2%)]\tLoss: 2.206638\n",
      "Train Epoch: 1 [2000/49000 (4%)]\tLoss: 2.142563\n",
      "Train Epoch: 1 [3000/49000 (6%)]\tLoss: 2.070078\n",
      "Train Epoch: 1 [4000/49000 (8%)]\tLoss: 2.020149\n",
      "Train Epoch: 1 [5000/49000 (10%)]\tLoss: 1.946073\n",
      "Train Epoch: 1 [6000/49000 (12%)]\tLoss: 1.930376\n",
      "Train Epoch: 1 [7000/49000 (14%)]\tLoss: 1.852232\n",
      "Train Epoch: 1 [8000/49000 (16%)]\tLoss: 1.827001\n",
      "Train Epoch: 1 [9000/49000 (18%)]\tLoss: 1.821858\n",
      "Train Epoch: 1 [10000/49000 (20%)]\tLoss: 1.796880\n",
      "Train Epoch: 1 [11000/49000 (22%)]\tLoss: 1.819167\n",
      "Train Epoch: 1 [12000/49000 (24%)]\tLoss: 1.743275\n",
      "Train Epoch: 1 [13000/49000 (27%)]\tLoss: 1.769252\n",
      "Train Epoch: 1 [14000/49000 (29%)]\tLoss: 1.761779\n",
      "Train Epoch: 1 [15000/49000 (31%)]\tLoss: 1.715479\n",
      "Train Epoch: 1 [16000/49000 (33%)]\tLoss: 1.708005\n",
      "Train Epoch: 1 [17000/49000 (35%)]\tLoss: 1.714012\n",
      "Train Epoch: 1 [18000/49000 (37%)]\tLoss: 1.757169\n",
      "Train Epoch: 1 [19000/49000 (39%)]\tLoss: 1.689790\n",
      "Train Epoch: 1 [20000/49000 (41%)]\tLoss: 1.687680\n",
      "Train Epoch: 1 [21000/49000 (43%)]\tLoss: 1.688606\n",
      "Train Epoch: 1 [22000/49000 (45%)]\tLoss: 1.696694\n",
      "Train Epoch: 1 [23000/49000 (47%)]\tLoss: 1.703827\n",
      "Train Epoch: 1 [24000/49000 (49%)]\tLoss: 1.679594\n",
      "Train Epoch: 1 [25000/49000 (51%)]\tLoss: 1.694342\n",
      "Train Epoch: 1 [26000/49000 (53%)]\tLoss: 1.654593\n",
      "Train Epoch: 1 [27000/49000 (55%)]\tLoss: 1.677766\n",
      "Train Epoch: 1 [28000/49000 (57%)]\tLoss: 1.686136\n",
      "Train Epoch: 1 [29000/49000 (59%)]\tLoss: 1.702512\n",
      "Train Epoch: 1 [30000/49000 (61%)]\tLoss: 1.699849\n",
      "Train Epoch: 1 [31000/49000 (63%)]\tLoss: 1.637746\n",
      "Train Epoch: 1 [32000/49000 (65%)]\tLoss: 1.683562\n",
      "Train Epoch: 1 [33000/49000 (67%)]\tLoss: 1.665864\n",
      "Train Epoch: 1 [34000/49000 (69%)]\tLoss: 1.639428\n",
      "Train Epoch: 1 [35000/49000 (71%)]\tLoss: 1.664117\n",
      "Train Epoch: 1 [36000/49000 (73%)]\tLoss: 1.683018\n",
      "Train Epoch: 1 [37000/49000 (76%)]\tLoss: 1.643018\n",
      "Train Epoch: 1 [38000/49000 (78%)]\tLoss: 1.677520\n",
      "Train Epoch: 1 [39000/49000 (80%)]\tLoss: 1.667266\n",
      "Train Epoch: 1 [40000/49000 (82%)]\tLoss: 1.640554\n",
      "Train Epoch: 1 [41000/49000 (84%)]\tLoss: 1.627073\n",
      "Train Epoch: 1 [42000/49000 (86%)]\tLoss: 1.643109\n",
      "Train Epoch: 1 [43000/49000 (88%)]\tLoss: 1.651409\n",
      "Train Epoch: 1 [44000/49000 (90%)]\tLoss: 1.667844\n",
      "Train Epoch: 1 [45000/49000 (92%)]\tLoss: 1.643538\n",
      "Train Epoch: 1 [46000/49000 (94%)]\tLoss: 1.625550\n",
      "Train Epoch: 1 [47000/49000 (96%)]\tLoss: 1.617294\n",
      "Train Epoch: 1 [48000/49000 (98%)]\tLoss: 1.631386\n",
      "\n",
      "Test set: Avg. loss: 1.6373, Accuracy: 18187/21000 (86.60%)\n",
      "\n",
      "Train Epoch: 2 [0/49000 (0%)]\tLoss: 1.633600\n",
      "Train Epoch: 2 [1000/49000 (2%)]\tLoss: 1.630923\n",
      "Train Epoch: 2 [2000/49000 (4%)]\tLoss: 1.624829\n",
      "Train Epoch: 2 [3000/49000 (6%)]\tLoss: 1.601232\n",
      "Train Epoch: 2 [4000/49000 (8%)]\tLoss: 1.625718\n",
      "Train Epoch: 2 [5000/49000 (10%)]\tLoss: 1.638038\n",
      "Train Epoch: 2 [6000/49000 (12%)]\tLoss: 1.656991\n",
      "Train Epoch: 2 [7000/49000 (14%)]\tLoss: 1.617438\n",
      "Train Epoch: 2 [8000/49000 (16%)]\tLoss: 1.641296\n",
      "Train Epoch: 2 [9000/49000 (18%)]\tLoss: 1.585380\n",
      "Train Epoch: 2 [10000/49000 (20%)]\tLoss: 1.600877\n",
      "Train Epoch: 2 [11000/49000 (22%)]\tLoss: 1.611893\n",
      "Train Epoch: 2 [12000/49000 (24%)]\tLoss: 1.616328\n",
      "Train Epoch: 2 [13000/49000 (27%)]\tLoss: 1.599344\n",
      "Train Epoch: 2 [14000/49000 (29%)]\tLoss: 1.657031\n",
      "Train Epoch: 2 [15000/49000 (31%)]\tLoss: 1.640983\n",
      "Train Epoch: 2 [16000/49000 (33%)]\tLoss: 1.617940\n",
      "Train Epoch: 2 [17000/49000 (35%)]\tLoss: 1.630361\n",
      "Train Epoch: 2 [18000/49000 (37%)]\tLoss: 1.585284\n",
      "Train Epoch: 2 [19000/49000 (39%)]\tLoss: 1.614275\n",
      "Train Epoch: 2 [20000/49000 (41%)]\tLoss: 1.634706\n",
      "Train Epoch: 2 [21000/49000 (43%)]\tLoss: 1.619115\n",
      "Train Epoch: 2 [22000/49000 (45%)]\tLoss: 1.622025\n",
      "Train Epoch: 2 [23000/49000 (47%)]\tLoss: 1.626293\n",
      "Train Epoch: 2 [24000/49000 (49%)]\tLoss: 1.603305\n",
      "Train Epoch: 2 [25000/49000 (51%)]\tLoss: 1.659893\n",
      "Train Epoch: 2 [26000/49000 (53%)]\tLoss: 1.591309\n",
      "Train Epoch: 2 [27000/49000 (55%)]\tLoss: 1.579131\n",
      "Train Epoch: 2 [28000/49000 (57%)]\tLoss: 1.629346\n",
      "Train Epoch: 2 [29000/49000 (59%)]\tLoss: 1.610401\n",
      "Train Epoch: 2 [30000/49000 (61%)]\tLoss: 1.607853\n",
      "Train Epoch: 2 [31000/49000 (63%)]\tLoss: 1.630336\n",
      "Train Epoch: 2 [32000/49000 (65%)]\tLoss: 1.589635\n",
      "Train Epoch: 2 [33000/49000 (67%)]\tLoss: 1.625914\n",
      "Train Epoch: 2 [34000/49000 (69%)]\tLoss: 1.597707\n",
      "Train Epoch: 2 [35000/49000 (71%)]\tLoss: 1.617628\n",
      "Train Epoch: 2 [36000/49000 (73%)]\tLoss: 1.600523\n",
      "Train Epoch: 2 [37000/49000 (76%)]\tLoss: 1.607568\n",
      "Train Epoch: 2 [38000/49000 (78%)]\tLoss: 1.603445\n",
      "Train Epoch: 2 [39000/49000 (80%)]\tLoss: 1.615806\n",
      "Train Epoch: 2 [40000/49000 (82%)]\tLoss: 1.601918\n",
      "Train Epoch: 2 [41000/49000 (84%)]\tLoss: 1.634200\n",
      "Train Epoch: 2 [42000/49000 (86%)]\tLoss: 1.612422\n",
      "Train Epoch: 2 [43000/49000 (88%)]\tLoss: 1.606378\n",
      "Train Epoch: 2 [44000/49000 (90%)]\tLoss: 1.603409\n",
      "Train Epoch: 2 [45000/49000 (92%)]\tLoss: 1.643317\n",
      "Train Epoch: 2 [46000/49000 (94%)]\tLoss: 1.607510\n",
      "Train Epoch: 2 [47000/49000 (96%)]\tLoss: 1.617709\n",
      "Train Epoch: 2 [48000/49000 (98%)]\tLoss: 1.629880\n",
      "\n",
      "Test set: Avg. loss: 1.6041, Accuracy: 18528/21000 (88.23%)\n",
      "\n",
      "Train Epoch: 3 [0/49000 (0%)]\tLoss: 1.569717\n",
      "Train Epoch: 3 [1000/49000 (2%)]\tLoss: 1.603975\n",
      "Train Epoch: 3 [2000/49000 (4%)]\tLoss: 1.602499\n",
      "Train Epoch: 3 [3000/49000 (6%)]\tLoss: 1.601845\n",
      "Train Epoch: 3 [4000/49000 (8%)]\tLoss: 1.598385\n",
      "Train Epoch: 3 [5000/49000 (10%)]\tLoss: 1.552164\n",
      "Train Epoch: 3 [6000/49000 (12%)]\tLoss: 1.580160\n",
      "Train Epoch: 3 [7000/49000 (14%)]\tLoss: 1.582195\n",
      "Train Epoch: 3 [8000/49000 (16%)]\tLoss: 1.611781\n",
      "Train Epoch: 3 [9000/49000 (18%)]\tLoss: 1.574106\n",
      "Train Epoch: 3 [10000/49000 (20%)]\tLoss: 1.582993\n",
      "Train Epoch: 3 [11000/49000 (22%)]\tLoss: 1.602168\n",
      "Train Epoch: 3 [12000/49000 (24%)]\tLoss: 1.593859\n",
      "Train Epoch: 3 [13000/49000 (27%)]\tLoss: 1.591493\n",
      "Train Epoch: 3 [14000/49000 (29%)]\tLoss: 1.604462\n",
      "Train Epoch: 3 [15000/49000 (31%)]\tLoss: 1.643328\n",
      "Train Epoch: 3 [16000/49000 (33%)]\tLoss: 1.627601\n",
      "Train Epoch: 3 [17000/49000 (35%)]\tLoss: 1.596298\n",
      "Train Epoch: 3 [18000/49000 (37%)]\tLoss: 1.610775\n",
      "Train Epoch: 3 [19000/49000 (39%)]\tLoss: 1.580235\n",
      "Train Epoch: 3 [20000/49000 (41%)]\tLoss: 1.602708\n",
      "Train Epoch: 3 [21000/49000 (43%)]\tLoss: 1.615436\n",
      "Train Epoch: 3 [22000/49000 (45%)]\tLoss: 1.625581\n",
      "Train Epoch: 3 [23000/49000 (47%)]\tLoss: 1.558984\n",
      "Train Epoch: 3 [24000/49000 (49%)]\tLoss: 1.587183\n",
      "Train Epoch: 3 [25000/49000 (51%)]\tLoss: 1.599639\n",
      "Train Epoch: 3 [26000/49000 (53%)]\tLoss: 1.576704\n",
      "Train Epoch: 3 [27000/49000 (55%)]\tLoss: 1.607469\n",
      "Train Epoch: 3 [28000/49000 (57%)]\tLoss: 1.578285\n",
      "Train Epoch: 3 [29000/49000 (59%)]\tLoss: 1.586251\n",
      "Train Epoch: 3 [30000/49000 (61%)]\tLoss: 1.553777\n",
      "Train Epoch: 3 [31000/49000 (63%)]\tLoss: 1.574583\n",
      "Train Epoch: 3 [32000/49000 (65%)]\tLoss: 1.535808\n",
      "Train Epoch: 3 [33000/49000 (67%)]\tLoss: 1.603840\n",
      "Train Epoch: 3 [34000/49000 (69%)]\tLoss: 1.560791\n",
      "Train Epoch: 3 [35000/49000 (71%)]\tLoss: 1.580716\n",
      "Train Epoch: 3 [36000/49000 (73%)]\tLoss: 1.575420\n",
      "Train Epoch: 3 [37000/49000 (76%)]\tLoss: 1.633921\n",
      "Train Epoch: 3 [38000/49000 (78%)]\tLoss: 1.611454\n",
      "Train Epoch: 3 [39000/49000 (80%)]\tLoss: 1.595828\n",
      "Train Epoch: 3 [40000/49000 (82%)]\tLoss: 1.583799\n",
      "Train Epoch: 3 [41000/49000 (84%)]\tLoss: 1.583081\n",
      "Train Epoch: 3 [42000/49000 (86%)]\tLoss: 1.575044\n",
      "Train Epoch: 3 [43000/49000 (88%)]\tLoss: 1.597779\n",
      "Train Epoch: 3 [44000/49000 (90%)]\tLoss: 1.577944\n",
      "Train Epoch: 3 [45000/49000 (92%)]\tLoss: 1.566240\n",
      "Train Epoch: 3 [46000/49000 (94%)]\tLoss: 1.552576\n",
      "Train Epoch: 3 [47000/49000 (96%)]\tLoss: 1.621933\n",
      "Train Epoch: 3 [48000/49000 (98%)]\tLoss: 1.597920\n",
      "\n",
      "Test set: Avg. loss: 1.5904, Accuracy: 18663/21000 (88.87%)\n",
      "\n",
      "Train Epoch: 4 [0/49000 (0%)]\tLoss: 1.583528\n",
      "Train Epoch: 4 [1000/49000 (2%)]\tLoss: 1.603742\n",
      "Train Epoch: 4 [2000/49000 (4%)]\tLoss: 1.597054\n",
      "Train Epoch: 4 [3000/49000 (6%)]\tLoss: 1.629795\n",
      "Train Epoch: 4 [4000/49000 (8%)]\tLoss: 1.559314\n",
      "Train Epoch: 4 [5000/49000 (10%)]\tLoss: 1.586936\n",
      "Train Epoch: 4 [6000/49000 (12%)]\tLoss: 1.583475\n",
      "Train Epoch: 4 [7000/49000 (14%)]\tLoss: 1.537874\n",
      "Train Epoch: 4 [8000/49000 (16%)]\tLoss: 1.583675\n",
      "Train Epoch: 4 [9000/49000 (18%)]\tLoss: 1.576380\n",
      "Train Epoch: 4 [10000/49000 (20%)]\tLoss: 1.552408\n",
      "Train Epoch: 4 [11000/49000 (22%)]\tLoss: 1.597836\n",
      "Train Epoch: 4 [12000/49000 (24%)]\tLoss: 1.611873\n",
      "Train Epoch: 4 [13000/49000 (27%)]\tLoss: 1.575247\n",
      "Train Epoch: 4 [14000/49000 (29%)]\tLoss: 1.578837\n",
      "Train Epoch: 4 [15000/49000 (31%)]\tLoss: 1.581416\n",
      "Train Epoch: 4 [16000/49000 (33%)]\tLoss: 1.618376\n",
      "Train Epoch: 4 [17000/49000 (35%)]\tLoss: 1.569395\n",
      "Train Epoch: 4 [18000/49000 (37%)]\tLoss: 1.563570\n",
      "Train Epoch: 4 [19000/49000 (39%)]\tLoss: 1.584481\n",
      "Train Epoch: 4 [20000/49000 (41%)]\tLoss: 1.573398\n",
      "Train Epoch: 4 [21000/49000 (43%)]\tLoss: 1.598732\n",
      "Train Epoch: 4 [22000/49000 (45%)]\tLoss: 1.556747\n",
      "Train Epoch: 4 [23000/49000 (47%)]\tLoss: 1.592700\n",
      "Train Epoch: 4 [24000/49000 (49%)]\tLoss: 1.588341\n",
      "Train Epoch: 4 [25000/49000 (51%)]\tLoss: 1.570667\n",
      "Train Epoch: 4 [26000/49000 (53%)]\tLoss: 1.616031\n",
      "Train Epoch: 4 [27000/49000 (55%)]\tLoss: 1.585255\n",
      "Train Epoch: 4 [28000/49000 (57%)]\tLoss: 1.583346\n",
      "Train Epoch: 4 [29000/49000 (59%)]\tLoss: 1.573743\n",
      "Train Epoch: 4 [30000/49000 (61%)]\tLoss: 1.581994\n",
      "Train Epoch: 4 [31000/49000 (63%)]\tLoss: 1.630223\n",
      "Train Epoch: 4 [32000/49000 (65%)]\tLoss: 1.556494\n",
      "Train Epoch: 4 [33000/49000 (67%)]\tLoss: 1.581667\n",
      "Train Epoch: 4 [34000/49000 (69%)]\tLoss: 1.550237\n",
      "Train Epoch: 4 [35000/49000 (71%)]\tLoss: 1.556305\n",
      "Train Epoch: 4 [36000/49000 (73%)]\tLoss: 1.581164\n",
      "Train Epoch: 4 [37000/49000 (76%)]\tLoss: 1.555950\n",
      "Train Epoch: 4 [38000/49000 (78%)]\tLoss: 1.615475\n",
      "Train Epoch: 4 [39000/49000 (80%)]\tLoss: 1.592789\n",
      "Train Epoch: 4 [40000/49000 (82%)]\tLoss: 1.617895\n",
      "Train Epoch: 4 [41000/49000 (84%)]\tLoss: 1.559401\n",
      "Train Epoch: 4 [42000/49000 (86%)]\tLoss: 1.563630\n",
      "Train Epoch: 4 [43000/49000 (88%)]\tLoss: 1.622927\n",
      "Train Epoch: 4 [44000/49000 (90%)]\tLoss: 1.562641\n",
      "Train Epoch: 4 [45000/49000 (92%)]\tLoss: 1.583338\n",
      "Train Epoch: 4 [46000/49000 (94%)]\tLoss: 1.585060\n",
      "Train Epoch: 4 [47000/49000 (96%)]\tLoss: 1.595306\n",
      "Train Epoch: 4 [48000/49000 (98%)]\tLoss: 1.571861\n",
      "\n",
      "Test set: Avg. loss: 1.5811, Accuracy: 18753/21000 (89.30%)\n",
      "\n",
      "Train Epoch: 5 [0/49000 (0%)]\tLoss: 1.547168\n",
      "Train Epoch: 5 [1000/49000 (2%)]\tLoss: 1.597003\n",
      "Train Epoch: 5 [2000/49000 (4%)]\tLoss: 1.566121\n",
      "Train Epoch: 5 [3000/49000 (6%)]\tLoss: 1.583212\n",
      "Train Epoch: 5 [4000/49000 (8%)]\tLoss: 1.569747\n",
      "Train Epoch: 5 [5000/49000 (10%)]\tLoss: 1.568436\n",
      "Train Epoch: 5 [6000/49000 (12%)]\tLoss: 1.579266\n",
      "Train Epoch: 5 [7000/49000 (14%)]\tLoss: 1.609460\n",
      "Train Epoch: 5 [8000/49000 (16%)]\tLoss: 1.594351\n",
      "Train Epoch: 5 [9000/49000 (18%)]\tLoss: 1.566522\n",
      "Train Epoch: 5 [10000/49000 (20%)]\tLoss: 1.604135\n",
      "Train Epoch: 5 [11000/49000 (22%)]\tLoss: 1.547324\n",
      "Train Epoch: 5 [12000/49000 (24%)]\tLoss: 1.537800\n",
      "Train Epoch: 5 [13000/49000 (27%)]\tLoss: 1.554553\n",
      "Train Epoch: 5 [14000/49000 (29%)]\tLoss: 1.580547\n",
      "Train Epoch: 5 [15000/49000 (31%)]\tLoss: 1.583957\n",
      "Train Epoch: 5 [16000/49000 (33%)]\tLoss: 1.526146\n",
      "Train Epoch: 5 [17000/49000 (35%)]\tLoss: 1.542139\n",
      "Train Epoch: 5 [18000/49000 (37%)]\tLoss: 1.548117\n",
      "Train Epoch: 5 [19000/49000 (39%)]\tLoss: 1.548823\n",
      "Train Epoch: 5 [20000/49000 (41%)]\tLoss: 1.552258\n",
      "Train Epoch: 5 [21000/49000 (43%)]\tLoss: 1.555687\n",
      "Train Epoch: 5 [22000/49000 (45%)]\tLoss: 1.558145\n",
      "Train Epoch: 5 [23000/49000 (47%)]\tLoss: 1.574559\n",
      "Train Epoch: 5 [24000/49000 (49%)]\tLoss: 1.550869\n",
      "Train Epoch: 5 [25000/49000 (51%)]\tLoss: 1.583691\n",
      "Train Epoch: 5 [26000/49000 (53%)]\tLoss: 1.555696\n",
      "Train Epoch: 5 [27000/49000 (55%)]\tLoss: 1.570096\n",
      "Train Epoch: 5 [28000/49000 (57%)]\tLoss: 1.583309\n",
      "Train Epoch: 5 [29000/49000 (59%)]\tLoss: 1.591533\n",
      "Train Epoch: 5 [30000/49000 (61%)]\tLoss: 1.567314\n",
      "Train Epoch: 5 [31000/49000 (63%)]\tLoss: 1.567222\n",
      "Train Epoch: 5 [32000/49000 (65%)]\tLoss: 1.600965\n",
      "Train Epoch: 5 [33000/49000 (67%)]\tLoss: 1.587920\n",
      "Train Epoch: 5 [34000/49000 (69%)]\tLoss: 1.564175\n",
      "Train Epoch: 5 [35000/49000 (71%)]\tLoss: 1.599663\n",
      "Train Epoch: 5 [36000/49000 (73%)]\tLoss: 1.569110\n",
      "Train Epoch: 5 [37000/49000 (76%)]\tLoss: 1.559872\n",
      "Train Epoch: 5 [38000/49000 (78%)]\tLoss: 1.564355\n",
      "Train Epoch: 5 [39000/49000 (80%)]\tLoss: 1.549303\n",
      "Train Epoch: 5 [40000/49000 (82%)]\tLoss: 1.558699\n",
      "Train Epoch: 5 [41000/49000 (84%)]\tLoss: 1.619510\n",
      "Train Epoch: 5 [42000/49000 (86%)]\tLoss: 1.578639\n",
      "Train Epoch: 5 [43000/49000 (88%)]\tLoss: 1.555137\n",
      "Train Epoch: 5 [44000/49000 (90%)]\tLoss: 1.614602\n",
      "Train Epoch: 5 [45000/49000 (92%)]\tLoss: 1.598302\n",
      "Train Epoch: 5 [46000/49000 (94%)]\tLoss: 1.570392\n",
      "Train Epoch: 5 [47000/49000 (96%)]\tLoss: 1.576885\n",
      "Train Epoch: 5 [48000/49000 (98%)]\tLoss: 1.597801\n",
      "\n",
      "Test set: Avg. loss: 1.5763, Accuracy: 18797/21000 (89.51%)\n",
      "\n",
      "Train Epoch: 6 [0/49000 (0%)]\tLoss: 1.550028\n",
      "Train Epoch: 6 [1000/49000 (2%)]\tLoss: 1.608322\n",
      "Train Epoch: 6 [2000/49000 (4%)]\tLoss: 1.571106\n",
      "Train Epoch: 6 [3000/49000 (6%)]\tLoss: 1.581202\n",
      "Train Epoch: 6 [4000/49000 (8%)]\tLoss: 1.586285\n",
      "Train Epoch: 6 [5000/49000 (10%)]\tLoss: 1.606992\n",
      "Train Epoch: 6 [6000/49000 (12%)]\tLoss: 1.601780\n",
      "Train Epoch: 6 [7000/49000 (14%)]\tLoss: 1.531885\n",
      "Train Epoch: 6 [8000/49000 (16%)]\tLoss: 1.558945\n",
      "Train Epoch: 6 [9000/49000 (18%)]\tLoss: 1.583655\n",
      "Train Epoch: 6 [10000/49000 (20%)]\tLoss: 1.573329\n",
      "Train Epoch: 6 [11000/49000 (22%)]\tLoss: 1.608667\n",
      "Train Epoch: 6 [12000/49000 (24%)]\tLoss: 1.576465\n",
      "Train Epoch: 6 [13000/49000 (27%)]\tLoss: 1.555537\n",
      "Train Epoch: 6 [14000/49000 (29%)]\tLoss: 1.550123\n",
      "Train Epoch: 6 [15000/49000 (31%)]\tLoss: 1.585896\n",
      "Train Epoch: 6 [16000/49000 (33%)]\tLoss: 1.540026\n",
      "Train Epoch: 6 [17000/49000 (35%)]\tLoss: 1.599427\n",
      "Train Epoch: 6 [18000/49000 (37%)]\tLoss: 1.552816\n",
      "Train Epoch: 6 [19000/49000 (39%)]\tLoss: 1.539773\n",
      "Train Epoch: 6 [20000/49000 (41%)]\tLoss: 1.571720\n",
      "Train Epoch: 6 [21000/49000 (43%)]\tLoss: 1.556960\n",
      "Train Epoch: 6 [22000/49000 (45%)]\tLoss: 1.602377\n",
      "Train Epoch: 6 [23000/49000 (47%)]\tLoss: 1.588776\n",
      "Train Epoch: 6 [24000/49000 (49%)]\tLoss: 1.530651\n",
      "Train Epoch: 6 [25000/49000 (51%)]\tLoss: 1.603668\n",
      "Train Epoch: 6 [26000/49000 (53%)]\tLoss: 1.543670\n",
      "Train Epoch: 6 [27000/49000 (55%)]\tLoss: 1.592807\n",
      "Train Epoch: 6 [28000/49000 (57%)]\tLoss: 1.551981\n",
      "Train Epoch: 6 [29000/49000 (59%)]\tLoss: 1.556415\n",
      "Train Epoch: 6 [30000/49000 (61%)]\tLoss: 1.527026\n",
      "Train Epoch: 6 [31000/49000 (63%)]\tLoss: 1.596845\n",
      "Train Epoch: 6 [32000/49000 (65%)]\tLoss: 1.605719\n",
      "Train Epoch: 6 [33000/49000 (67%)]\tLoss: 1.559901\n",
      "Train Epoch: 6 [34000/49000 (69%)]\tLoss: 1.570730\n",
      "Train Epoch: 6 [35000/49000 (71%)]\tLoss: 1.594300\n",
      "Train Epoch: 6 [36000/49000 (73%)]\tLoss: 1.590469\n",
      "Train Epoch: 6 [37000/49000 (76%)]\tLoss: 1.572453\n",
      "Train Epoch: 6 [38000/49000 (78%)]\tLoss: 1.587745\n",
      "Train Epoch: 6 [39000/49000 (80%)]\tLoss: 1.518062\n",
      "Train Epoch: 6 [40000/49000 (82%)]\tLoss: 1.574454\n",
      "Train Epoch: 6 [41000/49000 (84%)]\tLoss: 1.574904\n",
      "Train Epoch: 6 [42000/49000 (86%)]\tLoss: 1.564160\n",
      "Train Epoch: 6 [43000/49000 (88%)]\tLoss: 1.534968\n",
      "Train Epoch: 6 [44000/49000 (90%)]\tLoss: 1.572286\n",
      "Train Epoch: 6 [45000/49000 (92%)]\tLoss: 1.549356\n",
      "Train Epoch: 6 [46000/49000 (94%)]\tLoss: 1.554927\n",
      "Train Epoch: 6 [47000/49000 (96%)]\tLoss: 1.606952\n",
      "Train Epoch: 6 [48000/49000 (98%)]\tLoss: 1.544712\n",
      "\n",
      "Test set: Avg. loss: 1.5725, Accuracy: 18827/21000 (89.65%)\n",
      "\n",
      "Train Epoch: 7 [0/49000 (0%)]\tLoss: 1.571078\n",
      "Train Epoch: 7 [1000/49000 (2%)]\tLoss: 1.615623\n",
      "Train Epoch: 7 [2000/49000 (4%)]\tLoss: 1.560591\n",
      "Train Epoch: 7 [3000/49000 (6%)]\tLoss: 1.607517\n",
      "Train Epoch: 7 [4000/49000 (8%)]\tLoss: 1.562575\n",
      "Train Epoch: 7 [5000/49000 (10%)]\tLoss: 1.586020\n",
      "Train Epoch: 7 [6000/49000 (12%)]\tLoss: 1.574842\n",
      "Train Epoch: 7 [7000/49000 (14%)]\tLoss: 1.590564\n",
      "Train Epoch: 7 [8000/49000 (16%)]\tLoss: 1.579444\n",
      "Train Epoch: 7 [9000/49000 (18%)]\tLoss: 1.563691\n",
      "Train Epoch: 7 [10000/49000 (20%)]\tLoss: 1.580211\n",
      "Train Epoch: 7 [11000/49000 (22%)]\tLoss: 1.565651\n",
      "Train Epoch: 7 [12000/49000 (24%)]\tLoss: 1.604840\n",
      "Train Epoch: 7 [13000/49000 (27%)]\tLoss: 1.584025\n",
      "Train Epoch: 7 [14000/49000 (29%)]\tLoss: 1.568361\n",
      "Train Epoch: 7 [15000/49000 (31%)]\tLoss: 1.541369\n",
      "Train Epoch: 7 [16000/49000 (33%)]\tLoss: 1.546487\n",
      "Train Epoch: 7 [17000/49000 (35%)]\tLoss: 1.556043\n",
      "Train Epoch: 7 [18000/49000 (37%)]\tLoss: 1.593407\n",
      "Train Epoch: 7 [19000/49000 (39%)]\tLoss: 1.538992\n",
      "Train Epoch: 7 [20000/49000 (41%)]\tLoss: 1.539477\n",
      "Train Epoch: 7 [21000/49000 (43%)]\tLoss: 1.601365\n",
      "Train Epoch: 7 [22000/49000 (45%)]\tLoss: 1.559217\n",
      "Train Epoch: 7 [23000/49000 (47%)]\tLoss: 1.577536\n",
      "Train Epoch: 7 [24000/49000 (49%)]\tLoss: 1.543568\n",
      "Train Epoch: 7 [25000/49000 (51%)]\tLoss: 1.578487\n",
      "Train Epoch: 7 [26000/49000 (53%)]\tLoss: 1.583000\n",
      "Train Epoch: 7 [27000/49000 (55%)]\tLoss: 1.601601\n",
      "Train Epoch: 7 [28000/49000 (57%)]\tLoss: 1.569486\n",
      "Train Epoch: 7 [29000/49000 (59%)]\tLoss: 1.542249\n",
      "Train Epoch: 7 [30000/49000 (61%)]\tLoss: 1.602742\n",
      "Train Epoch: 7 [31000/49000 (63%)]\tLoss: 1.562458\n",
      "Train Epoch: 7 [32000/49000 (65%)]\tLoss: 1.608652\n",
      "Train Epoch: 7 [33000/49000 (67%)]\tLoss: 1.562181\n",
      "Train Epoch: 7 [34000/49000 (69%)]\tLoss: 1.627271\n",
      "Train Epoch: 7 [35000/49000 (71%)]\tLoss: 1.569471\n",
      "Train Epoch: 7 [36000/49000 (73%)]\tLoss: 1.573288\n",
      "Train Epoch: 7 [37000/49000 (76%)]\tLoss: 1.553171\n",
      "Train Epoch: 7 [38000/49000 (78%)]\tLoss: 1.571791\n",
      "Train Epoch: 7 [39000/49000 (80%)]\tLoss: 1.564430\n",
      "Train Epoch: 7 [40000/49000 (82%)]\tLoss: 1.573419\n",
      "Train Epoch: 7 [41000/49000 (84%)]\tLoss: 1.538961\n",
      "Train Epoch: 7 [42000/49000 (86%)]\tLoss: 1.543035\n",
      "Train Epoch: 7 [43000/49000 (88%)]\tLoss: 1.557489\n",
      "Train Epoch: 7 [44000/49000 (90%)]\tLoss: 1.586833\n",
      "Train Epoch: 7 [45000/49000 (92%)]\tLoss: 1.599675\n",
      "Train Epoch: 7 [46000/49000 (94%)]\tLoss: 1.562775\n",
      "Train Epoch: 7 [47000/49000 (96%)]\tLoss: 1.562903\n",
      "Train Epoch: 7 [48000/49000 (98%)]\tLoss: 1.565244\n",
      "\n",
      "Test set: Avg. loss: 1.5738, Accuracy: 18789/21000 (89.47%)\n",
      "\n",
      "Train Epoch: 8 [0/49000 (0%)]\tLoss: 1.558843\n",
      "Train Epoch: 8 [1000/49000 (2%)]\tLoss: 1.603814\n",
      "Train Epoch: 8 [2000/49000 (4%)]\tLoss: 1.557825\n",
      "Train Epoch: 8 [3000/49000 (6%)]\tLoss: 1.577994\n",
      "Train Epoch: 8 [4000/49000 (8%)]\tLoss: 1.541300\n",
      "Train Epoch: 8 [5000/49000 (10%)]\tLoss: 1.585838\n",
      "Train Epoch: 8 [6000/49000 (12%)]\tLoss: 1.554626\n",
      "Train Epoch: 8 [7000/49000 (14%)]\tLoss: 1.571325\n",
      "Train Epoch: 8 [8000/49000 (16%)]\tLoss: 1.565767\n",
      "Train Epoch: 8 [9000/49000 (18%)]\tLoss: 1.554698\n",
      "Train Epoch: 8 [10000/49000 (20%)]\tLoss: 1.544700\n",
      "Train Epoch: 8 [11000/49000 (22%)]\tLoss: 1.629640\n",
      "Train Epoch: 8 [12000/49000 (24%)]\tLoss: 1.576620\n",
      "Train Epoch: 8 [13000/49000 (27%)]\tLoss: 1.549861\n",
      "Train Epoch: 8 [14000/49000 (29%)]\tLoss: 1.540933\n",
      "Train Epoch: 8 [15000/49000 (31%)]\tLoss: 1.579026\n",
      "Train Epoch: 8 [16000/49000 (33%)]\tLoss: 1.594480\n",
      "Train Epoch: 8 [17000/49000 (35%)]\tLoss: 1.558261\n",
      "Train Epoch: 8 [18000/49000 (37%)]\tLoss: 1.581041\n",
      "Train Epoch: 8 [19000/49000 (39%)]\tLoss: 1.555783\n",
      "Train Epoch: 8 [20000/49000 (41%)]\tLoss: 1.563598\n",
      "Train Epoch: 8 [21000/49000 (43%)]\tLoss: 1.531645\n",
      "Train Epoch: 8 [22000/49000 (45%)]\tLoss: 1.551303\n",
      "Train Epoch: 8 [23000/49000 (47%)]\tLoss: 1.581985\n",
      "Train Epoch: 8 [24000/49000 (49%)]\tLoss: 1.580193\n",
      "Train Epoch: 8 [25000/49000 (51%)]\tLoss: 1.578884\n",
      "Train Epoch: 8 [26000/49000 (53%)]\tLoss: 1.551363\n",
      "Train Epoch: 8 [27000/49000 (55%)]\tLoss: 1.543805\n",
      "Train Epoch: 8 [28000/49000 (57%)]\tLoss: 1.539268\n",
      "Train Epoch: 8 [29000/49000 (59%)]\tLoss: 1.616669\n",
      "Train Epoch: 8 [30000/49000 (61%)]\tLoss: 1.572199\n",
      "Train Epoch: 8 [31000/49000 (63%)]\tLoss: 1.566745\n",
      "Train Epoch: 8 [32000/49000 (65%)]\tLoss: 1.555172\n",
      "Train Epoch: 8 [33000/49000 (67%)]\tLoss: 1.526004\n",
      "Train Epoch: 8 [34000/49000 (69%)]\tLoss: 1.532005\n",
      "Train Epoch: 8 [35000/49000 (71%)]\tLoss: 1.568309\n",
      "Train Epoch: 8 [36000/49000 (73%)]\tLoss: 1.544074\n",
      "Train Epoch: 8 [37000/49000 (76%)]\tLoss: 1.567896\n",
      "Train Epoch: 8 [38000/49000 (78%)]\tLoss: 1.602765\n",
      "Train Epoch: 8 [39000/49000 (80%)]\tLoss: 1.556183\n",
      "Train Epoch: 8 [40000/49000 (82%)]\tLoss: 1.594148\n",
      "Train Epoch: 8 [41000/49000 (84%)]\tLoss: 1.553264\n",
      "Train Epoch: 8 [42000/49000 (86%)]\tLoss: 1.545418\n",
      "Train Epoch: 8 [43000/49000 (88%)]\tLoss: 1.551256\n",
      "Train Epoch: 8 [44000/49000 (90%)]\tLoss: 1.566884\n",
      "Train Epoch: 8 [45000/49000 (92%)]\tLoss: 1.581618\n",
      "Train Epoch: 8 [46000/49000 (94%)]\tLoss: 1.534251\n",
      "Train Epoch: 8 [47000/49000 (96%)]\tLoss: 1.542873\n",
      "Train Epoch: 8 [48000/49000 (98%)]\tLoss: 1.595022\n",
      "\n",
      "Test set: Avg. loss: 1.5698, Accuracy: 18863/21000 (89.82%)\n",
      "\n",
      "Train Epoch: 9 [0/49000 (0%)]\tLoss: 1.570835\n",
      "Train Epoch: 9 [1000/49000 (2%)]\tLoss: 1.594096\n",
      "Train Epoch: 9 [2000/49000 (4%)]\tLoss: 1.585749\n",
      "Train Epoch: 9 [3000/49000 (6%)]\tLoss: 1.556032\n",
      "Train Epoch: 9 [4000/49000 (8%)]\tLoss: 1.582314\n",
      "Train Epoch: 9 [5000/49000 (10%)]\tLoss: 1.574350\n",
      "Train Epoch: 9 [6000/49000 (12%)]\tLoss: 1.522267\n",
      "Train Epoch: 9 [7000/49000 (14%)]\tLoss: 1.583032\n",
      "Train Epoch: 9 [8000/49000 (16%)]\tLoss: 1.563933\n",
      "Train Epoch: 9 [9000/49000 (18%)]\tLoss: 1.547499\n",
      "Train Epoch: 9 [10000/49000 (20%)]\tLoss: 1.574435\n",
      "Train Epoch: 9 [11000/49000 (22%)]\tLoss: 1.552344\n",
      "Train Epoch: 9 [12000/49000 (24%)]\tLoss: 1.545426\n",
      "Train Epoch: 9 [13000/49000 (27%)]\tLoss: 1.542657\n",
      "Train Epoch: 9 [14000/49000 (29%)]\tLoss: 1.561111\n",
      "Train Epoch: 9 [15000/49000 (31%)]\tLoss: 1.561300\n",
      "Train Epoch: 9 [16000/49000 (33%)]\tLoss: 1.582040\n",
      "Train Epoch: 9 [17000/49000 (35%)]\tLoss: 1.604496\n",
      "Train Epoch: 9 [18000/49000 (37%)]\tLoss: 1.562240\n",
      "Train Epoch: 9 [19000/49000 (39%)]\tLoss: 1.602271\n",
      "Train Epoch: 9 [20000/49000 (41%)]\tLoss: 1.591836\n",
      "Train Epoch: 9 [21000/49000 (43%)]\tLoss: 1.579763\n",
      "Train Epoch: 9 [22000/49000 (45%)]\tLoss: 1.547415\n",
      "Train Epoch: 9 [23000/49000 (47%)]\tLoss: 1.557719\n",
      "Train Epoch: 9 [24000/49000 (49%)]\tLoss: 1.575487\n",
      "Train Epoch: 9 [25000/49000 (51%)]\tLoss: 1.549006\n",
      "Train Epoch: 9 [26000/49000 (53%)]\tLoss: 1.534140\n",
      "Train Epoch: 9 [27000/49000 (55%)]\tLoss: 1.562443\n",
      "Train Epoch: 9 [28000/49000 (57%)]\tLoss: 1.584129\n",
      "Train Epoch: 9 [29000/49000 (59%)]\tLoss: 1.523328\n",
      "Train Epoch: 9 [30000/49000 (61%)]\tLoss: 1.568345\n",
      "Train Epoch: 9 [31000/49000 (63%)]\tLoss: 1.549341\n",
      "Train Epoch: 9 [32000/49000 (65%)]\tLoss: 1.578771\n",
      "Train Epoch: 9 [33000/49000 (67%)]\tLoss: 1.536596\n",
      "Train Epoch: 9 [34000/49000 (69%)]\tLoss: 1.534232\n",
      "Train Epoch: 9 [35000/49000 (71%)]\tLoss: 1.533760\n",
      "Train Epoch: 9 [36000/49000 (73%)]\tLoss: 1.558582\n",
      "Train Epoch: 9 [37000/49000 (76%)]\tLoss: 1.551633\n",
      "Train Epoch: 9 [38000/49000 (78%)]\tLoss: 1.584763\n",
      "Train Epoch: 9 [39000/49000 (80%)]\tLoss: 1.535360\n",
      "Train Epoch: 9 [40000/49000 (82%)]\tLoss: 1.550338\n",
      "Train Epoch: 9 [41000/49000 (84%)]\tLoss: 1.531254\n",
      "Train Epoch: 9 [42000/49000 (86%)]\tLoss: 1.549694\n",
      "Train Epoch: 9 [43000/49000 (88%)]\tLoss: 1.547811\n",
      "Train Epoch: 9 [44000/49000 (90%)]\tLoss: 1.559677\n",
      "Train Epoch: 9 [45000/49000 (92%)]\tLoss: 1.540210\n",
      "Train Epoch: 9 [46000/49000 (94%)]\tLoss: 1.558929\n",
      "Train Epoch: 9 [47000/49000 (96%)]\tLoss: 1.522127\n",
      "Train Epoch: 9 [48000/49000 (98%)]\tLoss: 1.564021\n",
      "\n",
      "Test set: Avg. loss: 1.5683, Accuracy: 18824/21000 (89.64%)\n",
      "\n",
      "Train Epoch: 10 [0/49000 (0%)]\tLoss: 1.559456\n",
      "Train Epoch: 10 [1000/49000 (2%)]\tLoss: 1.556928\n",
      "Train Epoch: 10 [2000/49000 (4%)]\tLoss: 1.557768\n",
      "Train Epoch: 10 [3000/49000 (6%)]\tLoss: 1.529354\n",
      "Train Epoch: 10 [4000/49000 (8%)]\tLoss: 1.540528\n",
      "Train Epoch: 10 [5000/49000 (10%)]\tLoss: 1.530790\n",
      "Train Epoch: 10 [6000/49000 (12%)]\tLoss: 1.601627\n",
      "Train Epoch: 10 [7000/49000 (14%)]\tLoss: 1.526733\n",
      "Train Epoch: 10 [8000/49000 (16%)]\tLoss: 1.598638\n",
      "Train Epoch: 10 [9000/49000 (18%)]\tLoss: 1.561889\n",
      "Train Epoch: 10 [10000/49000 (20%)]\tLoss: 1.547565\n",
      "Train Epoch: 10 [11000/49000 (22%)]\tLoss: 1.558717\n",
      "Train Epoch: 10 [12000/49000 (24%)]\tLoss: 1.546546\n",
      "Train Epoch: 10 [13000/49000 (27%)]\tLoss: 1.574835\n",
      "Train Epoch: 10 [14000/49000 (29%)]\tLoss: 1.552025\n",
      "Train Epoch: 10 [15000/49000 (31%)]\tLoss: 1.538162\n",
      "Train Epoch: 10 [16000/49000 (33%)]\tLoss: 1.578789\n",
      "Train Epoch: 10 [17000/49000 (35%)]\tLoss: 1.579306\n",
      "Train Epoch: 10 [18000/49000 (37%)]\tLoss: 1.545360\n",
      "Train Epoch: 10 [19000/49000 (39%)]\tLoss: 1.580827\n",
      "Train Epoch: 10 [20000/49000 (41%)]\tLoss: 1.575873\n",
      "Train Epoch: 10 [21000/49000 (43%)]\tLoss: 1.586013\n",
      "Train Epoch: 10 [22000/49000 (45%)]\tLoss: 1.568047\n",
      "Train Epoch: 10 [23000/49000 (47%)]\tLoss: 1.553925\n",
      "Train Epoch: 10 [24000/49000 (49%)]\tLoss: 1.610043\n",
      "Train Epoch: 10 [25000/49000 (51%)]\tLoss: 1.593793\n",
      "Train Epoch: 10 [26000/49000 (53%)]\tLoss: 1.557900\n",
      "Train Epoch: 10 [27000/49000 (55%)]\tLoss: 1.587607\n",
      "Train Epoch: 10 [28000/49000 (57%)]\tLoss: 1.579394\n",
      "Train Epoch: 10 [29000/49000 (59%)]\tLoss: 1.548866\n",
      "Train Epoch: 10 [30000/49000 (61%)]\tLoss: 1.609907\n",
      "Train Epoch: 10 [31000/49000 (63%)]\tLoss: 1.558831\n",
      "Train Epoch: 10 [32000/49000 (65%)]\tLoss: 1.607620\n",
      "Train Epoch: 10 [33000/49000 (67%)]\tLoss: 1.547374\n",
      "Train Epoch: 10 [34000/49000 (69%)]\tLoss: 1.551516\n",
      "Train Epoch: 10 [35000/49000 (71%)]\tLoss: 1.545509\n",
      "Train Epoch: 10 [36000/49000 (73%)]\tLoss: 1.557784\n",
      "Train Epoch: 10 [37000/49000 (76%)]\tLoss: 1.548092\n",
      "Train Epoch: 10 [38000/49000 (78%)]\tLoss: 1.538526\n",
      "Train Epoch: 10 [39000/49000 (80%)]\tLoss: 1.545480\n",
      "Train Epoch: 10 [40000/49000 (82%)]\tLoss: 1.587621\n",
      "Train Epoch: 10 [41000/49000 (84%)]\tLoss: 1.559362\n",
      "Train Epoch: 10 [42000/49000 (86%)]\tLoss: 1.591342\n",
      "Train Epoch: 10 [43000/49000 (88%)]\tLoss: 1.572241\n",
      "Train Epoch: 10 [44000/49000 (90%)]\tLoss: 1.543881\n",
      "Train Epoch: 10 [45000/49000 (92%)]\tLoss: 1.550002\n",
      "Train Epoch: 10 [46000/49000 (94%)]\tLoss: 1.581472\n",
      "Train Epoch: 10 [47000/49000 (96%)]\tLoss: 1.543017\n",
      "Train Epoch: 10 [48000/49000 (98%)]\tLoss: 1.520849\n",
      "\n",
      "Test set: Avg. loss: 1.5644, Accuracy: 18921/21000 (90.10%)\n",
      "\n",
      "Train Epoch: 11 [0/49000 (0%)]\tLoss: 1.562554\n",
      "Train Epoch: 11 [1000/49000 (2%)]\tLoss: 1.556323\n",
      "Train Epoch: 11 [2000/49000 (4%)]\tLoss: 1.564191\n",
      "Train Epoch: 11 [3000/49000 (6%)]\tLoss: 1.538344\n",
      "Train Epoch: 11 [4000/49000 (8%)]\tLoss: 1.587373\n",
      "Train Epoch: 11 [5000/49000 (10%)]\tLoss: 1.546428\n",
      "Train Epoch: 11 [6000/49000 (12%)]\tLoss: 1.542832\n",
      "Train Epoch: 11 [7000/49000 (14%)]\tLoss: 1.541940\n",
      "Train Epoch: 11 [8000/49000 (16%)]\tLoss: 1.602167\n",
      "Train Epoch: 11 [9000/49000 (18%)]\tLoss: 1.534701\n",
      "Train Epoch: 11 [10000/49000 (20%)]\tLoss: 1.564824\n",
      "Train Epoch: 11 [11000/49000 (22%)]\tLoss: 1.557186\n",
      "Train Epoch: 11 [12000/49000 (24%)]\tLoss: 1.561956\n",
      "Train Epoch: 11 [13000/49000 (27%)]\tLoss: 1.588077\n",
      "Train Epoch: 11 [14000/49000 (29%)]\tLoss: 1.562643\n",
      "Train Epoch: 11 [15000/49000 (31%)]\tLoss: 1.581409\n",
      "Train Epoch: 11 [16000/49000 (33%)]\tLoss: 1.539165\n",
      "Train Epoch: 11 [17000/49000 (35%)]\tLoss: 1.608739\n",
      "Train Epoch: 11 [18000/49000 (37%)]\tLoss: 1.550522\n",
      "Train Epoch: 11 [19000/49000 (39%)]\tLoss: 1.583704\n",
      "Train Epoch: 11 [20000/49000 (41%)]\tLoss: 1.560170\n",
      "Train Epoch: 11 [21000/49000 (43%)]\tLoss: 1.529923\n",
      "Train Epoch: 11 [22000/49000 (45%)]\tLoss: 1.556192\n",
      "Train Epoch: 11 [23000/49000 (47%)]\tLoss: 1.570928\n",
      "Train Epoch: 11 [24000/49000 (49%)]\tLoss: 1.554321\n",
      "Train Epoch: 11 [25000/49000 (51%)]\tLoss: 1.532503\n",
      "Train Epoch: 11 [26000/49000 (53%)]\tLoss: 1.573753\n",
      "Train Epoch: 11 [27000/49000 (55%)]\tLoss: 1.552861\n",
      "Train Epoch: 11 [28000/49000 (57%)]\tLoss: 1.527324\n",
      "Train Epoch: 11 [29000/49000 (59%)]\tLoss: 1.550510\n",
      "Train Epoch: 11 [30000/49000 (61%)]\tLoss: 1.558728\n",
      "Train Epoch: 11 [31000/49000 (63%)]\tLoss: 1.568387\n",
      "Train Epoch: 11 [32000/49000 (65%)]\tLoss: 1.590690\n",
      "Train Epoch: 11 [33000/49000 (67%)]\tLoss: 1.571581\n",
      "Train Epoch: 11 [34000/49000 (69%)]\tLoss: 1.536850\n",
      "Train Epoch: 11 [35000/49000 (71%)]\tLoss: 1.579887\n",
      "Train Epoch: 11 [36000/49000 (73%)]\tLoss: 1.620445\n",
      "Train Epoch: 11 [37000/49000 (76%)]\tLoss: 1.563316\n",
      "Train Epoch: 11 [38000/49000 (78%)]\tLoss: 1.566575\n",
      "Train Epoch: 11 [39000/49000 (80%)]\tLoss: 1.570106\n",
      "Train Epoch: 11 [40000/49000 (82%)]\tLoss: 1.536977\n",
      "Train Epoch: 11 [41000/49000 (84%)]\tLoss: 1.543749\n",
      "Train Epoch: 11 [42000/49000 (86%)]\tLoss: 1.579100\n",
      "Train Epoch: 11 [43000/49000 (88%)]\tLoss: 1.587025\n",
      "Train Epoch: 11 [44000/49000 (90%)]\tLoss: 1.551518\n",
      "Train Epoch: 11 [45000/49000 (92%)]\tLoss: 1.560410\n",
      "Train Epoch: 11 [46000/49000 (94%)]\tLoss: 1.533944\n",
      "Train Epoch: 11 [47000/49000 (96%)]\tLoss: 1.562083\n",
      "Train Epoch: 11 [48000/49000 (98%)]\tLoss: 1.551715\n",
      "\n",
      "Test set: Avg. loss: 1.5647, Accuracy: 18917/21000 (90.08%)\n",
      "\n",
      "Train Epoch: 12 [0/49000 (0%)]\tLoss: 1.555081\n",
      "Train Epoch: 12 [1000/49000 (2%)]\tLoss: 1.570994\n",
      "Train Epoch: 12 [2000/49000 (4%)]\tLoss: 1.586909\n",
      "Train Epoch: 12 [3000/49000 (6%)]\tLoss: 1.537936\n",
      "Train Epoch: 12 [4000/49000 (8%)]\tLoss: 1.545531\n",
      "Train Epoch: 12 [5000/49000 (10%)]\tLoss: 1.516365\n",
      "Train Epoch: 12 [6000/49000 (12%)]\tLoss: 1.520445\n",
      "Train Epoch: 12 [7000/49000 (14%)]\tLoss: 1.550277\n",
      "Train Epoch: 12 [8000/49000 (16%)]\tLoss: 1.577024\n",
      "Train Epoch: 12 [9000/49000 (18%)]\tLoss: 1.598245\n",
      "Train Epoch: 12 [10000/49000 (20%)]\tLoss: 1.561070\n",
      "Train Epoch: 12 [11000/49000 (22%)]\tLoss: 1.568229\n",
      "Train Epoch: 12 [12000/49000 (24%)]\tLoss: 1.543257\n",
      "Train Epoch: 12 [13000/49000 (27%)]\tLoss: 1.564208\n",
      "Train Epoch: 12 [14000/49000 (29%)]\tLoss: 1.576198\n",
      "Train Epoch: 12 [15000/49000 (31%)]\tLoss: 1.597480\n",
      "Train Epoch: 12 [16000/49000 (33%)]\tLoss: 1.568222\n",
      "Train Epoch: 12 [17000/49000 (35%)]\tLoss: 1.534827\n",
      "Train Epoch: 12 [18000/49000 (37%)]\tLoss: 1.550800\n",
      "Train Epoch: 12 [19000/49000 (39%)]\tLoss: 1.565816\n",
      "Train Epoch: 12 [20000/49000 (41%)]\tLoss: 1.548923\n",
      "Train Epoch: 12 [21000/49000 (43%)]\tLoss: 1.572116\n",
      "Train Epoch: 12 [22000/49000 (45%)]\tLoss: 1.567305\n",
      "Train Epoch: 12 [23000/49000 (47%)]\tLoss: 1.583774\n",
      "Train Epoch: 12 [24000/49000 (49%)]\tLoss: 1.545286\n",
      "Train Epoch: 12 [25000/49000 (51%)]\tLoss: 1.538889\n",
      "Train Epoch: 12 [26000/49000 (53%)]\tLoss: 1.565448\n",
      "Train Epoch: 12 [27000/49000 (55%)]\tLoss: 1.583922\n",
      "Train Epoch: 12 [28000/49000 (57%)]\tLoss: 1.546466\n",
      "Train Epoch: 12 [29000/49000 (59%)]\tLoss: 1.563197\n",
      "Train Epoch: 12 [30000/49000 (61%)]\tLoss: 1.557193\n",
      "Train Epoch: 12 [31000/49000 (63%)]\tLoss: 1.547184\n",
      "Train Epoch: 12 [32000/49000 (65%)]\tLoss: 1.518873\n",
      "Train Epoch: 12 [33000/49000 (67%)]\tLoss: 1.571687\n",
      "Train Epoch: 12 [34000/49000 (69%)]\tLoss: 1.558782\n",
      "Train Epoch: 12 [35000/49000 (71%)]\tLoss: 1.556522\n",
      "Train Epoch: 12 [36000/49000 (73%)]\tLoss: 1.562435\n",
      "Train Epoch: 12 [37000/49000 (76%)]\tLoss: 1.556743\n",
      "Train Epoch: 12 [38000/49000 (78%)]\tLoss: 1.576238\n",
      "Train Epoch: 12 [39000/49000 (80%)]\tLoss: 1.581698\n",
      "Train Epoch: 12 [40000/49000 (82%)]\tLoss: 1.576074\n",
      "Train Epoch: 12 [41000/49000 (84%)]\tLoss: 1.559912\n",
      "Train Epoch: 12 [42000/49000 (86%)]\tLoss: 1.581117\n",
      "Train Epoch: 12 [43000/49000 (88%)]\tLoss: 1.532962\n",
      "Train Epoch: 12 [44000/49000 (90%)]\tLoss: 1.565271\n",
      "Train Epoch: 12 [45000/49000 (92%)]\tLoss: 1.572321\n",
      "Train Epoch: 12 [46000/49000 (94%)]\tLoss: 1.520183\n",
      "Train Epoch: 12 [47000/49000 (96%)]\tLoss: 1.532013\n",
      "Train Epoch: 12 [48000/49000 (98%)]\tLoss: 1.574879\n",
      "\n",
      "Test set: Avg. loss: 1.5636, Accuracy: 18883/21000 (89.92%)\n",
      "\n",
      "Train Epoch: 13 [0/49000 (0%)]\tLoss: 1.549682\n",
      "Train Epoch: 13 [1000/49000 (2%)]\tLoss: 1.543645\n",
      "Train Epoch: 13 [2000/49000 (4%)]\tLoss: 1.568081\n",
      "Train Epoch: 13 [3000/49000 (6%)]\tLoss: 1.537623\n",
      "Train Epoch: 13 [4000/49000 (8%)]\tLoss: 1.555264\n",
      "Train Epoch: 13 [5000/49000 (10%)]\tLoss: 1.544002\n",
      "Train Epoch: 13 [6000/49000 (12%)]\tLoss: 1.530735\n",
      "Train Epoch: 13 [7000/49000 (14%)]\tLoss: 1.530093\n",
      "Train Epoch: 13 [8000/49000 (16%)]\tLoss: 1.544784\n",
      "Train Epoch: 13 [9000/49000 (18%)]\tLoss: 1.554933\n",
      "Train Epoch: 13 [10000/49000 (20%)]\tLoss: 1.564502\n",
      "Train Epoch: 13 [11000/49000 (22%)]\tLoss: 1.558971\n",
      "Train Epoch: 13 [12000/49000 (24%)]\tLoss: 1.576988\n",
      "Train Epoch: 13 [13000/49000 (27%)]\tLoss: 1.520089\n",
      "Train Epoch: 13 [14000/49000 (29%)]\tLoss: 1.538599\n",
      "Train Epoch: 13 [15000/49000 (31%)]\tLoss: 1.530178\n",
      "Train Epoch: 13 [16000/49000 (33%)]\tLoss: 1.562007\n",
      "Train Epoch: 13 [17000/49000 (35%)]\tLoss: 1.556076\n",
      "Train Epoch: 13 [18000/49000 (37%)]\tLoss: 1.562906\n",
      "Train Epoch: 13 [19000/49000 (39%)]\tLoss: 1.571057\n",
      "Train Epoch: 13 [20000/49000 (41%)]\tLoss: 1.591533\n",
      "Train Epoch: 13 [21000/49000 (43%)]\tLoss: 1.573463\n",
      "Train Epoch: 13 [22000/49000 (45%)]\tLoss: 1.540991\n",
      "Train Epoch: 13 [23000/49000 (47%)]\tLoss: 1.534349\n",
      "Train Epoch: 13 [24000/49000 (49%)]\tLoss: 1.543578\n",
      "Train Epoch: 13 [25000/49000 (51%)]\tLoss: 1.555744\n",
      "Train Epoch: 13 [26000/49000 (53%)]\tLoss: 1.561854\n",
      "Train Epoch: 13 [27000/49000 (55%)]\tLoss: 1.582371\n",
      "Train Epoch: 13 [28000/49000 (57%)]\tLoss: 1.561632\n",
      "Train Epoch: 13 [29000/49000 (59%)]\tLoss: 1.578301\n",
      "Train Epoch: 13 [30000/49000 (61%)]\tLoss: 1.563140\n",
      "Train Epoch: 13 [31000/49000 (63%)]\tLoss: 1.566039\n",
      "Train Epoch: 13 [32000/49000 (65%)]\tLoss: 1.539017\n",
      "Train Epoch: 13 [33000/49000 (67%)]\tLoss: 1.532494\n",
      "Train Epoch: 13 [34000/49000 (69%)]\tLoss: 1.574644\n",
      "Train Epoch: 13 [35000/49000 (71%)]\tLoss: 1.516078\n",
      "Train Epoch: 13 [36000/49000 (73%)]\tLoss: 1.530284\n",
      "Train Epoch: 13 [37000/49000 (76%)]\tLoss: 1.508447\n",
      "Train Epoch: 13 [38000/49000 (78%)]\tLoss: 1.523529\n",
      "Train Epoch: 13 [39000/49000 (80%)]\tLoss: 1.574396\n",
      "Train Epoch: 13 [40000/49000 (82%)]\tLoss: 1.559817\n",
      "Train Epoch: 13 [41000/49000 (84%)]\tLoss: 1.531381\n",
      "Train Epoch: 13 [42000/49000 (86%)]\tLoss: 1.565640\n",
      "Train Epoch: 13 [43000/49000 (88%)]\tLoss: 1.562072\n",
      "Train Epoch: 13 [44000/49000 (90%)]\tLoss: 1.526634\n",
      "Train Epoch: 13 [45000/49000 (92%)]\tLoss: 1.566523\n",
      "Train Epoch: 13 [46000/49000 (94%)]\tLoss: 1.572618\n",
      "Train Epoch: 13 [47000/49000 (96%)]\tLoss: 1.580013\n",
      "Train Epoch: 13 [48000/49000 (98%)]\tLoss: 1.581347\n",
      "\n",
      "Test set: Avg. loss: 1.5621, Accuracy: 18981/21000 (90.39%)\n",
      "\n",
      "Train Epoch: 14 [0/49000 (0%)]\tLoss: 1.540292\n",
      "Train Epoch: 14 [1000/49000 (2%)]\tLoss: 1.600640\n",
      "Train Epoch: 14 [2000/49000 (4%)]\tLoss: 1.559000\n",
      "Train Epoch: 14 [3000/49000 (6%)]\tLoss: 1.568002\n",
      "Train Epoch: 14 [4000/49000 (8%)]\tLoss: 1.558266\n",
      "Train Epoch: 14 [5000/49000 (10%)]\tLoss: 1.554900\n",
      "Train Epoch: 14 [6000/49000 (12%)]\tLoss: 1.540882\n",
      "Train Epoch: 14 [7000/49000 (14%)]\tLoss: 1.576166\n",
      "Train Epoch: 14 [8000/49000 (16%)]\tLoss: 1.568027\n",
      "Train Epoch: 14 [9000/49000 (18%)]\tLoss: 1.534194\n",
      "Train Epoch: 14 [10000/49000 (20%)]\tLoss: 1.566745\n",
      "Train Epoch: 14 [11000/49000 (22%)]\tLoss: 1.556385\n",
      "Train Epoch: 14 [12000/49000 (24%)]\tLoss: 1.558074\n",
      "Train Epoch: 14 [13000/49000 (27%)]\tLoss: 1.533528\n",
      "Train Epoch: 14 [14000/49000 (29%)]\tLoss: 1.559215\n",
      "Train Epoch: 14 [15000/49000 (31%)]\tLoss: 1.549144\n",
      "Train Epoch: 14 [16000/49000 (33%)]\tLoss: 1.576924\n",
      "Train Epoch: 14 [17000/49000 (35%)]\tLoss: 1.551919\n",
      "Train Epoch: 14 [18000/49000 (37%)]\tLoss: 1.556304\n",
      "Train Epoch: 14 [19000/49000 (39%)]\tLoss: 1.576302\n",
      "Train Epoch: 14 [20000/49000 (41%)]\tLoss: 1.593240\n",
      "Train Epoch: 14 [21000/49000 (43%)]\tLoss: 1.541444\n",
      "Train Epoch: 14 [22000/49000 (45%)]\tLoss: 1.559706\n",
      "Train Epoch: 14 [23000/49000 (47%)]\tLoss: 1.575953\n",
      "Train Epoch: 14 [24000/49000 (49%)]\tLoss: 1.568085\n",
      "Train Epoch: 14 [25000/49000 (51%)]\tLoss: 1.532624\n",
      "Train Epoch: 14 [26000/49000 (53%)]\tLoss: 1.561023\n",
      "Train Epoch: 14 [27000/49000 (55%)]\tLoss: 1.555623\n",
      "Train Epoch: 14 [28000/49000 (57%)]\tLoss: 1.544148\n",
      "Train Epoch: 14 [29000/49000 (59%)]\tLoss: 1.558981\n",
      "Train Epoch: 14 [30000/49000 (61%)]\tLoss: 1.545429\n",
      "Train Epoch: 14 [31000/49000 (63%)]\tLoss: 1.544930\n",
      "Train Epoch: 14 [32000/49000 (65%)]\tLoss: 1.592871\n",
      "Train Epoch: 14 [33000/49000 (67%)]\tLoss: 1.557446\n",
      "Train Epoch: 14 [34000/49000 (69%)]\tLoss: 1.524566\n",
      "Train Epoch: 14 [35000/49000 (71%)]\tLoss: 1.592569\n",
      "Train Epoch: 14 [36000/49000 (73%)]\tLoss: 1.533623\n",
      "Train Epoch: 14 [37000/49000 (76%)]\tLoss: 1.572739\n",
      "Train Epoch: 14 [38000/49000 (78%)]\tLoss: 1.545044\n",
      "Train Epoch: 14 [39000/49000 (80%)]\tLoss: 1.572411\n",
      "Train Epoch: 14 [40000/49000 (82%)]\tLoss: 1.554708\n",
      "Train Epoch: 14 [41000/49000 (84%)]\tLoss: 1.549745\n",
      "Train Epoch: 14 [42000/49000 (86%)]\tLoss: 1.567556\n",
      "Train Epoch: 14 [43000/49000 (88%)]\tLoss: 1.569080\n",
      "Train Epoch: 14 [44000/49000 (90%)]\tLoss: 1.583167\n",
      "Train Epoch: 14 [45000/49000 (92%)]\tLoss: 1.530231\n",
      "Train Epoch: 14 [46000/49000 (94%)]\tLoss: 1.534834\n",
      "Train Epoch: 14 [47000/49000 (96%)]\tLoss: 1.557657\n",
      "Train Epoch: 14 [48000/49000 (98%)]\tLoss: 1.529741\n",
      "\n",
      "Test set: Avg. loss: 1.5610, Accuracy: 18983/21000 (90.40%)\n",
      "\n",
      "Train Epoch: 15 [0/49000 (0%)]\tLoss: 1.544401\n",
      "Train Epoch: 15 [1000/49000 (2%)]\tLoss: 1.532945\n",
      "Train Epoch: 15 [2000/49000 (4%)]\tLoss: 1.537391\n",
      "Train Epoch: 15 [3000/49000 (6%)]\tLoss: 1.575809\n",
      "Train Epoch: 15 [4000/49000 (8%)]\tLoss: 1.571437\n",
      "Train Epoch: 15 [5000/49000 (10%)]\tLoss: 1.528382\n",
      "Train Epoch: 15 [6000/49000 (12%)]\tLoss: 1.565991\n",
      "Train Epoch: 15 [7000/49000 (14%)]\tLoss: 1.555770\n",
      "Train Epoch: 15 [8000/49000 (16%)]\tLoss: 1.554552\n",
      "Train Epoch: 15 [9000/49000 (18%)]\tLoss: 1.562529\n",
      "Train Epoch: 15 [10000/49000 (20%)]\tLoss: 1.523957\n",
      "Train Epoch: 15 [11000/49000 (22%)]\tLoss: 1.529490\n",
      "Train Epoch: 15 [12000/49000 (24%)]\tLoss: 1.521819\n",
      "Train Epoch: 15 [13000/49000 (27%)]\tLoss: 1.570821\n",
      "Train Epoch: 15 [14000/49000 (29%)]\tLoss: 1.558528\n",
      "Train Epoch: 15 [15000/49000 (31%)]\tLoss: 1.536898\n",
      "Train Epoch: 15 [16000/49000 (33%)]\tLoss: 1.525536\n",
      "Train Epoch: 15 [17000/49000 (35%)]\tLoss: 1.545213\n",
      "Train Epoch: 15 [18000/49000 (37%)]\tLoss: 1.547904\n",
      "Train Epoch: 15 [19000/49000 (39%)]\tLoss: 1.545056\n",
      "Train Epoch: 15 [20000/49000 (41%)]\tLoss: 1.542042\n",
      "Train Epoch: 15 [21000/49000 (43%)]\tLoss: 1.579104\n",
      "Train Epoch: 15 [22000/49000 (45%)]\tLoss: 1.544450\n",
      "Train Epoch: 15 [23000/49000 (47%)]\tLoss: 1.562472\n",
      "Train Epoch: 15 [24000/49000 (49%)]\tLoss: 1.613810\n",
      "Train Epoch: 15 [25000/49000 (51%)]\tLoss: 1.533492\n",
      "Train Epoch: 15 [26000/49000 (53%)]\tLoss: 1.536285\n",
      "Train Epoch: 15 [27000/49000 (55%)]\tLoss: 1.543540\n",
      "Train Epoch: 15 [28000/49000 (57%)]\tLoss: 1.575975\n",
      "Train Epoch: 15 [29000/49000 (59%)]\tLoss: 1.545245\n",
      "Train Epoch: 15 [30000/49000 (61%)]\tLoss: 1.542462\n",
      "Train Epoch: 15 [31000/49000 (63%)]\tLoss: 1.557166\n",
      "Train Epoch: 15 [32000/49000 (65%)]\tLoss: 1.516850\n",
      "Train Epoch: 15 [33000/49000 (67%)]\tLoss: 1.532515\n",
      "Train Epoch: 15 [34000/49000 (69%)]\tLoss: 1.586042\n",
      "Train Epoch: 15 [35000/49000 (71%)]\tLoss: 1.546759\n",
      "Train Epoch: 15 [36000/49000 (73%)]\tLoss: 1.547552\n",
      "Train Epoch: 15 [37000/49000 (76%)]\tLoss: 1.534598\n",
      "Train Epoch: 15 [38000/49000 (78%)]\tLoss: 1.543327\n",
      "Train Epoch: 15 [39000/49000 (80%)]\tLoss: 1.559986\n",
      "Train Epoch: 15 [40000/49000 (82%)]\tLoss: 1.531124\n",
      "Train Epoch: 15 [41000/49000 (84%)]\tLoss: 1.549266\n",
      "Train Epoch: 15 [42000/49000 (86%)]\tLoss: 1.571357\n",
      "Train Epoch: 15 [43000/49000 (88%)]\tLoss: 1.522048\n",
      "Train Epoch: 15 [44000/49000 (90%)]\tLoss: 1.541820\n",
      "Train Epoch: 15 [45000/49000 (92%)]\tLoss: 1.554830\n",
      "Train Epoch: 15 [46000/49000 (94%)]\tLoss: 1.553183\n",
      "Train Epoch: 15 [47000/49000 (96%)]\tLoss: 1.575121\n",
      "Train Epoch: 15 [48000/49000 (98%)]\tLoss: 1.572057\n",
      "\n",
      "Test set: Avg. loss: 1.5602, Accuracy: 18994/21000 (90.45%)\n",
      "\n",
      "Train Epoch: 16 [0/49000 (0%)]\tLoss: 1.556313\n",
      "Train Epoch: 16 [1000/49000 (2%)]\tLoss: 1.565530\n",
      "Train Epoch: 16 [2000/49000 (4%)]\tLoss: 1.563850\n",
      "Train Epoch: 16 [3000/49000 (6%)]\tLoss: 1.540980\n",
      "Train Epoch: 16 [4000/49000 (8%)]\tLoss: 1.608042\n",
      "Train Epoch: 16 [5000/49000 (10%)]\tLoss: 1.514265\n",
      "Train Epoch: 16 [6000/49000 (12%)]\tLoss: 1.572849\n",
      "Train Epoch: 16 [7000/49000 (14%)]\tLoss: 1.564075\n",
      "Train Epoch: 16 [8000/49000 (16%)]\tLoss: 1.536476\n",
      "Train Epoch: 16 [9000/49000 (18%)]\tLoss: 1.565246\n",
      "Train Epoch: 16 [10000/49000 (20%)]\tLoss: 1.531651\n",
      "Train Epoch: 16 [11000/49000 (22%)]\tLoss: 1.560843\n",
      "Train Epoch: 16 [12000/49000 (24%)]\tLoss: 1.544625\n",
      "Train Epoch: 16 [13000/49000 (27%)]\tLoss: 1.543438\n",
      "Train Epoch: 16 [14000/49000 (29%)]\tLoss: 1.544957\n",
      "Train Epoch: 16 [15000/49000 (31%)]\tLoss: 1.522930\n",
      "Train Epoch: 16 [16000/49000 (33%)]\tLoss: 1.568765\n",
      "Train Epoch: 16 [17000/49000 (35%)]\tLoss: 1.543428\n",
      "Train Epoch: 16 [18000/49000 (37%)]\tLoss: 1.574055\n",
      "Train Epoch: 16 [19000/49000 (39%)]\tLoss: 1.531264\n",
      "Train Epoch: 16 [20000/49000 (41%)]\tLoss: 1.577669\n",
      "Train Epoch: 16 [21000/49000 (43%)]\tLoss: 1.550718\n",
      "Train Epoch: 16 [22000/49000 (45%)]\tLoss: 1.567894\n",
      "Train Epoch: 16 [23000/49000 (47%)]\tLoss: 1.570590\n",
      "Train Epoch: 16 [24000/49000 (49%)]\tLoss: 1.502502\n",
      "Train Epoch: 16 [25000/49000 (51%)]\tLoss: 1.548828\n",
      "Train Epoch: 16 [26000/49000 (53%)]\tLoss: 1.534482\n",
      "Train Epoch: 16 [27000/49000 (55%)]\tLoss: 1.555231\n",
      "Train Epoch: 16 [28000/49000 (57%)]\tLoss: 1.551260\n",
      "Train Epoch: 16 [29000/49000 (59%)]\tLoss: 1.559119\n",
      "Train Epoch: 16 [30000/49000 (61%)]\tLoss: 1.571482\n",
      "Train Epoch: 16 [31000/49000 (63%)]\tLoss: 1.540717\n",
      "Train Epoch: 16 [32000/49000 (65%)]\tLoss: 1.536590\n",
      "Train Epoch: 16 [33000/49000 (67%)]\tLoss: 1.583864\n",
      "Train Epoch: 16 [34000/49000 (69%)]\tLoss: 1.533807\n",
      "Train Epoch: 16 [35000/49000 (71%)]\tLoss: 1.595410\n",
      "Train Epoch: 16 [36000/49000 (73%)]\tLoss: 1.558131\n",
      "Train Epoch: 16 [37000/49000 (76%)]\tLoss: 1.590348\n",
      "Train Epoch: 16 [38000/49000 (78%)]\tLoss: 1.547946\n",
      "Train Epoch: 16 [39000/49000 (80%)]\tLoss: 1.533219\n",
      "Train Epoch: 16 [40000/49000 (82%)]\tLoss: 1.558082\n",
      "Train Epoch: 16 [41000/49000 (84%)]\tLoss: 1.552149\n",
      "Train Epoch: 16 [42000/49000 (86%)]\tLoss: 1.545903\n",
      "Train Epoch: 16 [43000/49000 (88%)]\tLoss: 1.551103\n",
      "Train Epoch: 16 [44000/49000 (90%)]\tLoss: 1.578393\n",
      "Train Epoch: 16 [45000/49000 (92%)]\tLoss: 1.548663\n",
      "Train Epoch: 16 [46000/49000 (94%)]\tLoss: 1.550840\n",
      "Train Epoch: 16 [47000/49000 (96%)]\tLoss: 1.555006\n",
      "Train Epoch: 16 [48000/49000 (98%)]\tLoss: 1.567382\n",
      "\n",
      "Test set: Avg. loss: 1.5600, Accuracy: 18991/21000 (90.43%)\n",
      "\n",
      "Train Epoch: 17 [0/49000 (0%)]\tLoss: 1.580416\n",
      "Train Epoch: 17 [1000/49000 (2%)]\tLoss: 1.564741\n",
      "Train Epoch: 17 [2000/49000 (4%)]\tLoss: 1.532375\n",
      "Train Epoch: 17 [3000/49000 (6%)]\tLoss: 1.539040\n",
      "Train Epoch: 17 [4000/49000 (8%)]\tLoss: 1.590905\n",
      "Train Epoch: 17 [5000/49000 (10%)]\tLoss: 1.569469\n",
      "Train Epoch: 17 [6000/49000 (12%)]\tLoss: 1.535165\n",
      "Train Epoch: 17 [7000/49000 (14%)]\tLoss: 1.549299\n",
      "Train Epoch: 17 [8000/49000 (16%)]\tLoss: 1.554074\n",
      "Train Epoch: 17 [9000/49000 (18%)]\tLoss: 1.547966\n",
      "Train Epoch: 17 [10000/49000 (20%)]\tLoss: 1.534994\n",
      "Train Epoch: 17 [11000/49000 (22%)]\tLoss: 1.506219\n",
      "Train Epoch: 17 [12000/49000 (24%)]\tLoss: 1.567262\n",
      "Train Epoch: 17 [13000/49000 (27%)]\tLoss: 1.546384\n",
      "Train Epoch: 17 [14000/49000 (29%)]\tLoss: 1.499668\n",
      "Train Epoch: 17 [15000/49000 (31%)]\tLoss: 1.566221\n",
      "Train Epoch: 17 [16000/49000 (33%)]\tLoss: 1.564993\n",
      "Train Epoch: 17 [17000/49000 (35%)]\tLoss: 1.567429\n",
      "Train Epoch: 17 [18000/49000 (37%)]\tLoss: 1.529179\n",
      "Train Epoch: 17 [19000/49000 (39%)]\tLoss: 1.532804\n",
      "Train Epoch: 17 [20000/49000 (41%)]\tLoss: 1.582675\n",
      "Train Epoch: 17 [21000/49000 (43%)]\tLoss: 1.559757\n",
      "Train Epoch: 17 [22000/49000 (45%)]\tLoss: 1.523998\n",
      "Train Epoch: 17 [23000/49000 (47%)]\tLoss: 1.570591\n",
      "Train Epoch: 17 [24000/49000 (49%)]\tLoss: 1.603250\n",
      "Train Epoch: 17 [25000/49000 (51%)]\tLoss: 1.499591\n",
      "Train Epoch: 17 [26000/49000 (53%)]\tLoss: 1.556689\n",
      "Train Epoch: 17 [27000/49000 (55%)]\tLoss: 1.512700\n",
      "Train Epoch: 17 [28000/49000 (57%)]\tLoss: 1.565159\n",
      "Train Epoch: 17 [29000/49000 (59%)]\tLoss: 1.578186\n",
      "Train Epoch: 17 [30000/49000 (61%)]\tLoss: 1.596524\n",
      "Train Epoch: 17 [31000/49000 (63%)]\tLoss: 1.541606\n",
      "Train Epoch: 17 [32000/49000 (65%)]\tLoss: 1.566862\n",
      "Train Epoch: 17 [33000/49000 (67%)]\tLoss: 1.535349\n",
      "Train Epoch: 17 [34000/49000 (69%)]\tLoss: 1.532584\n",
      "Train Epoch: 17 [35000/49000 (71%)]\tLoss: 1.532488\n",
      "Train Epoch: 17 [36000/49000 (73%)]\tLoss: 1.530165\n",
      "Train Epoch: 17 [37000/49000 (76%)]\tLoss: 1.575146\n",
      "Train Epoch: 17 [38000/49000 (78%)]\tLoss: 1.538997\n",
      "Train Epoch: 17 [39000/49000 (80%)]\tLoss: 1.524882\n",
      "Train Epoch: 17 [40000/49000 (82%)]\tLoss: 1.592652\n",
      "Train Epoch: 17 [41000/49000 (84%)]\tLoss: 1.574339\n",
      "Train Epoch: 17 [42000/49000 (86%)]\tLoss: 1.525321\n",
      "Train Epoch: 17 [43000/49000 (88%)]\tLoss: 1.526368\n",
      "Train Epoch: 17 [44000/49000 (90%)]\tLoss: 1.533841\n",
      "Train Epoch: 17 [45000/49000 (92%)]\tLoss: 1.545483\n",
      "Train Epoch: 17 [46000/49000 (94%)]\tLoss: 1.565982\n",
      "Train Epoch: 17 [47000/49000 (96%)]\tLoss: 1.543872\n",
      "Train Epoch: 17 [48000/49000 (98%)]\tLoss: 1.601480\n",
      "\n",
      "Test set: Avg. loss: 1.5592, Accuracy: 19006/21000 (90.50%)\n",
      "\n",
      "Train Epoch: 18 [0/49000 (0%)]\tLoss: 1.527223\n",
      "Train Epoch: 18 [1000/49000 (2%)]\tLoss: 1.583191\n",
      "Train Epoch: 18 [2000/49000 (4%)]\tLoss: 1.540472\n",
      "Train Epoch: 18 [3000/49000 (6%)]\tLoss: 1.530489\n",
      "Train Epoch: 18 [4000/49000 (8%)]\tLoss: 1.547143\n",
      "Train Epoch: 18 [5000/49000 (10%)]\tLoss: 1.529795\n",
      "Train Epoch: 18 [6000/49000 (12%)]\tLoss: 1.579960\n",
      "Train Epoch: 18 [7000/49000 (14%)]\tLoss: 1.559061\n",
      "Train Epoch: 18 [8000/49000 (16%)]\tLoss: 1.568687\n",
      "Train Epoch: 18 [9000/49000 (18%)]\tLoss: 1.570455\n",
      "Train Epoch: 18 [10000/49000 (20%)]\tLoss: 1.545002\n",
      "Train Epoch: 18 [11000/49000 (22%)]\tLoss: 1.551042\n",
      "Train Epoch: 18 [12000/49000 (24%)]\tLoss: 1.555747\n",
      "Train Epoch: 18 [13000/49000 (27%)]\tLoss: 1.572935\n",
      "Train Epoch: 18 [14000/49000 (29%)]\tLoss: 1.565427\n",
      "Train Epoch: 18 [15000/49000 (31%)]\tLoss: 1.590064\n",
      "Train Epoch: 18 [16000/49000 (33%)]\tLoss: 1.560544\n",
      "Train Epoch: 18 [17000/49000 (35%)]\tLoss: 1.553065\n",
      "Train Epoch: 18 [18000/49000 (37%)]\tLoss: 1.571668\n",
      "Train Epoch: 18 [19000/49000 (39%)]\tLoss: 1.595213\n",
      "Train Epoch: 18 [20000/49000 (41%)]\tLoss: 1.582400\n",
      "Train Epoch: 18 [21000/49000 (43%)]\tLoss: 1.516617\n",
      "Train Epoch: 18 [22000/49000 (45%)]\tLoss: 1.559713\n",
      "Train Epoch: 18 [23000/49000 (47%)]\tLoss: 1.530288\n",
      "Train Epoch: 18 [24000/49000 (49%)]\tLoss: 1.537804\n",
      "Train Epoch: 18 [25000/49000 (51%)]\tLoss: 1.535751\n",
      "Train Epoch: 18 [26000/49000 (53%)]\tLoss: 1.560096\n",
      "Train Epoch: 18 [27000/49000 (55%)]\tLoss: 1.534771\n",
      "Train Epoch: 18 [28000/49000 (57%)]\tLoss: 1.549690\n",
      "Train Epoch: 18 [29000/49000 (59%)]\tLoss: 1.571087\n",
      "Train Epoch: 18 [30000/49000 (61%)]\tLoss: 1.535234\n",
      "Train Epoch: 18 [31000/49000 (63%)]\tLoss: 1.559627\n",
      "Train Epoch: 18 [32000/49000 (65%)]\tLoss: 1.580409\n",
      "Train Epoch: 18 [33000/49000 (67%)]\tLoss: 1.557458\n",
      "Train Epoch: 18 [34000/49000 (69%)]\tLoss: 1.547009\n",
      "Train Epoch: 18 [35000/49000 (71%)]\tLoss: 1.548417\n",
      "Train Epoch: 18 [36000/49000 (73%)]\tLoss: 1.557128\n",
      "Train Epoch: 18 [37000/49000 (76%)]\tLoss: 1.568848\n",
      "Train Epoch: 18 [38000/49000 (78%)]\tLoss: 1.537879\n",
      "Train Epoch: 18 [39000/49000 (80%)]\tLoss: 1.516695\n",
      "Train Epoch: 18 [40000/49000 (82%)]\tLoss: 1.522238\n",
      "Train Epoch: 18 [41000/49000 (84%)]\tLoss: 1.576892\n",
      "Train Epoch: 18 [42000/49000 (86%)]\tLoss: 1.532286\n",
      "Train Epoch: 18 [43000/49000 (88%)]\tLoss: 1.561158\n",
      "Train Epoch: 18 [44000/49000 (90%)]\tLoss: 1.561579\n",
      "Train Epoch: 18 [45000/49000 (92%)]\tLoss: 1.553615\n",
      "Train Epoch: 18 [46000/49000 (94%)]\tLoss: 1.557747\n",
      "Train Epoch: 18 [47000/49000 (96%)]\tLoss: 1.610613\n",
      "Train Epoch: 18 [48000/49000 (98%)]\tLoss: 1.561239\n",
      "\n",
      "Test set: Avg. loss: 1.5630, Accuracy: 18878/21000 (89.90%)\n",
      "\n",
      "Train Epoch: 19 [0/49000 (0%)]\tLoss: 1.567069\n",
      "Train Epoch: 19 [1000/49000 (2%)]\tLoss: 1.538986\n",
      "Train Epoch: 19 [2000/49000 (4%)]\tLoss: 1.575726\n",
      "Train Epoch: 19 [3000/49000 (6%)]\tLoss: 1.538623\n",
      "Train Epoch: 19 [4000/49000 (8%)]\tLoss: 1.533700\n",
      "Train Epoch: 19 [5000/49000 (10%)]\tLoss: 1.569562\n",
      "Train Epoch: 19 [6000/49000 (12%)]\tLoss: 1.582731\n",
      "Train Epoch: 19 [7000/49000 (14%)]\tLoss: 1.535178\n",
      "Train Epoch: 19 [8000/49000 (16%)]\tLoss: 1.540030\n",
      "Train Epoch: 19 [9000/49000 (18%)]\tLoss: 1.579314\n",
      "Train Epoch: 19 [10000/49000 (20%)]\tLoss: 1.548632\n",
      "Train Epoch: 19 [11000/49000 (22%)]\tLoss: 1.564773\n",
      "Train Epoch: 19 [12000/49000 (24%)]\tLoss: 1.541579\n",
      "Train Epoch: 19 [13000/49000 (27%)]\tLoss: 1.552734\n",
      "Train Epoch: 19 [14000/49000 (29%)]\tLoss: 1.557399\n",
      "Train Epoch: 19 [15000/49000 (31%)]\tLoss: 1.548275\n",
      "Train Epoch: 19 [16000/49000 (33%)]\tLoss: 1.531242\n",
      "Train Epoch: 19 [17000/49000 (35%)]\tLoss: 1.548327\n",
      "Train Epoch: 19 [18000/49000 (37%)]\tLoss: 1.547749\n",
      "Train Epoch: 19 [19000/49000 (39%)]\tLoss: 1.520617\n",
      "Train Epoch: 19 [20000/49000 (41%)]\tLoss: 1.515888\n",
      "Train Epoch: 19 [21000/49000 (43%)]\tLoss: 1.537987\n",
      "Train Epoch: 19 [22000/49000 (45%)]\tLoss: 1.557770\n",
      "Train Epoch: 19 [23000/49000 (47%)]\tLoss: 1.569968\n",
      "Train Epoch: 19 [24000/49000 (49%)]\tLoss: 1.548411\n",
      "Train Epoch: 19 [25000/49000 (51%)]\tLoss: 1.615239\n",
      "Train Epoch: 19 [26000/49000 (53%)]\tLoss: 1.522028\n",
      "Train Epoch: 19 [27000/49000 (55%)]\tLoss: 1.537465\n",
      "Train Epoch: 19 [28000/49000 (57%)]\tLoss: 1.534093\n",
      "Train Epoch: 19 [29000/49000 (59%)]\tLoss: 1.580201\n",
      "Train Epoch: 19 [30000/49000 (61%)]\tLoss: 1.555527\n",
      "Train Epoch: 19 [31000/49000 (63%)]\tLoss: 1.634826\n",
      "Train Epoch: 19 [32000/49000 (65%)]\tLoss: 1.527324\n",
      "Train Epoch: 19 [33000/49000 (67%)]\tLoss: 1.564674\n",
      "Train Epoch: 19 [34000/49000 (69%)]\tLoss: 1.534569\n",
      "Train Epoch: 19 [35000/49000 (71%)]\tLoss: 1.557543\n",
      "Train Epoch: 19 [36000/49000 (73%)]\tLoss: 1.583198\n",
      "Train Epoch: 19 [37000/49000 (76%)]\tLoss: 1.561868\n",
      "Train Epoch: 19 [38000/49000 (78%)]\tLoss: 1.518349\n",
      "Train Epoch: 19 [39000/49000 (80%)]\tLoss: 1.570827\n",
      "Train Epoch: 19 [40000/49000 (82%)]\tLoss: 1.541158\n",
      "Train Epoch: 19 [41000/49000 (84%)]\tLoss: 1.545998\n",
      "Train Epoch: 19 [42000/49000 (86%)]\tLoss: 1.531423\n",
      "Train Epoch: 19 [43000/49000 (88%)]\tLoss: 1.566819\n",
      "Train Epoch: 19 [44000/49000 (90%)]\tLoss: 1.579909\n",
      "Train Epoch: 19 [45000/49000 (92%)]\tLoss: 1.587810\n",
      "Train Epoch: 19 [46000/49000 (94%)]\tLoss: 1.548016\n",
      "Train Epoch: 19 [47000/49000 (96%)]\tLoss: 1.552398\n",
      "Train Epoch: 19 [48000/49000 (98%)]\tLoss: 1.538419\n",
      "\n",
      "Test set: Avg. loss: 1.5585, Accuracy: 19045/21000 (90.69%)\n",
      "\n",
      "Train Epoch: 20 [0/49000 (0%)]\tLoss: 1.525486\n",
      "Train Epoch: 20 [1000/49000 (2%)]\tLoss: 1.542785\n",
      "Train Epoch: 20 [2000/49000 (4%)]\tLoss: 1.573421\n",
      "Train Epoch: 20 [3000/49000 (6%)]\tLoss: 1.524894\n",
      "Train Epoch: 20 [4000/49000 (8%)]\tLoss: 1.559455\n",
      "Train Epoch: 20 [5000/49000 (10%)]\tLoss: 1.593179\n",
      "Train Epoch: 20 [6000/49000 (12%)]\tLoss: 1.521246\n",
      "Train Epoch: 20 [7000/49000 (14%)]\tLoss: 1.545727\n",
      "Train Epoch: 20 [8000/49000 (16%)]\tLoss: 1.524077\n",
      "Train Epoch: 20 [9000/49000 (18%)]\tLoss: 1.538350\n",
      "Train Epoch: 20 [10000/49000 (20%)]\tLoss: 1.593358\n",
      "Train Epoch: 20 [11000/49000 (22%)]\tLoss: 1.544478\n",
      "Train Epoch: 20 [12000/49000 (24%)]\tLoss: 1.549000\n",
      "Train Epoch: 20 [13000/49000 (27%)]\tLoss: 1.530351\n",
      "Train Epoch: 20 [14000/49000 (29%)]\tLoss: 1.578539\n",
      "Train Epoch: 20 [15000/49000 (31%)]\tLoss: 1.551473\n",
      "Train Epoch: 20 [16000/49000 (33%)]\tLoss: 1.589678\n",
      "Train Epoch: 20 [17000/49000 (35%)]\tLoss: 1.521139\n",
      "Train Epoch: 20 [18000/49000 (37%)]\tLoss: 1.532610\n",
      "Train Epoch: 20 [19000/49000 (39%)]\tLoss: 1.573786\n",
      "Train Epoch: 20 [20000/49000 (41%)]\tLoss: 1.581112\n",
      "Train Epoch: 20 [21000/49000 (43%)]\tLoss: 1.558834\n",
      "Train Epoch: 20 [22000/49000 (45%)]\tLoss: 1.575087\n",
      "Train Epoch: 20 [23000/49000 (47%)]\tLoss: 1.521667\n",
      "Train Epoch: 20 [24000/49000 (49%)]\tLoss: 1.568683\n",
      "Train Epoch: 20 [25000/49000 (51%)]\tLoss: 1.540158\n",
      "Train Epoch: 20 [26000/49000 (53%)]\tLoss: 1.542691\n",
      "Train Epoch: 20 [27000/49000 (55%)]\tLoss: 1.568567\n",
      "Train Epoch: 20 [28000/49000 (57%)]\tLoss: 1.544502\n",
      "Train Epoch: 20 [29000/49000 (59%)]\tLoss: 1.551589\n",
      "Train Epoch: 20 [30000/49000 (61%)]\tLoss: 1.542544\n",
      "Train Epoch: 20 [31000/49000 (63%)]\tLoss: 1.544637\n",
      "Train Epoch: 20 [32000/49000 (65%)]\tLoss: 1.547846\n",
      "Train Epoch: 20 [33000/49000 (67%)]\tLoss: 1.528102\n",
      "Train Epoch: 20 [34000/49000 (69%)]\tLoss: 1.544763\n",
      "Train Epoch: 20 [35000/49000 (71%)]\tLoss: 1.549271\n",
      "Train Epoch: 20 [36000/49000 (73%)]\tLoss: 1.564900\n",
      "Train Epoch: 20 [37000/49000 (76%)]\tLoss: 1.559772\n",
      "Train Epoch: 20 [38000/49000 (78%)]\tLoss: 1.522370\n",
      "Train Epoch: 20 [39000/49000 (80%)]\tLoss: 1.574988\n",
      "Train Epoch: 20 [40000/49000 (82%)]\tLoss: 1.556047\n",
      "Train Epoch: 20 [41000/49000 (84%)]\tLoss: 1.590342\n",
      "Train Epoch: 20 [42000/49000 (86%)]\tLoss: 1.552503\n",
      "Train Epoch: 20 [43000/49000 (88%)]\tLoss: 1.568612\n",
      "Train Epoch: 20 [44000/49000 (90%)]\tLoss: 1.581798\n",
      "Train Epoch: 20 [45000/49000 (92%)]\tLoss: 1.521479\n",
      "Train Epoch: 20 [46000/49000 (94%)]\tLoss: 1.532485\n",
      "Train Epoch: 20 [47000/49000 (96%)]\tLoss: 1.588925\n",
      "Train Epoch: 20 [48000/49000 (98%)]\tLoss: 1.519180\n",
      "\n",
      "Test set: Avg. loss: 1.5587, Accuracy: 19016/21000 (90.55%)\n",
      "\n",
      "Train Epoch: 21 [0/49000 (0%)]\tLoss: 1.585720\n",
      "Train Epoch: 21 [1000/49000 (2%)]\tLoss: 1.511596\n",
      "Train Epoch: 21 [2000/49000 (4%)]\tLoss: 1.531234\n",
      "Train Epoch: 21 [3000/49000 (6%)]\tLoss: 1.526196\n",
      "Train Epoch: 21 [4000/49000 (8%)]\tLoss: 1.581002\n",
      "Train Epoch: 21 [5000/49000 (10%)]\tLoss: 1.552088\n",
      "Train Epoch: 21 [6000/49000 (12%)]\tLoss: 1.546830\n",
      "Train Epoch: 21 [7000/49000 (14%)]\tLoss: 1.561276\n",
      "Train Epoch: 21 [8000/49000 (16%)]\tLoss: 1.558254\n",
      "Train Epoch: 21 [9000/49000 (18%)]\tLoss: 1.556152\n",
      "Train Epoch: 21 [10000/49000 (20%)]\tLoss: 1.528316\n",
      "Train Epoch: 21 [11000/49000 (22%)]\tLoss: 1.523535\n",
      "Train Epoch: 21 [12000/49000 (24%)]\tLoss: 1.516238\n",
      "Train Epoch: 21 [13000/49000 (27%)]\tLoss: 1.560761\n",
      "Train Epoch: 21 [14000/49000 (29%)]\tLoss: 1.522609\n",
      "Train Epoch: 21 [15000/49000 (31%)]\tLoss: 1.541685\n",
      "Train Epoch: 21 [16000/49000 (33%)]\tLoss: 1.570482\n",
      "Train Epoch: 21 [17000/49000 (35%)]\tLoss: 1.550670\n",
      "Train Epoch: 21 [18000/49000 (37%)]\tLoss: 1.519002\n",
      "Train Epoch: 21 [19000/49000 (39%)]\tLoss: 1.587135\n",
      "Train Epoch: 21 [20000/49000 (41%)]\tLoss: 1.555479\n",
      "Train Epoch: 21 [21000/49000 (43%)]\tLoss: 1.543852\n",
      "Train Epoch: 21 [22000/49000 (45%)]\tLoss: 1.568263\n",
      "Train Epoch: 21 [23000/49000 (47%)]\tLoss: 1.527101\n",
      "Train Epoch: 21 [24000/49000 (49%)]\tLoss: 1.535426\n",
      "Train Epoch: 21 [25000/49000 (51%)]\tLoss: 1.560174\n",
      "Train Epoch: 21 [26000/49000 (53%)]\tLoss: 1.580870\n",
      "Train Epoch: 21 [27000/49000 (55%)]\tLoss: 1.578894\n",
      "Train Epoch: 21 [28000/49000 (57%)]\tLoss: 1.563623\n",
      "Train Epoch: 21 [29000/49000 (59%)]\tLoss: 1.534069\n",
      "Train Epoch: 21 [30000/49000 (61%)]\tLoss: 1.550639\n",
      "Train Epoch: 21 [31000/49000 (63%)]\tLoss: 1.550131\n",
      "Train Epoch: 21 [32000/49000 (65%)]\tLoss: 1.547715\n",
      "Train Epoch: 21 [33000/49000 (67%)]\tLoss: 1.580078\n",
      "Train Epoch: 21 [34000/49000 (69%)]\tLoss: 1.530271\n",
      "Train Epoch: 21 [35000/49000 (71%)]\tLoss: 1.546825\n",
      "Train Epoch: 21 [36000/49000 (73%)]\tLoss: 1.568604\n",
      "Train Epoch: 21 [37000/49000 (76%)]\tLoss: 1.544335\n",
      "Train Epoch: 21 [38000/49000 (78%)]\tLoss: 1.569758\n",
      "Train Epoch: 21 [39000/49000 (80%)]\tLoss: 1.526085\n",
      "Train Epoch: 21 [40000/49000 (82%)]\tLoss: 1.536690\n",
      "Train Epoch: 21 [41000/49000 (84%)]\tLoss: 1.563801\n",
      "Train Epoch: 21 [42000/49000 (86%)]\tLoss: 1.580231\n",
      "Train Epoch: 21 [43000/49000 (88%)]\tLoss: 1.548767\n",
      "Train Epoch: 21 [44000/49000 (90%)]\tLoss: 1.532317\n",
      "Train Epoch: 21 [45000/49000 (92%)]\tLoss: 1.528658\n",
      "Train Epoch: 21 [46000/49000 (94%)]\tLoss: 1.539120\n",
      "Train Epoch: 21 [47000/49000 (96%)]\tLoss: 1.531564\n",
      "Train Epoch: 21 [48000/49000 (98%)]\tLoss: 1.547737\n",
      "\n",
      "Test set: Avg. loss: 1.5571, Accuracy: 19014/21000 (90.54%)\n",
      "\n",
      "Train Epoch: 22 [0/49000 (0%)]\tLoss: 1.533869\n",
      "Train Epoch: 22 [1000/49000 (2%)]\tLoss: 1.544296\n",
      "Train Epoch: 22 [2000/49000 (4%)]\tLoss: 1.546661\n",
      "Train Epoch: 22 [3000/49000 (6%)]\tLoss: 1.552808\n",
      "Train Epoch: 22 [4000/49000 (8%)]\tLoss: 1.593742\n",
      "Train Epoch: 22 [5000/49000 (10%)]\tLoss: 1.550837\n",
      "Train Epoch: 22 [6000/49000 (12%)]\tLoss: 1.568420\n",
      "Train Epoch: 22 [7000/49000 (14%)]\tLoss: 1.543743\n",
      "Train Epoch: 22 [8000/49000 (16%)]\tLoss: 1.542382\n",
      "Train Epoch: 22 [9000/49000 (18%)]\tLoss: 1.529935\n",
      "Train Epoch: 22 [10000/49000 (20%)]\tLoss: 1.539694\n",
      "Train Epoch: 22 [11000/49000 (22%)]\tLoss: 1.545569\n",
      "Train Epoch: 22 [12000/49000 (24%)]\tLoss: 1.589559\n",
      "Train Epoch: 22 [13000/49000 (27%)]\tLoss: 1.524270\n",
      "Train Epoch: 22 [14000/49000 (29%)]\tLoss: 1.552129\n",
      "Train Epoch: 22 [15000/49000 (31%)]\tLoss: 1.590551\n",
      "Train Epoch: 22 [16000/49000 (33%)]\tLoss: 1.517519\n",
      "Train Epoch: 22 [17000/49000 (35%)]\tLoss: 1.524969\n",
      "Train Epoch: 22 [18000/49000 (37%)]\tLoss: 1.586388\n",
      "Train Epoch: 22 [19000/49000 (39%)]\tLoss: 1.527458\n",
      "Train Epoch: 22 [20000/49000 (41%)]\tLoss: 1.539122\n",
      "Train Epoch: 22 [21000/49000 (43%)]\tLoss: 1.559148\n",
      "Train Epoch: 22 [22000/49000 (45%)]\tLoss: 1.550395\n",
      "Train Epoch: 22 [23000/49000 (47%)]\tLoss: 1.576191\n",
      "Train Epoch: 22 [24000/49000 (49%)]\tLoss: 1.536130\n",
      "Train Epoch: 22 [25000/49000 (51%)]\tLoss: 1.595469\n",
      "Train Epoch: 22 [26000/49000 (53%)]\tLoss: 1.530235\n",
      "Train Epoch: 22 [27000/49000 (55%)]\tLoss: 1.514203\n",
      "Train Epoch: 22 [28000/49000 (57%)]\tLoss: 1.579935\n",
      "Train Epoch: 22 [29000/49000 (59%)]\tLoss: 1.539227\n",
      "Train Epoch: 22 [30000/49000 (61%)]\tLoss: 1.533255\n",
      "Train Epoch: 22 [31000/49000 (63%)]\tLoss: 1.568279\n",
      "Train Epoch: 22 [32000/49000 (65%)]\tLoss: 1.562697\n",
      "Train Epoch: 22 [33000/49000 (67%)]\tLoss: 1.559902\n",
      "Train Epoch: 22 [34000/49000 (69%)]\tLoss: 1.577825\n",
      "Train Epoch: 22 [35000/49000 (71%)]\tLoss: 1.532128\n",
      "Train Epoch: 22 [36000/49000 (73%)]\tLoss: 1.583496\n",
      "Train Epoch: 22 [37000/49000 (76%)]\tLoss: 1.558137\n",
      "Train Epoch: 22 [38000/49000 (78%)]\tLoss: 1.542091\n",
      "Train Epoch: 22 [39000/49000 (80%)]\tLoss: 1.555006\n",
      "Train Epoch: 22 [40000/49000 (82%)]\tLoss: 1.556906\n",
      "Train Epoch: 22 [41000/49000 (84%)]\tLoss: 1.554264\n",
      "Train Epoch: 22 [42000/49000 (86%)]\tLoss: 1.530925\n",
      "Train Epoch: 22 [43000/49000 (88%)]\tLoss: 1.547191\n",
      "Train Epoch: 22 [44000/49000 (90%)]\tLoss: 1.599817\n",
      "Train Epoch: 22 [45000/49000 (92%)]\tLoss: 1.534690\n",
      "Train Epoch: 22 [46000/49000 (94%)]\tLoss: 1.597582\n",
      "Train Epoch: 22 [47000/49000 (96%)]\tLoss: 1.518743\n",
      "Train Epoch: 22 [48000/49000 (98%)]\tLoss: 1.559210\n",
      "\n",
      "Test set: Avg. loss: 1.5591, Accuracy: 19016/21000 (90.55%)\n",
      "\n",
      "Train Epoch: 23 [0/49000 (0%)]\tLoss: 1.556482\n",
      "Train Epoch: 23 [1000/49000 (2%)]\tLoss: 1.571105\n",
      "Train Epoch: 23 [2000/49000 (4%)]\tLoss: 1.532227\n",
      "Train Epoch: 23 [3000/49000 (6%)]\tLoss: 1.538926\n",
      "Train Epoch: 23 [4000/49000 (8%)]\tLoss: 1.531178\n",
      "Train Epoch: 23 [5000/49000 (10%)]\tLoss: 1.534488\n",
      "Train Epoch: 23 [6000/49000 (12%)]\tLoss: 1.576409\n",
      "Train Epoch: 23 [7000/49000 (14%)]\tLoss: 1.565560\n",
      "Train Epoch: 23 [8000/49000 (16%)]\tLoss: 1.526822\n",
      "Train Epoch: 23 [9000/49000 (18%)]\tLoss: 1.576537\n",
      "Train Epoch: 23 [10000/49000 (20%)]\tLoss: 1.569717\n",
      "Train Epoch: 23 [11000/49000 (22%)]\tLoss: 1.595936\n",
      "Train Epoch: 23 [12000/49000 (24%)]\tLoss: 1.586309\n",
      "Train Epoch: 23 [13000/49000 (27%)]\tLoss: 1.547411\n",
      "Train Epoch: 23 [14000/49000 (29%)]\tLoss: 1.549565\n",
      "Train Epoch: 23 [15000/49000 (31%)]\tLoss: 1.533025\n",
      "Train Epoch: 23 [16000/49000 (33%)]\tLoss: 1.539991\n",
      "Train Epoch: 23 [17000/49000 (35%)]\tLoss: 1.558484\n",
      "Train Epoch: 23 [18000/49000 (37%)]\tLoss: 1.535045\n",
      "Train Epoch: 23 [19000/49000 (39%)]\tLoss: 1.556314\n",
      "Train Epoch: 23 [20000/49000 (41%)]\tLoss: 1.529838\n",
      "Train Epoch: 23 [21000/49000 (43%)]\tLoss: 1.552506\n",
      "Train Epoch: 23 [22000/49000 (45%)]\tLoss: 1.569109\n",
      "Train Epoch: 23 [23000/49000 (47%)]\tLoss: 1.546214\n",
      "Train Epoch: 23 [24000/49000 (49%)]\tLoss: 1.564155\n",
      "Train Epoch: 23 [25000/49000 (51%)]\tLoss: 1.531538\n",
      "Train Epoch: 23 [26000/49000 (53%)]\tLoss: 1.520323\n",
      "Train Epoch: 23 [27000/49000 (55%)]\tLoss: 1.580085\n",
      "Train Epoch: 23 [28000/49000 (57%)]\tLoss: 1.515283\n",
      "Train Epoch: 23 [29000/49000 (59%)]\tLoss: 1.548398\n",
      "Train Epoch: 23 [30000/49000 (61%)]\tLoss: 1.563447\n",
      "Train Epoch: 23 [31000/49000 (63%)]\tLoss: 1.555551\n",
      "Train Epoch: 23 [32000/49000 (65%)]\tLoss: 1.544917\n",
      "Train Epoch: 23 [33000/49000 (67%)]\tLoss: 1.555157\n",
      "Train Epoch: 23 [34000/49000 (69%)]\tLoss: 1.552026\n",
      "Train Epoch: 23 [35000/49000 (71%)]\tLoss: 1.576613\n",
      "Train Epoch: 23 [36000/49000 (73%)]\tLoss: 1.584990\n",
      "Train Epoch: 23 [37000/49000 (76%)]\tLoss: 1.547740\n",
      "Train Epoch: 23 [38000/49000 (78%)]\tLoss: 1.572279\n",
      "Train Epoch: 23 [39000/49000 (80%)]\tLoss: 1.587225\n",
      "Train Epoch: 23 [40000/49000 (82%)]\tLoss: 1.565545\n",
      "Train Epoch: 23 [41000/49000 (84%)]\tLoss: 1.608617\n",
      "Train Epoch: 23 [42000/49000 (86%)]\tLoss: 1.584952\n",
      "Train Epoch: 23 [43000/49000 (88%)]\tLoss: 1.581258\n",
      "Train Epoch: 23 [44000/49000 (90%)]\tLoss: 1.538848\n",
      "Train Epoch: 23 [45000/49000 (92%)]\tLoss: 1.539435\n",
      "Train Epoch: 23 [46000/49000 (94%)]\tLoss: 1.559169\n",
      "Train Epoch: 23 [47000/49000 (96%)]\tLoss: 1.534435\n",
      "Train Epoch: 23 [48000/49000 (98%)]\tLoss: 1.565106\n",
      "\n",
      "Test set: Avg. loss: 1.5564, Accuracy: 19032/21000 (90.63%)\n",
      "\n",
      "Train Epoch: 24 [0/49000 (0%)]\tLoss: 1.573702\n",
      "Train Epoch: 24 [1000/49000 (2%)]\tLoss: 1.567406\n",
      "Train Epoch: 24 [2000/49000 (4%)]\tLoss: 1.566455\n",
      "Train Epoch: 24 [3000/49000 (6%)]\tLoss: 1.556300\n",
      "Train Epoch: 24 [4000/49000 (8%)]\tLoss: 1.520351\n",
      "Train Epoch: 24 [5000/49000 (10%)]\tLoss: 1.526846\n",
      "Train Epoch: 24 [6000/49000 (12%)]\tLoss: 1.553450\n",
      "Train Epoch: 24 [7000/49000 (14%)]\tLoss: 1.524246\n",
      "Train Epoch: 24 [8000/49000 (16%)]\tLoss: 1.535685\n",
      "Train Epoch: 24 [9000/49000 (18%)]\tLoss: 1.546141\n",
      "Train Epoch: 24 [10000/49000 (20%)]\tLoss: 1.548172\n",
      "Train Epoch: 24 [11000/49000 (22%)]\tLoss: 1.562117\n",
      "Train Epoch: 24 [12000/49000 (24%)]\tLoss: 1.564751\n",
      "Train Epoch: 24 [13000/49000 (27%)]\tLoss: 1.559150\n",
      "Train Epoch: 24 [14000/49000 (29%)]\tLoss: 1.567468\n",
      "Train Epoch: 24 [15000/49000 (31%)]\tLoss: 1.556800\n",
      "Train Epoch: 24 [16000/49000 (33%)]\tLoss: 1.562968\n",
      "Train Epoch: 24 [17000/49000 (35%)]\tLoss: 1.609203\n",
      "Train Epoch: 24 [18000/49000 (37%)]\tLoss: 1.521527\n",
      "Train Epoch: 24 [19000/49000 (39%)]\tLoss: 1.531874\n",
      "Train Epoch: 24 [20000/49000 (41%)]\tLoss: 1.530760\n",
      "Train Epoch: 24 [21000/49000 (43%)]\tLoss: 1.577809\n",
      "Train Epoch: 24 [22000/49000 (45%)]\tLoss: 1.581513\n",
      "Train Epoch: 24 [23000/49000 (47%)]\tLoss: 1.533784\n",
      "Train Epoch: 24 [24000/49000 (49%)]\tLoss: 1.553560\n",
      "Train Epoch: 24 [25000/49000 (51%)]\tLoss: 1.546365\n",
      "Train Epoch: 24 [26000/49000 (53%)]\tLoss: 1.533258\n",
      "Train Epoch: 24 [27000/49000 (55%)]\tLoss: 1.574196\n",
      "Train Epoch: 24 [28000/49000 (57%)]\tLoss: 1.555612\n",
      "Train Epoch: 24 [29000/49000 (59%)]\tLoss: 1.553637\n",
      "Train Epoch: 24 [30000/49000 (61%)]\tLoss: 1.576908\n",
      "Train Epoch: 24 [31000/49000 (63%)]\tLoss: 1.533247\n",
      "Train Epoch: 24 [32000/49000 (65%)]\tLoss: 1.594802\n",
      "Train Epoch: 24 [33000/49000 (67%)]\tLoss: 1.569534\n",
      "Train Epoch: 24 [34000/49000 (69%)]\tLoss: 1.556470\n",
      "Train Epoch: 24 [35000/49000 (71%)]\tLoss: 1.502996\n",
      "Train Epoch: 24 [36000/49000 (73%)]\tLoss: 1.545947\n",
      "Train Epoch: 24 [37000/49000 (76%)]\tLoss: 1.553090\n",
      "Train Epoch: 24 [38000/49000 (78%)]\tLoss: 1.544145\n",
      "Train Epoch: 24 [39000/49000 (80%)]\tLoss: 1.519164\n",
      "Train Epoch: 24 [40000/49000 (82%)]\tLoss: 1.576155\n",
      "Train Epoch: 24 [41000/49000 (84%)]\tLoss: 1.561973\n",
      "Train Epoch: 24 [42000/49000 (86%)]\tLoss: 1.544195\n",
      "Train Epoch: 24 [43000/49000 (88%)]\tLoss: 1.566197\n",
      "Train Epoch: 24 [44000/49000 (90%)]\tLoss: 1.536646\n",
      "Train Epoch: 24 [45000/49000 (92%)]\tLoss: 1.547476\n",
      "Train Epoch: 24 [46000/49000 (94%)]\tLoss: 1.514096\n",
      "Train Epoch: 24 [47000/49000 (96%)]\tLoss: 1.544510\n",
      "Train Epoch: 24 [48000/49000 (98%)]\tLoss: 1.573098\n",
      "\n",
      "Test set: Avg. loss: 1.5564, Accuracy: 19025/21000 (90.60%)\n",
      "\n",
      "Train Epoch: 25 [0/49000 (0%)]\tLoss: 1.546400\n",
      "Train Epoch: 25 [1000/49000 (2%)]\tLoss: 1.542939\n",
      "Train Epoch: 25 [2000/49000 (4%)]\tLoss: 1.544544\n",
      "Train Epoch: 25 [3000/49000 (6%)]\tLoss: 1.563762\n",
      "Train Epoch: 25 [4000/49000 (8%)]\tLoss: 1.556691\n",
      "Train Epoch: 25 [5000/49000 (10%)]\tLoss: 1.557918\n",
      "Train Epoch: 25 [6000/49000 (12%)]\tLoss: 1.507157\n",
      "Train Epoch: 25 [7000/49000 (14%)]\tLoss: 1.550411\n",
      "Train Epoch: 25 [8000/49000 (16%)]\tLoss: 1.568347\n",
      "Train Epoch: 25 [9000/49000 (18%)]\tLoss: 1.538484\n",
      "Train Epoch: 25 [10000/49000 (20%)]\tLoss: 1.620124\n",
      "Train Epoch: 25 [11000/49000 (22%)]\tLoss: 1.543471\n",
      "Train Epoch: 25 [12000/49000 (24%)]\tLoss: 1.546676\n",
      "Train Epoch: 25 [13000/49000 (27%)]\tLoss: 1.527895\n",
      "Train Epoch: 25 [14000/49000 (29%)]\tLoss: 1.554402\n",
      "Train Epoch: 25 [15000/49000 (31%)]\tLoss: 1.558185\n",
      "Train Epoch: 25 [16000/49000 (33%)]\tLoss: 1.547893\n",
      "Train Epoch: 25 [17000/49000 (35%)]\tLoss: 1.556241\n",
      "Train Epoch: 25 [18000/49000 (37%)]\tLoss: 1.526628\n",
      "Train Epoch: 25 [19000/49000 (39%)]\tLoss: 1.556569\n",
      "Train Epoch: 25 [20000/49000 (41%)]\tLoss: 1.583558\n",
      "Train Epoch: 25 [21000/49000 (43%)]\tLoss: 1.547614\n",
      "Train Epoch: 25 [22000/49000 (45%)]\tLoss: 1.543939\n",
      "Train Epoch: 25 [23000/49000 (47%)]\tLoss: 1.567931\n",
      "Train Epoch: 25 [24000/49000 (49%)]\tLoss: 1.538593\n",
      "Train Epoch: 25 [25000/49000 (51%)]\tLoss: 1.565735\n",
      "Train Epoch: 25 [26000/49000 (53%)]\tLoss: 1.586914\n",
      "Train Epoch: 25 [27000/49000 (55%)]\tLoss: 1.542877\n",
      "Train Epoch: 25 [28000/49000 (57%)]\tLoss: 1.571264\n",
      "Train Epoch: 25 [29000/49000 (59%)]\tLoss: 1.588521\n",
      "Train Epoch: 25 [30000/49000 (61%)]\tLoss: 1.544324\n",
      "Train Epoch: 25 [31000/49000 (63%)]\tLoss: 1.593357\n",
      "Train Epoch: 25 [32000/49000 (65%)]\tLoss: 1.544805\n",
      "Train Epoch: 25 [33000/49000 (67%)]\tLoss: 1.531553\n",
      "Train Epoch: 25 [34000/49000 (69%)]\tLoss: 1.521555\n",
      "Train Epoch: 25 [35000/49000 (71%)]\tLoss: 1.555614\n",
      "Train Epoch: 25 [36000/49000 (73%)]\tLoss: 1.560146\n",
      "Train Epoch: 25 [37000/49000 (76%)]\tLoss: 1.552220\n",
      "Train Epoch: 25 [38000/49000 (78%)]\tLoss: 1.575244\n",
      "Train Epoch: 25 [39000/49000 (80%)]\tLoss: 1.558115\n",
      "Train Epoch: 25 [40000/49000 (82%)]\tLoss: 1.559207\n",
      "Train Epoch: 25 [41000/49000 (84%)]\tLoss: 1.532304\n",
      "Train Epoch: 25 [42000/49000 (86%)]\tLoss: 1.547270\n",
      "Train Epoch: 25 [43000/49000 (88%)]\tLoss: 1.564645\n",
      "Train Epoch: 25 [44000/49000 (90%)]\tLoss: 1.549935\n",
      "Train Epoch: 25 [45000/49000 (92%)]\tLoss: 1.519797\n",
      "Train Epoch: 25 [46000/49000 (94%)]\tLoss: 1.543370\n",
      "Train Epoch: 25 [47000/49000 (96%)]\tLoss: 1.547402\n",
      "Train Epoch: 25 [48000/49000 (98%)]\tLoss: 1.532536\n",
      "\n",
      "Test set: Avg. loss: 1.5585, Accuracy: 19052/21000 (90.72%)\n",
      "\n",
      "Train Epoch: 26 [0/49000 (0%)]\tLoss: 1.566367\n",
      "Train Epoch: 26 [1000/49000 (2%)]\tLoss: 1.519921\n",
      "Train Epoch: 26 [2000/49000 (4%)]\tLoss: 1.523572\n",
      "Train Epoch: 26 [3000/49000 (6%)]\tLoss: 1.571763\n",
      "Train Epoch: 26 [4000/49000 (8%)]\tLoss: 1.583818\n",
      "Train Epoch: 26 [5000/49000 (10%)]\tLoss: 1.557512\n",
      "Train Epoch: 26 [6000/49000 (12%)]\tLoss: 1.555267\n",
      "Train Epoch: 26 [7000/49000 (14%)]\tLoss: 1.545487\n",
      "Train Epoch: 26 [8000/49000 (16%)]\tLoss: 1.509771\n",
      "Train Epoch: 26 [9000/49000 (18%)]\tLoss: 1.504842\n",
      "Train Epoch: 26 [10000/49000 (20%)]\tLoss: 1.530909\n",
      "Train Epoch: 26 [11000/49000 (22%)]\tLoss: 1.539729\n",
      "Train Epoch: 26 [12000/49000 (24%)]\tLoss: 1.593020\n",
      "Train Epoch: 26 [13000/49000 (27%)]\tLoss: 1.599478\n",
      "Train Epoch: 26 [14000/49000 (29%)]\tLoss: 1.579315\n",
      "Train Epoch: 26 [15000/49000 (31%)]\tLoss: 1.534323\n",
      "Train Epoch: 26 [16000/49000 (33%)]\tLoss: 1.535378\n",
      "Train Epoch: 26 [17000/49000 (35%)]\tLoss: 1.533560\n",
      "Train Epoch: 26 [18000/49000 (37%)]\tLoss: 1.521876\n",
      "Train Epoch: 26 [19000/49000 (39%)]\tLoss: 1.592488\n",
      "Train Epoch: 26 [20000/49000 (41%)]\tLoss: 1.568711\n",
      "Train Epoch: 26 [21000/49000 (43%)]\tLoss: 1.569654\n",
      "Train Epoch: 26 [22000/49000 (45%)]\tLoss: 1.541449\n",
      "Train Epoch: 26 [23000/49000 (47%)]\tLoss: 1.530124\n",
      "Train Epoch: 26 [24000/49000 (49%)]\tLoss: 1.512975\n",
      "Train Epoch: 26 [25000/49000 (51%)]\tLoss: 1.540794\n",
      "Train Epoch: 26 [26000/49000 (53%)]\tLoss: 1.543588\n",
      "Train Epoch: 26 [27000/49000 (55%)]\tLoss: 1.519982\n",
      "Train Epoch: 26 [28000/49000 (57%)]\tLoss: 1.539143\n",
      "Train Epoch: 26 [29000/49000 (59%)]\tLoss: 1.528571\n",
      "Train Epoch: 26 [30000/49000 (61%)]\tLoss: 1.530735\n",
      "Train Epoch: 26 [31000/49000 (63%)]\tLoss: 1.533706\n",
      "Train Epoch: 26 [32000/49000 (65%)]\tLoss: 1.557794\n",
      "Train Epoch: 26 [33000/49000 (67%)]\tLoss: 1.593289\n",
      "Train Epoch: 26 [34000/49000 (69%)]\tLoss: 1.576206\n",
      "Train Epoch: 26 [35000/49000 (71%)]\tLoss: 1.545961\n",
      "Train Epoch: 26 [36000/49000 (73%)]\tLoss: 1.554610\n",
      "Train Epoch: 26 [37000/49000 (76%)]\tLoss: 1.569122\n",
      "Train Epoch: 26 [38000/49000 (78%)]\tLoss: 1.552011\n",
      "Train Epoch: 26 [39000/49000 (80%)]\tLoss: 1.542875\n",
      "Train Epoch: 26 [40000/49000 (82%)]\tLoss: 1.580807\n",
      "Train Epoch: 26 [41000/49000 (84%)]\tLoss: 1.552988\n",
      "Train Epoch: 26 [42000/49000 (86%)]\tLoss: 1.568664\n",
      "Train Epoch: 26 [43000/49000 (88%)]\tLoss: 1.546731\n",
      "Train Epoch: 26 [44000/49000 (90%)]\tLoss: 1.520038\n",
      "Train Epoch: 26 [45000/49000 (92%)]\tLoss: 1.584485\n",
      "Train Epoch: 26 [46000/49000 (94%)]\tLoss: 1.559283\n",
      "Train Epoch: 26 [47000/49000 (96%)]\tLoss: 1.511480\n",
      "Train Epoch: 26 [48000/49000 (98%)]\tLoss: 1.539731\n",
      "\n",
      "Test set: Avg. loss: 1.5554, Accuracy: 19047/21000 (90.70%)\n",
      "\n",
      "Train Epoch: 27 [0/49000 (0%)]\tLoss: 1.511787\n",
      "Train Epoch: 27 [1000/49000 (2%)]\tLoss: 1.584847\n",
      "Train Epoch: 27 [2000/49000 (4%)]\tLoss: 1.521353\n",
      "Train Epoch: 27 [3000/49000 (6%)]\tLoss: 1.579533\n",
      "Train Epoch: 27 [4000/49000 (8%)]\tLoss: 1.552914\n",
      "Train Epoch: 27 [5000/49000 (10%)]\tLoss: 1.556813\n",
      "Train Epoch: 27 [6000/49000 (12%)]\tLoss: 1.572415\n",
      "Train Epoch: 27 [7000/49000 (14%)]\tLoss: 1.567673\n",
      "Train Epoch: 27 [8000/49000 (16%)]\tLoss: 1.552895\n",
      "Train Epoch: 27 [9000/49000 (18%)]\tLoss: 1.536020\n",
      "Train Epoch: 27 [10000/49000 (20%)]\tLoss: 1.560584\n",
      "Train Epoch: 27 [11000/49000 (22%)]\tLoss: 1.497842\n",
      "Train Epoch: 27 [12000/49000 (24%)]\tLoss: 1.559629\n",
      "Train Epoch: 27 [13000/49000 (27%)]\tLoss: 1.567410\n",
      "Train Epoch: 27 [14000/49000 (29%)]\tLoss: 1.587577\n",
      "Train Epoch: 27 [15000/49000 (31%)]\tLoss: 1.525514\n",
      "Train Epoch: 27 [16000/49000 (33%)]\tLoss: 1.557546\n",
      "Train Epoch: 27 [17000/49000 (35%)]\tLoss: 1.524371\n",
      "Train Epoch: 27 [18000/49000 (37%)]\tLoss: 1.520107\n",
      "Train Epoch: 27 [19000/49000 (39%)]\tLoss: 1.610033\n",
      "Train Epoch: 27 [20000/49000 (41%)]\tLoss: 1.537193\n",
      "Train Epoch: 27 [21000/49000 (43%)]\tLoss: 1.566002\n",
      "Train Epoch: 27 [22000/49000 (45%)]\tLoss: 1.520520\n",
      "Train Epoch: 27 [23000/49000 (47%)]\tLoss: 1.547138\n",
      "Train Epoch: 27 [24000/49000 (49%)]\tLoss: 1.591602\n",
      "Train Epoch: 27 [25000/49000 (51%)]\tLoss: 1.555921\n",
      "Train Epoch: 27 [26000/49000 (53%)]\tLoss: 1.582097\n",
      "Train Epoch: 27 [27000/49000 (55%)]\tLoss: 1.563475\n",
      "Train Epoch: 27 [28000/49000 (57%)]\tLoss: 1.540870\n",
      "Train Epoch: 27 [29000/49000 (59%)]\tLoss: 1.538288\n",
      "Train Epoch: 27 [30000/49000 (61%)]\tLoss: 1.537672\n",
      "Train Epoch: 27 [31000/49000 (63%)]\tLoss: 1.539821\n",
      "Train Epoch: 27 [32000/49000 (65%)]\tLoss: 1.513262\n",
      "Train Epoch: 27 [33000/49000 (67%)]\tLoss: 1.552700\n",
      "Train Epoch: 27 [34000/49000 (69%)]\tLoss: 1.578719\n",
      "Train Epoch: 27 [35000/49000 (71%)]\tLoss: 1.549905\n",
      "Train Epoch: 27 [36000/49000 (73%)]\tLoss: 1.531381\n",
      "Train Epoch: 27 [37000/49000 (76%)]\tLoss: 1.566394\n",
      "Train Epoch: 27 [38000/49000 (78%)]\tLoss: 1.541383\n",
      "Train Epoch: 27 [39000/49000 (80%)]\tLoss: 1.529754\n",
      "Train Epoch: 27 [40000/49000 (82%)]\tLoss: 1.519436\n",
      "Train Epoch: 27 [41000/49000 (84%)]\tLoss: 1.543246\n",
      "Train Epoch: 27 [42000/49000 (86%)]\tLoss: 1.553752\n",
      "Train Epoch: 27 [43000/49000 (88%)]\tLoss: 1.555927\n",
      "Train Epoch: 27 [44000/49000 (90%)]\tLoss: 1.537906\n",
      "Train Epoch: 27 [45000/49000 (92%)]\tLoss: 1.527372\n",
      "Train Epoch: 27 [46000/49000 (94%)]\tLoss: 1.556173\n",
      "Train Epoch: 27 [47000/49000 (96%)]\tLoss: 1.569305\n",
      "Train Epoch: 27 [48000/49000 (98%)]\tLoss: 1.549803\n",
      "\n",
      "Test set: Avg. loss: 1.5569, Accuracy: 18957/21000 (90.27%)\n",
      "\n",
      "Train Epoch: 28 [0/49000 (0%)]\tLoss: 1.553143\n",
      "Train Epoch: 28 [1000/49000 (2%)]\tLoss: 1.595274\n",
      "Train Epoch: 28 [2000/49000 (4%)]\tLoss: 1.551546\n",
      "Train Epoch: 28 [3000/49000 (6%)]\tLoss: 1.580178\n",
      "Train Epoch: 28 [4000/49000 (8%)]\tLoss: 1.531825\n",
      "Train Epoch: 28 [5000/49000 (10%)]\tLoss: 1.566495\n",
      "Train Epoch: 28 [6000/49000 (12%)]\tLoss: 1.540100\n",
      "Train Epoch: 28 [7000/49000 (14%)]\tLoss: 1.545980\n",
      "Train Epoch: 28 [8000/49000 (16%)]\tLoss: 1.538168\n",
      "Train Epoch: 28 [9000/49000 (18%)]\tLoss: 1.523344\n",
      "Train Epoch: 28 [10000/49000 (20%)]\tLoss: 1.526274\n",
      "Train Epoch: 28 [11000/49000 (22%)]\tLoss: 1.514304\n",
      "Train Epoch: 28 [12000/49000 (24%)]\tLoss: 1.534847\n",
      "Train Epoch: 28 [13000/49000 (27%)]\tLoss: 1.577122\n",
      "Train Epoch: 28 [14000/49000 (29%)]\tLoss: 1.554307\n",
      "Train Epoch: 28 [15000/49000 (31%)]\tLoss: 1.560168\n",
      "Train Epoch: 28 [16000/49000 (33%)]\tLoss: 1.555466\n",
      "Train Epoch: 28 [17000/49000 (35%)]\tLoss: 1.521317\n",
      "Train Epoch: 28 [18000/49000 (37%)]\tLoss: 1.533455\n",
      "Train Epoch: 28 [19000/49000 (39%)]\tLoss: 1.542650\n",
      "Train Epoch: 28 [20000/49000 (41%)]\tLoss: 1.537426\n",
      "Train Epoch: 28 [21000/49000 (43%)]\tLoss: 1.530009\n",
      "Train Epoch: 28 [22000/49000 (45%)]\tLoss: 1.603581\n",
      "Train Epoch: 28 [23000/49000 (47%)]\tLoss: 1.538918\n",
      "Train Epoch: 28 [24000/49000 (49%)]\tLoss: 1.579236\n",
      "Train Epoch: 28 [25000/49000 (51%)]\tLoss: 1.566655\n",
      "Train Epoch: 28 [26000/49000 (53%)]\tLoss: 1.549151\n",
      "Train Epoch: 28 [27000/49000 (55%)]\tLoss: 1.525819\n",
      "Train Epoch: 28 [28000/49000 (57%)]\tLoss: 1.589133\n",
      "Train Epoch: 28 [29000/49000 (59%)]\tLoss: 1.564229\n",
      "Train Epoch: 28 [30000/49000 (61%)]\tLoss: 1.538031\n",
      "Train Epoch: 28 [31000/49000 (63%)]\tLoss: 1.551489\n",
      "Train Epoch: 28 [32000/49000 (65%)]\tLoss: 1.538685\n",
      "Train Epoch: 28 [33000/49000 (67%)]\tLoss: 1.560746\n",
      "Train Epoch: 28 [34000/49000 (69%)]\tLoss: 1.524346\n",
      "Train Epoch: 28 [35000/49000 (71%)]\tLoss: 1.551219\n",
      "Train Epoch: 28 [36000/49000 (73%)]\tLoss: 1.538835\n",
      "Train Epoch: 28 [37000/49000 (76%)]\tLoss: 1.584057\n",
      "Train Epoch: 28 [38000/49000 (78%)]\tLoss: 1.538151\n",
      "Train Epoch: 28 [39000/49000 (80%)]\tLoss: 1.534788\n",
      "Train Epoch: 28 [40000/49000 (82%)]\tLoss: 1.547962\n",
      "Train Epoch: 28 [41000/49000 (84%)]\tLoss: 1.551929\n",
      "Train Epoch: 28 [42000/49000 (86%)]\tLoss: 1.517613\n",
      "Train Epoch: 28 [43000/49000 (88%)]\tLoss: 1.545863\n",
      "Train Epoch: 28 [44000/49000 (90%)]\tLoss: 1.551898\n",
      "Train Epoch: 28 [45000/49000 (92%)]\tLoss: 1.545943\n",
      "Train Epoch: 28 [46000/49000 (94%)]\tLoss: 1.522911\n",
      "Train Epoch: 28 [47000/49000 (96%)]\tLoss: 1.548952\n",
      "Train Epoch: 28 [48000/49000 (98%)]\tLoss: 1.558576\n",
      "\n",
      "Test set: Avg. loss: 1.5563, Accuracy: 19075/21000 (90.83%)\n",
      "\n",
      "Train Epoch: 29 [0/49000 (0%)]\tLoss: 1.605533\n",
      "Train Epoch: 29 [1000/49000 (2%)]\tLoss: 1.558026\n",
      "Train Epoch: 29 [2000/49000 (4%)]\tLoss: 1.562371\n",
      "Train Epoch: 29 [3000/49000 (6%)]\tLoss: 1.530503\n",
      "Train Epoch: 29 [4000/49000 (8%)]\tLoss: 1.554405\n",
      "Train Epoch: 29 [5000/49000 (10%)]\tLoss: 1.556496\n",
      "Train Epoch: 29 [6000/49000 (12%)]\tLoss: 1.534885\n",
      "Train Epoch: 29 [7000/49000 (14%)]\tLoss: 1.536907\n",
      "Train Epoch: 29 [8000/49000 (16%)]\tLoss: 1.528247\n",
      "Train Epoch: 29 [9000/49000 (18%)]\tLoss: 1.558651\n",
      "Train Epoch: 29 [10000/49000 (20%)]\tLoss: 1.518749\n",
      "Train Epoch: 29 [11000/49000 (22%)]\tLoss: 1.557918\n",
      "Train Epoch: 29 [12000/49000 (24%)]\tLoss: 1.511282\n",
      "Train Epoch: 29 [13000/49000 (27%)]\tLoss: 1.521408\n",
      "Train Epoch: 29 [14000/49000 (29%)]\tLoss: 1.556472\n",
      "Train Epoch: 29 [15000/49000 (31%)]\tLoss: 1.564619\n",
      "Train Epoch: 29 [16000/49000 (33%)]\tLoss: 1.518632\n",
      "Train Epoch: 29 [17000/49000 (35%)]\tLoss: 1.578277\n",
      "Train Epoch: 29 [18000/49000 (37%)]\tLoss: 1.528831\n",
      "Train Epoch: 29 [19000/49000 (39%)]\tLoss: 1.509058\n",
      "Train Epoch: 29 [20000/49000 (41%)]\tLoss: 1.518872\n",
      "Train Epoch: 29 [21000/49000 (43%)]\tLoss: 1.550387\n",
      "Train Epoch: 29 [22000/49000 (45%)]\tLoss: 1.566564\n",
      "Train Epoch: 29 [23000/49000 (47%)]\tLoss: 1.522404\n",
      "Train Epoch: 29 [24000/49000 (49%)]\tLoss: 1.549831\n",
      "Train Epoch: 29 [25000/49000 (51%)]\tLoss: 1.541919\n",
      "Train Epoch: 29 [26000/49000 (53%)]\tLoss: 1.561560\n",
      "Train Epoch: 29 [27000/49000 (55%)]\tLoss: 1.554479\n",
      "Train Epoch: 29 [28000/49000 (57%)]\tLoss: 1.529378\n",
      "Train Epoch: 29 [29000/49000 (59%)]\tLoss: 1.591470\n",
      "Train Epoch: 29 [30000/49000 (61%)]\tLoss: 1.528226\n",
      "Train Epoch: 29 [31000/49000 (63%)]\tLoss: 1.570064\n",
      "Train Epoch: 29 [32000/49000 (65%)]\tLoss: 1.544411\n",
      "Train Epoch: 29 [33000/49000 (67%)]\tLoss: 1.545666\n",
      "Train Epoch: 29 [34000/49000 (69%)]\tLoss: 1.543029\n",
      "Train Epoch: 29 [35000/49000 (71%)]\tLoss: 1.550784\n",
      "Train Epoch: 29 [36000/49000 (73%)]\tLoss: 1.526034\n",
      "Train Epoch: 29 [37000/49000 (76%)]\tLoss: 1.567119\n",
      "Train Epoch: 29 [38000/49000 (78%)]\tLoss: 1.576594\n",
      "Train Epoch: 29 [39000/49000 (80%)]\tLoss: 1.602336\n",
      "Train Epoch: 29 [40000/49000 (82%)]\tLoss: 1.546106\n",
      "Train Epoch: 29 [41000/49000 (84%)]\tLoss: 1.565742\n",
      "Train Epoch: 29 [42000/49000 (86%)]\tLoss: 1.530333\n",
      "Train Epoch: 29 [43000/49000 (88%)]\tLoss: 1.583074\n",
      "Train Epoch: 29 [44000/49000 (90%)]\tLoss: 1.562512\n",
      "Train Epoch: 29 [45000/49000 (92%)]\tLoss: 1.525492\n",
      "Train Epoch: 29 [46000/49000 (94%)]\tLoss: 1.527080\n",
      "Train Epoch: 29 [47000/49000 (96%)]\tLoss: 1.549013\n",
      "Train Epoch: 29 [48000/49000 (98%)]\tLoss: 1.516652\n",
      "\n",
      "Test set: Avg. loss: 1.5558, Accuracy: 19018/21000 (90.56%)\n",
      "\n",
      "Train Epoch: 30 [0/49000 (0%)]\tLoss: 1.542740\n",
      "Train Epoch: 30 [1000/49000 (2%)]\tLoss: 1.524629\n",
      "Train Epoch: 30 [2000/49000 (4%)]\tLoss: 1.540046\n",
      "Train Epoch: 30 [3000/49000 (6%)]\tLoss: 1.506061\n",
      "Train Epoch: 30 [4000/49000 (8%)]\tLoss: 1.534082\n",
      "Train Epoch: 30 [5000/49000 (10%)]\tLoss: 1.535636\n",
      "Train Epoch: 30 [6000/49000 (12%)]\tLoss: 1.538424\n",
      "Train Epoch: 30 [7000/49000 (14%)]\tLoss: 1.570795\n",
      "Train Epoch: 30 [8000/49000 (16%)]\tLoss: 1.565780\n",
      "Train Epoch: 30 [9000/49000 (18%)]\tLoss: 1.531997\n",
      "Train Epoch: 30 [10000/49000 (20%)]\tLoss: 1.530497\n",
      "Train Epoch: 30 [11000/49000 (22%)]\tLoss: 1.526466\n",
      "Train Epoch: 30 [12000/49000 (24%)]\tLoss: 1.544624\n",
      "Train Epoch: 30 [13000/49000 (27%)]\tLoss: 1.543337\n",
      "Train Epoch: 30 [14000/49000 (29%)]\tLoss: 1.548194\n",
      "Train Epoch: 30 [15000/49000 (31%)]\tLoss: 1.552590\n",
      "Train Epoch: 30 [16000/49000 (33%)]\tLoss: 1.584336\n",
      "Train Epoch: 30 [17000/49000 (35%)]\tLoss: 1.576078\n",
      "Train Epoch: 30 [18000/49000 (37%)]\tLoss: 1.523804\n",
      "Train Epoch: 30 [19000/49000 (39%)]\tLoss: 1.544383\n",
      "Train Epoch: 30 [20000/49000 (41%)]\tLoss: 1.528237\n",
      "Train Epoch: 30 [21000/49000 (43%)]\tLoss: 1.574377\n",
      "Train Epoch: 30 [22000/49000 (45%)]\tLoss: 1.563272\n",
      "Train Epoch: 30 [23000/49000 (47%)]\tLoss: 1.545501\n",
      "Train Epoch: 30 [24000/49000 (49%)]\tLoss: 1.527076\n",
      "Train Epoch: 30 [25000/49000 (51%)]\tLoss: 1.551374\n",
      "Train Epoch: 30 [26000/49000 (53%)]\tLoss: 1.536667\n",
      "Train Epoch: 30 [27000/49000 (55%)]\tLoss: 1.533728\n",
      "Train Epoch: 30 [28000/49000 (57%)]\tLoss: 1.577093\n",
      "Train Epoch: 30 [29000/49000 (59%)]\tLoss: 1.527908\n",
      "Train Epoch: 30 [30000/49000 (61%)]\tLoss: 1.551676\n",
      "Train Epoch: 30 [31000/49000 (63%)]\tLoss: 1.569810\n",
      "Train Epoch: 30 [32000/49000 (65%)]\tLoss: 1.537242\n",
      "Train Epoch: 30 [33000/49000 (67%)]\tLoss: 1.561025\n",
      "Train Epoch: 30 [34000/49000 (69%)]\tLoss: 1.531297\n",
      "Train Epoch: 30 [35000/49000 (71%)]\tLoss: 1.532744\n",
      "Train Epoch: 30 [36000/49000 (73%)]\tLoss: 1.542683\n",
      "Train Epoch: 30 [37000/49000 (76%)]\tLoss: 1.586153\n",
      "Train Epoch: 30 [38000/49000 (78%)]\tLoss: 1.556446\n",
      "Train Epoch: 30 [39000/49000 (80%)]\tLoss: 1.537712\n",
      "Train Epoch: 30 [40000/49000 (82%)]\tLoss: 1.569882\n",
      "Train Epoch: 30 [41000/49000 (84%)]\tLoss: 1.563456\n",
      "Train Epoch: 30 [42000/49000 (86%)]\tLoss: 1.525911\n",
      "Train Epoch: 30 [43000/49000 (88%)]\tLoss: 1.542407\n",
      "Train Epoch: 30 [44000/49000 (90%)]\tLoss: 1.532930\n",
      "Train Epoch: 30 [45000/49000 (92%)]\tLoss: 1.565278\n",
      "Train Epoch: 30 [46000/49000 (94%)]\tLoss: 1.535917\n",
      "Train Epoch: 30 [47000/49000 (96%)]\tLoss: 1.570691\n",
      "Train Epoch: 30 [48000/49000 (98%)]\tLoss: 1.536821\n",
      "\n",
      "Test set: Avg. loss: 1.5549, Accuracy: 19053/21000 (90.73%)\n",
      "\n",
      "Train Epoch: 31 [0/49000 (0%)]\tLoss: 1.553502\n",
      "Train Epoch: 31 [1000/49000 (2%)]\tLoss: 1.528508\n",
      "Train Epoch: 31 [2000/49000 (4%)]\tLoss: 1.531558\n",
      "Train Epoch: 31 [3000/49000 (6%)]\tLoss: 1.548579\n",
      "Train Epoch: 31 [4000/49000 (8%)]\tLoss: 1.566509\n",
      "Train Epoch: 31 [5000/49000 (10%)]\tLoss: 1.518153\n",
      "Train Epoch: 31 [6000/49000 (12%)]\tLoss: 1.554017\n",
      "Train Epoch: 31 [7000/49000 (14%)]\tLoss: 1.541052\n",
      "Train Epoch: 31 [8000/49000 (16%)]\tLoss: 1.574120\n",
      "Train Epoch: 31 [9000/49000 (18%)]\tLoss: 1.498094\n",
      "Train Epoch: 31 [10000/49000 (20%)]\tLoss: 1.560604\n",
      "Train Epoch: 31 [11000/49000 (22%)]\tLoss: 1.542926\n",
      "Train Epoch: 31 [12000/49000 (24%)]\tLoss: 1.560560\n",
      "Train Epoch: 31 [13000/49000 (27%)]\tLoss: 1.559640\n",
      "Train Epoch: 31 [14000/49000 (29%)]\tLoss: 1.562942\n",
      "Train Epoch: 31 [15000/49000 (31%)]\tLoss: 1.573101\n",
      "Train Epoch: 31 [16000/49000 (33%)]\tLoss: 1.556236\n",
      "Train Epoch: 31 [17000/49000 (35%)]\tLoss: 1.542735\n",
      "Train Epoch: 31 [18000/49000 (37%)]\tLoss: 1.553222\n",
      "Train Epoch: 31 [19000/49000 (39%)]\tLoss: 1.542484\n",
      "Train Epoch: 31 [20000/49000 (41%)]\tLoss: 1.528185\n",
      "Train Epoch: 31 [21000/49000 (43%)]\tLoss: 1.535825\n",
      "Train Epoch: 31 [22000/49000 (45%)]\tLoss: 1.546222\n",
      "Train Epoch: 31 [23000/49000 (47%)]\tLoss: 1.551207\n",
      "Train Epoch: 31 [24000/49000 (49%)]\tLoss: 1.539576\n",
      "Train Epoch: 31 [25000/49000 (51%)]\tLoss: 1.559836\n",
      "Train Epoch: 31 [26000/49000 (53%)]\tLoss: 1.517098\n",
      "Train Epoch: 31 [27000/49000 (55%)]\tLoss: 1.491045\n",
      "Train Epoch: 31 [28000/49000 (57%)]\tLoss: 1.555632\n",
      "Train Epoch: 31 [29000/49000 (59%)]\tLoss: 1.585897\n",
      "Train Epoch: 31 [30000/49000 (61%)]\tLoss: 1.533490\n",
      "Train Epoch: 31 [31000/49000 (63%)]\tLoss: 1.534956\n",
      "Train Epoch: 31 [32000/49000 (65%)]\tLoss: 1.569864\n",
      "Train Epoch: 31 [33000/49000 (67%)]\tLoss: 1.599194\n",
      "Train Epoch: 31 [34000/49000 (69%)]\tLoss: 1.523556\n",
      "Train Epoch: 31 [35000/49000 (71%)]\tLoss: 1.550005\n",
      "Train Epoch: 31 [36000/49000 (73%)]\tLoss: 1.540979\n",
      "Train Epoch: 31 [37000/49000 (76%)]\tLoss: 1.556075\n",
      "Train Epoch: 31 [38000/49000 (78%)]\tLoss: 1.546731\n",
      "Train Epoch: 31 [39000/49000 (80%)]\tLoss: 1.566359\n",
      "Train Epoch: 31 [40000/49000 (82%)]\tLoss: 1.525909\n",
      "Train Epoch: 31 [41000/49000 (84%)]\tLoss: 1.559262\n",
      "Train Epoch: 31 [42000/49000 (86%)]\tLoss: 1.562121\n",
      "Train Epoch: 31 [43000/49000 (88%)]\tLoss: 1.564892\n",
      "Train Epoch: 31 [44000/49000 (90%)]\tLoss: 1.601182\n",
      "Train Epoch: 31 [45000/49000 (92%)]\tLoss: 1.554962\n",
      "Train Epoch: 31 [46000/49000 (94%)]\tLoss: 1.540561\n",
      "Train Epoch: 31 [47000/49000 (96%)]\tLoss: 1.534598\n",
      "Train Epoch: 31 [48000/49000 (98%)]\tLoss: 1.562511\n",
      "\n",
      "Test set: Avg. loss: 1.5549, Accuracy: 19058/21000 (90.75%)\n",
      "\n",
      "Train Epoch: 32 [0/49000 (0%)]\tLoss: 1.540532\n",
      "Train Epoch: 32 [1000/49000 (2%)]\tLoss: 1.551655\n",
      "Train Epoch: 32 [2000/49000 (4%)]\tLoss: 1.565277\n",
      "Train Epoch: 32 [3000/49000 (6%)]\tLoss: 1.554046\n",
      "Train Epoch: 32 [4000/49000 (8%)]\tLoss: 1.520744\n",
      "Train Epoch: 32 [5000/49000 (10%)]\tLoss: 1.559464\n",
      "Train Epoch: 32 [6000/49000 (12%)]\tLoss: 1.554757\n",
      "Train Epoch: 32 [7000/49000 (14%)]\tLoss: 1.525363\n",
      "Train Epoch: 32 [8000/49000 (16%)]\tLoss: 1.570190\n",
      "Train Epoch: 32 [9000/49000 (18%)]\tLoss: 1.549968\n",
      "Train Epoch: 32 [10000/49000 (20%)]\tLoss: 1.535388\n",
      "Train Epoch: 32 [11000/49000 (22%)]\tLoss: 1.533958\n",
      "Train Epoch: 32 [12000/49000 (24%)]\tLoss: 1.535396\n",
      "Train Epoch: 32 [13000/49000 (27%)]\tLoss: 1.545292\n",
      "Train Epoch: 32 [14000/49000 (29%)]\tLoss: 1.541928\n",
      "Train Epoch: 32 [15000/49000 (31%)]\tLoss: 1.566436\n",
      "Train Epoch: 32 [16000/49000 (33%)]\tLoss: 1.533469\n",
      "Train Epoch: 32 [17000/49000 (35%)]\tLoss: 1.520598\n",
      "Train Epoch: 32 [18000/49000 (37%)]\tLoss: 1.565957\n",
      "Train Epoch: 32 [19000/49000 (39%)]\tLoss: 1.566156\n",
      "Train Epoch: 32 [20000/49000 (41%)]\tLoss: 1.575370\n",
      "Train Epoch: 32 [21000/49000 (43%)]\tLoss: 1.525684\n",
      "Train Epoch: 32 [22000/49000 (45%)]\tLoss: 1.548159\n",
      "Train Epoch: 32 [23000/49000 (47%)]\tLoss: 1.540893\n",
      "Train Epoch: 32 [24000/49000 (49%)]\tLoss: 1.562964\n",
      "Train Epoch: 32 [25000/49000 (51%)]\tLoss: 1.523004\n",
      "Train Epoch: 32 [26000/49000 (53%)]\tLoss: 1.558549\n",
      "Train Epoch: 32 [27000/49000 (55%)]\tLoss: 1.532132\n",
      "Train Epoch: 32 [28000/49000 (57%)]\tLoss: 1.539090\n",
      "Train Epoch: 32 [29000/49000 (59%)]\tLoss: 1.516948\n",
      "Train Epoch: 32 [30000/49000 (61%)]\tLoss: 1.558704\n",
      "Train Epoch: 32 [31000/49000 (63%)]\tLoss: 1.535121\n",
      "Train Epoch: 32 [32000/49000 (65%)]\tLoss: 1.542821\n",
      "Train Epoch: 32 [33000/49000 (67%)]\tLoss: 1.517375\n",
      "Train Epoch: 32 [34000/49000 (69%)]\tLoss: 1.572228\n",
      "Train Epoch: 32 [35000/49000 (71%)]\tLoss: 1.541980\n",
      "Train Epoch: 32 [36000/49000 (73%)]\tLoss: 1.549400\n",
      "Train Epoch: 32 [37000/49000 (76%)]\tLoss: 1.539826\n",
      "Train Epoch: 32 [38000/49000 (78%)]\tLoss: 1.567953\n",
      "Train Epoch: 32 [39000/49000 (80%)]\tLoss: 1.584943\n",
      "Train Epoch: 32 [40000/49000 (82%)]\tLoss: 1.527861\n",
      "Train Epoch: 32 [41000/49000 (84%)]\tLoss: 1.541664\n",
      "Train Epoch: 32 [42000/49000 (86%)]\tLoss: 1.551926\n",
      "Train Epoch: 32 [43000/49000 (88%)]\tLoss: 1.526632\n",
      "Train Epoch: 32 [44000/49000 (90%)]\tLoss: 1.543169\n",
      "Train Epoch: 32 [45000/49000 (92%)]\tLoss: 1.536630\n",
      "Train Epoch: 32 [46000/49000 (94%)]\tLoss: 1.518859\n",
      "Train Epoch: 32 [47000/49000 (96%)]\tLoss: 1.512396\n",
      "Train Epoch: 32 [48000/49000 (98%)]\tLoss: 1.515005\n",
      "\n",
      "Test set: Avg. loss: 1.5540, Accuracy: 19047/21000 (90.70%)\n",
      "\n",
      "Train Epoch: 33 [0/49000 (0%)]\tLoss: 1.555235\n",
      "Train Epoch: 33 [1000/49000 (2%)]\tLoss: 1.556191\n",
      "Train Epoch: 33 [2000/49000 (4%)]\tLoss: 1.528078\n",
      "Train Epoch: 33 [3000/49000 (6%)]\tLoss: 1.552910\n",
      "Train Epoch: 33 [4000/49000 (8%)]\tLoss: 1.533769\n",
      "Train Epoch: 33 [5000/49000 (10%)]\tLoss: 1.534860\n",
      "Train Epoch: 33 [6000/49000 (12%)]\tLoss: 1.575786\n",
      "Train Epoch: 33 [7000/49000 (14%)]\tLoss: 1.554463\n",
      "Train Epoch: 33 [8000/49000 (16%)]\tLoss: 1.566503\n",
      "Train Epoch: 33 [9000/49000 (18%)]\tLoss: 1.550995\n",
      "Train Epoch: 33 [10000/49000 (20%)]\tLoss: 1.541804\n",
      "Train Epoch: 33 [11000/49000 (22%)]\tLoss: 1.569183\n",
      "Train Epoch: 33 [12000/49000 (24%)]\tLoss: 1.551447\n",
      "Train Epoch: 33 [13000/49000 (27%)]\tLoss: 1.566871\n",
      "Train Epoch: 33 [14000/49000 (29%)]\tLoss: 1.534616\n",
      "Train Epoch: 33 [15000/49000 (31%)]\tLoss: 1.551489\n",
      "Train Epoch: 33 [16000/49000 (33%)]\tLoss: 1.556571\n",
      "Train Epoch: 33 [17000/49000 (35%)]\tLoss: 1.544984\n",
      "Train Epoch: 33 [18000/49000 (37%)]\tLoss: 1.561321\n",
      "Train Epoch: 33 [19000/49000 (39%)]\tLoss: 1.580636\n",
      "Train Epoch: 33 [20000/49000 (41%)]\tLoss: 1.520519\n",
      "Train Epoch: 33 [21000/49000 (43%)]\tLoss: 1.560236\n",
      "Train Epoch: 33 [22000/49000 (45%)]\tLoss: 1.521401\n",
      "Train Epoch: 33 [23000/49000 (47%)]\tLoss: 1.540651\n",
      "Train Epoch: 33 [24000/49000 (49%)]\tLoss: 1.582433\n",
      "Train Epoch: 33 [25000/49000 (51%)]\tLoss: 1.565775\n",
      "Train Epoch: 33 [26000/49000 (53%)]\tLoss: 1.534713\n",
      "Train Epoch: 33 [27000/49000 (55%)]\tLoss: 1.570299\n",
      "Train Epoch: 33 [28000/49000 (57%)]\tLoss: 1.544076\n",
      "Train Epoch: 33 [29000/49000 (59%)]\tLoss: 1.538513\n",
      "Train Epoch: 33 [30000/49000 (61%)]\tLoss: 1.517071\n",
      "Train Epoch: 33 [31000/49000 (63%)]\tLoss: 1.533907\n",
      "Train Epoch: 33 [32000/49000 (65%)]\tLoss: 1.554574\n",
      "Train Epoch: 33 [33000/49000 (67%)]\tLoss: 1.538648\n",
      "Train Epoch: 33 [34000/49000 (69%)]\tLoss: 1.533595\n",
      "Train Epoch: 33 [35000/49000 (71%)]\tLoss: 1.557641\n",
      "Train Epoch: 33 [36000/49000 (73%)]\tLoss: 1.523244\n",
      "Train Epoch: 33 [37000/49000 (76%)]\tLoss: 1.520517\n",
      "Train Epoch: 33 [38000/49000 (78%)]\tLoss: 1.540243\n",
      "Train Epoch: 33 [39000/49000 (80%)]\tLoss: 1.545715\n",
      "Train Epoch: 33 [40000/49000 (82%)]\tLoss: 1.531759\n",
      "Train Epoch: 33 [41000/49000 (84%)]\tLoss: 1.528330\n",
      "Train Epoch: 33 [42000/49000 (86%)]\tLoss: 1.531597\n",
      "Train Epoch: 33 [43000/49000 (88%)]\tLoss: 1.534014\n",
      "Train Epoch: 33 [44000/49000 (90%)]\tLoss: 1.533913\n",
      "Train Epoch: 33 [45000/49000 (92%)]\tLoss: 1.554070\n",
      "Train Epoch: 33 [46000/49000 (94%)]\tLoss: 1.531103\n",
      "Train Epoch: 33 [47000/49000 (96%)]\tLoss: 1.567513\n",
      "Train Epoch: 33 [48000/49000 (98%)]\tLoss: 1.540554\n",
      "\n",
      "Test set: Avg. loss: 1.5560, Accuracy: 19068/21000 (90.80%)\n",
      "\n",
      "Train Epoch: 34 [0/49000 (0%)]\tLoss: 1.544854\n",
      "Train Epoch: 34 [1000/49000 (2%)]\tLoss: 1.547515\n",
      "Train Epoch: 34 [2000/49000 (4%)]\tLoss: 1.539083\n",
      "Train Epoch: 34 [3000/49000 (6%)]\tLoss: 1.536484\n",
      "Train Epoch: 34 [4000/49000 (8%)]\tLoss: 1.546081\n",
      "Train Epoch: 34 [5000/49000 (10%)]\tLoss: 1.542094\n",
      "Train Epoch: 34 [6000/49000 (12%)]\tLoss: 1.532249\n",
      "Train Epoch: 34 [7000/49000 (14%)]\tLoss: 1.542416\n",
      "Train Epoch: 34 [8000/49000 (16%)]\tLoss: 1.518461\n",
      "Train Epoch: 34 [9000/49000 (18%)]\tLoss: 1.503850\n",
      "Train Epoch: 34 [10000/49000 (20%)]\tLoss: 1.551310\n",
      "Train Epoch: 34 [11000/49000 (22%)]\tLoss: 1.532091\n",
      "Train Epoch: 34 [12000/49000 (24%)]\tLoss: 1.594582\n",
      "Train Epoch: 34 [13000/49000 (27%)]\tLoss: 1.544859\n",
      "Train Epoch: 34 [14000/49000 (29%)]\tLoss: 1.531587\n",
      "Train Epoch: 34 [15000/49000 (31%)]\tLoss: 1.541721\n",
      "Train Epoch: 34 [16000/49000 (33%)]\tLoss: 1.543945\n",
      "Train Epoch: 34 [17000/49000 (35%)]\tLoss: 1.542579\n",
      "Train Epoch: 34 [18000/49000 (37%)]\tLoss: 1.617352\n",
      "Train Epoch: 34 [19000/49000 (39%)]\tLoss: 1.553100\n",
      "Train Epoch: 34 [20000/49000 (41%)]\tLoss: 1.545385\n",
      "Train Epoch: 34 [21000/49000 (43%)]\tLoss: 1.518260\n",
      "Train Epoch: 34 [22000/49000 (45%)]\tLoss: 1.552055\n",
      "Train Epoch: 34 [23000/49000 (47%)]\tLoss: 1.533257\n",
      "Train Epoch: 34 [24000/49000 (49%)]\tLoss: 1.527608\n",
      "Train Epoch: 34 [25000/49000 (51%)]\tLoss: 1.547466\n",
      "Train Epoch: 34 [26000/49000 (53%)]\tLoss: 1.521047\n",
      "Train Epoch: 34 [27000/49000 (55%)]\tLoss: 1.545659\n",
      "Train Epoch: 34 [28000/49000 (57%)]\tLoss: 1.560900\n",
      "Train Epoch: 34 [29000/49000 (59%)]\tLoss: 1.543142\n",
      "Train Epoch: 34 [30000/49000 (61%)]\tLoss: 1.547420\n",
      "Train Epoch: 34 [31000/49000 (63%)]\tLoss: 1.584930\n",
      "Train Epoch: 34 [32000/49000 (65%)]\tLoss: 1.533976\n",
      "Train Epoch: 34 [33000/49000 (67%)]\tLoss: 1.526575\n",
      "Train Epoch: 34 [34000/49000 (69%)]\tLoss: 1.530355\n",
      "Train Epoch: 34 [35000/49000 (71%)]\tLoss: 1.529083\n",
      "Train Epoch: 34 [36000/49000 (73%)]\tLoss: 1.553173\n",
      "Train Epoch: 34 [37000/49000 (76%)]\tLoss: 1.560624\n",
      "Train Epoch: 34 [38000/49000 (78%)]\tLoss: 1.560379\n",
      "Train Epoch: 34 [39000/49000 (80%)]\tLoss: 1.550512\n",
      "Train Epoch: 34 [40000/49000 (82%)]\tLoss: 1.533291\n",
      "Train Epoch: 34 [41000/49000 (84%)]\tLoss: 1.501005\n",
      "Train Epoch: 34 [42000/49000 (86%)]\tLoss: 1.531902\n",
      "Train Epoch: 34 [43000/49000 (88%)]\tLoss: 1.558506\n",
      "Train Epoch: 34 [44000/49000 (90%)]\tLoss: 1.556369\n",
      "Train Epoch: 34 [45000/49000 (92%)]\tLoss: 1.553362\n",
      "Train Epoch: 34 [46000/49000 (94%)]\tLoss: 1.555585\n",
      "Train Epoch: 34 [47000/49000 (96%)]\tLoss: 1.522872\n",
      "Train Epoch: 34 [48000/49000 (98%)]\tLoss: 1.577243\n",
      "\n",
      "Test set: Avg. loss: 1.5536, Accuracy: 19084/21000 (90.88%)\n",
      "\n",
      "Train Epoch: 35 [0/49000 (0%)]\tLoss: 1.553012\n",
      "Train Epoch: 35 [1000/49000 (2%)]\tLoss: 1.534361\n",
      "Train Epoch: 35 [2000/49000 (4%)]\tLoss: 1.537618\n",
      "Train Epoch: 35 [3000/49000 (6%)]\tLoss: 1.553617\n",
      "Train Epoch: 35 [4000/49000 (8%)]\tLoss: 1.553442\n",
      "Train Epoch: 35 [5000/49000 (10%)]\tLoss: 1.537853\n",
      "Train Epoch: 35 [6000/49000 (12%)]\tLoss: 1.542688\n",
      "Train Epoch: 35 [7000/49000 (14%)]\tLoss: 1.569981\n",
      "Train Epoch: 35 [8000/49000 (16%)]\tLoss: 1.531801\n",
      "Train Epoch: 35 [9000/49000 (18%)]\tLoss: 1.515600\n",
      "Train Epoch: 35 [10000/49000 (20%)]\tLoss: 1.552917\n",
      "Train Epoch: 35 [11000/49000 (22%)]\tLoss: 1.532907\n",
      "Train Epoch: 35 [12000/49000 (24%)]\tLoss: 1.508938\n",
      "Train Epoch: 35 [13000/49000 (27%)]\tLoss: 1.528265\n",
      "Train Epoch: 35 [14000/49000 (29%)]\tLoss: 1.519075\n",
      "Train Epoch: 35 [15000/49000 (31%)]\tLoss: 1.524972\n",
      "Train Epoch: 35 [16000/49000 (33%)]\tLoss: 1.521751\n",
      "Train Epoch: 35 [17000/49000 (35%)]\tLoss: 1.575852\n",
      "Train Epoch: 35 [18000/49000 (37%)]\tLoss: 1.563909\n",
      "Train Epoch: 35 [19000/49000 (39%)]\tLoss: 1.522431\n",
      "Train Epoch: 35 [20000/49000 (41%)]\tLoss: 1.557080\n",
      "Train Epoch: 35 [21000/49000 (43%)]\tLoss: 1.541237\n",
      "Train Epoch: 35 [22000/49000 (45%)]\tLoss: 1.551755\n",
      "Train Epoch: 35 [23000/49000 (47%)]\tLoss: 1.556255\n",
      "Train Epoch: 35 [24000/49000 (49%)]\tLoss: 1.558178\n",
      "Train Epoch: 35 [25000/49000 (51%)]\tLoss: 1.510362\n",
      "Train Epoch: 35 [26000/49000 (53%)]\tLoss: 1.542005\n",
      "Train Epoch: 35 [27000/49000 (55%)]\tLoss: 1.524444\n",
      "Train Epoch: 35 [28000/49000 (57%)]\tLoss: 1.559261\n",
      "Train Epoch: 35 [29000/49000 (59%)]\tLoss: 1.565069\n",
      "Train Epoch: 35 [30000/49000 (61%)]\tLoss: 1.514867\n",
      "Train Epoch: 35 [31000/49000 (63%)]\tLoss: 1.544999\n",
      "Train Epoch: 35 [32000/49000 (65%)]\tLoss: 1.532087\n",
      "Train Epoch: 35 [33000/49000 (67%)]\tLoss: 1.566165\n",
      "Train Epoch: 35 [34000/49000 (69%)]\tLoss: 1.579499\n",
      "Train Epoch: 35 [35000/49000 (71%)]\tLoss: 1.548331\n",
      "Train Epoch: 35 [36000/49000 (73%)]\tLoss: 1.536901\n",
      "Train Epoch: 35 [37000/49000 (76%)]\tLoss: 1.510757\n",
      "Train Epoch: 35 [38000/49000 (78%)]\tLoss: 1.539356\n",
      "Train Epoch: 35 [39000/49000 (80%)]\tLoss: 1.505048\n",
      "Train Epoch: 35 [40000/49000 (82%)]\tLoss: 1.551208\n",
      "Train Epoch: 35 [41000/49000 (84%)]\tLoss: 1.534181\n",
      "Train Epoch: 35 [42000/49000 (86%)]\tLoss: 1.534472\n",
      "Train Epoch: 35 [43000/49000 (88%)]\tLoss: 1.566510\n",
      "Train Epoch: 35 [44000/49000 (90%)]\tLoss: 1.529804\n",
      "Train Epoch: 35 [45000/49000 (92%)]\tLoss: 1.535408\n",
      "Train Epoch: 35 [46000/49000 (94%)]\tLoss: 1.568919\n",
      "Train Epoch: 35 [47000/49000 (96%)]\tLoss: 1.515522\n",
      "Train Epoch: 35 [48000/49000 (98%)]\tLoss: 1.539819\n",
      "\n",
      "Test set: Avg. loss: 1.5555, Accuracy: 19035/21000 (90.64%)\n",
      "\n",
      "Train Epoch: 36 [0/49000 (0%)]\tLoss: 1.504543\n",
      "Train Epoch: 36 [1000/49000 (2%)]\tLoss: 1.560075\n",
      "Train Epoch: 36 [2000/49000 (4%)]\tLoss: 1.590437\n",
      "Train Epoch: 36 [3000/49000 (6%)]\tLoss: 1.571176\n",
      "Train Epoch: 36 [4000/49000 (8%)]\tLoss: 1.543869\n",
      "Train Epoch: 36 [5000/49000 (10%)]\tLoss: 1.563205\n",
      "Train Epoch: 36 [6000/49000 (12%)]\tLoss: 1.516817\n",
      "Train Epoch: 36 [7000/49000 (14%)]\tLoss: 1.525426\n",
      "Train Epoch: 36 [8000/49000 (16%)]\tLoss: 1.550655\n",
      "Train Epoch: 36 [9000/49000 (18%)]\tLoss: 1.564366\n",
      "Train Epoch: 36 [10000/49000 (20%)]\tLoss: 1.565957\n",
      "Train Epoch: 36 [11000/49000 (22%)]\tLoss: 1.539186\n",
      "Train Epoch: 36 [12000/49000 (24%)]\tLoss: 1.528316\n",
      "Train Epoch: 36 [13000/49000 (27%)]\tLoss: 1.524139\n",
      "Train Epoch: 36 [14000/49000 (29%)]\tLoss: 1.535038\n",
      "Train Epoch: 36 [15000/49000 (31%)]\tLoss: 1.543187\n",
      "Train Epoch: 36 [16000/49000 (33%)]\tLoss: 1.575967\n",
      "Train Epoch: 36 [17000/49000 (35%)]\tLoss: 1.537638\n",
      "Train Epoch: 36 [18000/49000 (37%)]\tLoss: 1.551419\n",
      "Train Epoch: 36 [19000/49000 (39%)]\tLoss: 1.535720\n",
      "Train Epoch: 36 [20000/49000 (41%)]\tLoss: 1.524958\n",
      "Train Epoch: 36 [21000/49000 (43%)]\tLoss: 1.532730\n",
      "Train Epoch: 36 [22000/49000 (45%)]\tLoss: 1.558928\n",
      "Train Epoch: 36 [23000/49000 (47%)]\tLoss: 1.557864\n",
      "Train Epoch: 36 [24000/49000 (49%)]\tLoss: 1.544788\n",
      "Train Epoch: 36 [25000/49000 (51%)]\tLoss: 1.517287\n",
      "Train Epoch: 36 [26000/49000 (53%)]\tLoss: 1.514604\n",
      "Train Epoch: 36 [27000/49000 (55%)]\tLoss: 1.545706\n",
      "Train Epoch: 36 [28000/49000 (57%)]\tLoss: 1.514507\n",
      "Train Epoch: 36 [29000/49000 (59%)]\tLoss: 1.553311\n",
      "Train Epoch: 36 [30000/49000 (61%)]\tLoss: 1.563303\n",
      "Train Epoch: 36 [31000/49000 (63%)]\tLoss: 1.569277\n",
      "Train Epoch: 36 [32000/49000 (65%)]\tLoss: 1.547650\n",
      "Train Epoch: 36 [33000/49000 (67%)]\tLoss: 1.586002\n",
      "Train Epoch: 36 [34000/49000 (69%)]\tLoss: 1.523477\n",
      "Train Epoch: 36 [35000/49000 (71%)]\tLoss: 1.552393\n",
      "Train Epoch: 36 [36000/49000 (73%)]\tLoss: 1.539571\n",
      "Train Epoch: 36 [37000/49000 (76%)]\tLoss: 1.540297\n",
      "Train Epoch: 36 [38000/49000 (78%)]\tLoss: 1.580118\n",
      "Train Epoch: 36 [39000/49000 (80%)]\tLoss: 1.524685\n",
      "Train Epoch: 36 [40000/49000 (82%)]\tLoss: 1.556801\n",
      "Train Epoch: 36 [41000/49000 (84%)]\tLoss: 1.524075\n",
      "Train Epoch: 36 [42000/49000 (86%)]\tLoss: 1.549131\n",
      "Train Epoch: 36 [43000/49000 (88%)]\tLoss: 1.536116\n",
      "Train Epoch: 36 [44000/49000 (90%)]\tLoss: 1.549234\n",
      "Train Epoch: 36 [45000/49000 (92%)]\tLoss: 1.573101\n",
      "Train Epoch: 36 [46000/49000 (94%)]\tLoss: 1.527638\n",
      "Train Epoch: 36 [47000/49000 (96%)]\tLoss: 1.532784\n",
      "Train Epoch: 36 [48000/49000 (98%)]\tLoss: 1.547732\n",
      "\n",
      "Test set: Avg. loss: 1.5541, Accuracy: 19064/21000 (90.78%)\n",
      "\n",
      "Train Epoch: 37 [0/49000 (0%)]\tLoss: 1.560327\n",
      "Train Epoch: 37 [1000/49000 (2%)]\tLoss: 1.572548\n",
      "Train Epoch: 37 [2000/49000 (4%)]\tLoss: 1.552569\n",
      "Train Epoch: 37 [3000/49000 (6%)]\tLoss: 1.554609\n",
      "Train Epoch: 37 [4000/49000 (8%)]\tLoss: 1.594903\n",
      "Train Epoch: 37 [5000/49000 (10%)]\tLoss: 1.531911\n",
      "Train Epoch: 37 [6000/49000 (12%)]\tLoss: 1.546957\n",
      "Train Epoch: 37 [7000/49000 (14%)]\tLoss: 1.560533\n",
      "Train Epoch: 37 [8000/49000 (16%)]\tLoss: 1.559336\n",
      "Train Epoch: 37 [9000/49000 (18%)]\tLoss: 1.530303\n",
      "Train Epoch: 37 [10000/49000 (20%)]\tLoss: 1.525751\n",
      "Train Epoch: 37 [11000/49000 (22%)]\tLoss: 1.568690\n",
      "Train Epoch: 37 [12000/49000 (24%)]\tLoss: 1.571192\n",
      "Train Epoch: 37 [13000/49000 (27%)]\tLoss: 1.565104\n",
      "Train Epoch: 37 [14000/49000 (29%)]\tLoss: 1.600780\n",
      "Train Epoch: 37 [15000/49000 (31%)]\tLoss: 1.504300\n",
      "Train Epoch: 37 [16000/49000 (33%)]\tLoss: 1.550911\n",
      "Train Epoch: 37 [17000/49000 (35%)]\tLoss: 1.531120\n",
      "Train Epoch: 37 [18000/49000 (37%)]\tLoss: 1.568084\n",
      "Train Epoch: 37 [19000/49000 (39%)]\tLoss: 1.572268\n",
      "Train Epoch: 37 [20000/49000 (41%)]\tLoss: 1.553748\n",
      "Train Epoch: 37 [21000/49000 (43%)]\tLoss: 1.559893\n",
      "Train Epoch: 37 [22000/49000 (45%)]\tLoss: 1.511859\n",
      "Train Epoch: 37 [23000/49000 (47%)]\tLoss: 1.550260\n",
      "Train Epoch: 37 [24000/49000 (49%)]\tLoss: 1.538586\n",
      "Train Epoch: 37 [25000/49000 (51%)]\tLoss: 1.544742\n",
      "Train Epoch: 37 [26000/49000 (53%)]\tLoss: 1.525375\n",
      "Train Epoch: 37 [27000/49000 (55%)]\tLoss: 1.536088\n",
      "Train Epoch: 37 [28000/49000 (57%)]\tLoss: 1.601343\n",
      "Train Epoch: 37 [29000/49000 (59%)]\tLoss: 1.539210\n",
      "Train Epoch: 37 [30000/49000 (61%)]\tLoss: 1.533484\n",
      "Train Epoch: 37 [31000/49000 (63%)]\tLoss: 1.543516\n",
      "Train Epoch: 37 [32000/49000 (65%)]\tLoss: 1.517588\n",
      "Train Epoch: 37 [33000/49000 (67%)]\tLoss: 1.552346\n",
      "Train Epoch: 37 [34000/49000 (69%)]\tLoss: 1.538677\n",
      "Train Epoch: 37 [35000/49000 (71%)]\tLoss: 1.553418\n",
      "Train Epoch: 37 [36000/49000 (73%)]\tLoss: 1.527034\n",
      "Train Epoch: 37 [37000/49000 (76%)]\tLoss: 1.518449\n",
      "Train Epoch: 37 [38000/49000 (78%)]\tLoss: 1.521851\n",
      "Train Epoch: 37 [39000/49000 (80%)]\tLoss: 1.526981\n",
      "Train Epoch: 37 [40000/49000 (82%)]\tLoss: 1.509349\n",
      "Train Epoch: 37 [41000/49000 (84%)]\tLoss: 1.536390\n",
      "Train Epoch: 37 [42000/49000 (86%)]\tLoss: 1.563017\n",
      "Train Epoch: 37 [43000/49000 (88%)]\tLoss: 1.562220\n",
      "Train Epoch: 37 [44000/49000 (90%)]\tLoss: 1.579401\n",
      "Train Epoch: 37 [45000/49000 (92%)]\tLoss: 1.508189\n",
      "Train Epoch: 37 [46000/49000 (94%)]\tLoss: 1.532670\n",
      "Train Epoch: 37 [47000/49000 (96%)]\tLoss: 1.541003\n",
      "Train Epoch: 37 [48000/49000 (98%)]\tLoss: 1.512678\n",
      "\n",
      "Test set: Avg. loss: 1.5543, Accuracy: 19091/21000 (90.91%)\n",
      "\n",
      "Train Epoch: 38 [0/49000 (0%)]\tLoss: 1.566862\n",
      "Train Epoch: 38 [1000/49000 (2%)]\tLoss: 1.557577\n",
      "Train Epoch: 38 [2000/49000 (4%)]\tLoss: 1.570860\n",
      "Train Epoch: 38 [3000/49000 (6%)]\tLoss: 1.541758\n",
      "Train Epoch: 38 [4000/49000 (8%)]\tLoss: 1.578708\n",
      "Train Epoch: 38 [5000/49000 (10%)]\tLoss: 1.548447\n",
      "Train Epoch: 38 [6000/49000 (12%)]\tLoss: 1.539819\n",
      "Train Epoch: 38 [7000/49000 (14%)]\tLoss: 1.568548\n",
      "Train Epoch: 38 [8000/49000 (16%)]\tLoss: 1.566606\n",
      "Train Epoch: 38 [9000/49000 (18%)]\tLoss: 1.570316\n",
      "Train Epoch: 38 [10000/49000 (20%)]\tLoss: 1.547635\n",
      "Train Epoch: 38 [11000/49000 (22%)]\tLoss: 1.575547\n",
      "Train Epoch: 38 [12000/49000 (24%)]\tLoss: 1.579880\n",
      "Train Epoch: 38 [13000/49000 (27%)]\tLoss: 1.582338\n",
      "Train Epoch: 38 [14000/49000 (29%)]\tLoss: 1.539948\n",
      "Train Epoch: 38 [15000/49000 (31%)]\tLoss: 1.569978\n",
      "Train Epoch: 38 [16000/49000 (33%)]\tLoss: 1.516737\n",
      "Train Epoch: 38 [17000/49000 (35%)]\tLoss: 1.571206\n",
      "Train Epoch: 38 [18000/49000 (37%)]\tLoss: 1.545198\n",
      "Train Epoch: 38 [19000/49000 (39%)]\tLoss: 1.577547\n",
      "Train Epoch: 38 [20000/49000 (41%)]\tLoss: 1.528429\n",
      "Train Epoch: 38 [21000/49000 (43%)]\tLoss: 1.541185\n",
      "Train Epoch: 38 [22000/49000 (45%)]\tLoss: 1.575385\n",
      "Train Epoch: 38 [23000/49000 (47%)]\tLoss: 1.574534\n",
      "Train Epoch: 38 [24000/49000 (49%)]\tLoss: 1.517985\n",
      "Train Epoch: 38 [25000/49000 (51%)]\tLoss: 1.553107\n",
      "Train Epoch: 38 [26000/49000 (53%)]\tLoss: 1.548312\n",
      "Train Epoch: 38 [27000/49000 (55%)]\tLoss: 1.533244\n",
      "Train Epoch: 38 [28000/49000 (57%)]\tLoss: 1.530818\n",
      "Train Epoch: 38 [29000/49000 (59%)]\tLoss: 1.537768\n",
      "Train Epoch: 38 [30000/49000 (61%)]\tLoss: 1.556516\n",
      "Train Epoch: 38 [31000/49000 (63%)]\tLoss: 1.513415\n",
      "Train Epoch: 38 [32000/49000 (65%)]\tLoss: 1.553133\n",
      "Train Epoch: 38 [33000/49000 (67%)]\tLoss: 1.556573\n",
      "Train Epoch: 38 [34000/49000 (69%)]\tLoss: 1.530846\n",
      "Train Epoch: 38 [35000/49000 (71%)]\tLoss: 1.537483\n",
      "Train Epoch: 38 [36000/49000 (73%)]\tLoss: 1.549753\n",
      "Train Epoch: 38 [37000/49000 (76%)]\tLoss: 1.525664\n",
      "Train Epoch: 38 [38000/49000 (78%)]\tLoss: 1.525070\n",
      "Train Epoch: 38 [39000/49000 (80%)]\tLoss: 1.550568\n",
      "Train Epoch: 38 [40000/49000 (82%)]\tLoss: 1.533762\n",
      "Train Epoch: 38 [41000/49000 (84%)]\tLoss: 1.563151\n",
      "Train Epoch: 38 [42000/49000 (86%)]\tLoss: 1.512325\n",
      "Train Epoch: 38 [43000/49000 (88%)]\tLoss: 1.555899\n",
      "Train Epoch: 38 [44000/49000 (90%)]\tLoss: 1.545950\n",
      "Train Epoch: 38 [45000/49000 (92%)]\tLoss: 1.544359\n",
      "Train Epoch: 38 [46000/49000 (94%)]\tLoss: 1.568375\n",
      "Train Epoch: 38 [47000/49000 (96%)]\tLoss: 1.553367\n",
      "Train Epoch: 38 [48000/49000 (98%)]\tLoss: 1.518032\n",
      "\n",
      "Test set: Avg. loss: 1.5545, Accuracy: 19088/21000 (90.90%)\n",
      "\n",
      "Train Epoch: 39 [0/49000 (0%)]\tLoss: 1.504002\n",
      "Train Epoch: 39 [1000/49000 (2%)]\tLoss: 1.559163\n",
      "Train Epoch: 39 [2000/49000 (4%)]\tLoss: 1.527637\n",
      "Train Epoch: 39 [3000/49000 (6%)]\tLoss: 1.559946\n",
      "Train Epoch: 39 [4000/49000 (8%)]\tLoss: 1.534593\n",
      "Train Epoch: 39 [5000/49000 (10%)]\tLoss: 1.530766\n",
      "Train Epoch: 39 [6000/49000 (12%)]\tLoss: 1.539379\n",
      "Train Epoch: 39 [7000/49000 (14%)]\tLoss: 1.524721\n",
      "Train Epoch: 39 [8000/49000 (16%)]\tLoss: 1.544454\n",
      "Train Epoch: 39 [9000/49000 (18%)]\tLoss: 1.545278\n",
      "Train Epoch: 39 [10000/49000 (20%)]\tLoss: 1.557562\n",
      "Train Epoch: 39 [11000/49000 (22%)]\tLoss: 1.560338\n",
      "Train Epoch: 39 [12000/49000 (24%)]\tLoss: 1.545648\n",
      "Train Epoch: 39 [13000/49000 (27%)]\tLoss: 1.558563\n",
      "Train Epoch: 39 [14000/49000 (29%)]\tLoss: 1.552256\n",
      "Train Epoch: 39 [15000/49000 (31%)]\tLoss: 1.531840\n",
      "Train Epoch: 39 [16000/49000 (33%)]\tLoss: 1.596675\n",
      "Train Epoch: 39 [17000/49000 (35%)]\tLoss: 1.568873\n",
      "Train Epoch: 39 [18000/49000 (37%)]\tLoss: 1.539236\n",
      "Train Epoch: 39 [19000/49000 (39%)]\tLoss: 1.527261\n",
      "Train Epoch: 39 [20000/49000 (41%)]\tLoss: 1.558037\n",
      "Train Epoch: 39 [21000/49000 (43%)]\tLoss: 1.548511\n",
      "Train Epoch: 39 [22000/49000 (45%)]\tLoss: 1.548647\n",
      "Train Epoch: 39 [23000/49000 (47%)]\tLoss: 1.564410\n",
      "Train Epoch: 39 [24000/49000 (49%)]\tLoss: 1.575802\n",
      "Train Epoch: 39 [25000/49000 (51%)]\tLoss: 1.548842\n",
      "Train Epoch: 39 [26000/49000 (53%)]\tLoss: 1.519993\n",
      "Train Epoch: 39 [27000/49000 (55%)]\tLoss: 1.552430\n",
      "Train Epoch: 39 [28000/49000 (57%)]\tLoss: 1.542065\n",
      "Train Epoch: 39 [29000/49000 (59%)]\tLoss: 1.551998\n",
      "Train Epoch: 39 [30000/49000 (61%)]\tLoss: 1.547336\n",
      "Train Epoch: 39 [31000/49000 (63%)]\tLoss: 1.562473\n",
      "Train Epoch: 39 [32000/49000 (65%)]\tLoss: 1.519707\n",
      "Train Epoch: 39 [33000/49000 (67%)]\tLoss: 1.485323\n",
      "Train Epoch: 39 [34000/49000 (69%)]\tLoss: 1.587156\n",
      "Train Epoch: 39 [35000/49000 (71%)]\tLoss: 1.542493\n",
      "Train Epoch: 39 [36000/49000 (73%)]\tLoss: 1.527516\n",
      "Train Epoch: 39 [37000/49000 (76%)]\tLoss: 1.550365\n",
      "Train Epoch: 39 [38000/49000 (78%)]\tLoss: 1.550493\n",
      "Train Epoch: 39 [39000/49000 (80%)]\tLoss: 1.540986\n",
      "Train Epoch: 39 [40000/49000 (82%)]\tLoss: 1.530327\n",
      "Train Epoch: 39 [41000/49000 (84%)]\tLoss: 1.540846\n",
      "Train Epoch: 39 [42000/49000 (86%)]\tLoss: 1.556141\n",
      "Train Epoch: 39 [43000/49000 (88%)]\tLoss: 1.521425\n",
      "Train Epoch: 39 [44000/49000 (90%)]\tLoss: 1.548820\n",
      "Train Epoch: 39 [45000/49000 (92%)]\tLoss: 1.518899\n",
      "Train Epoch: 39 [46000/49000 (94%)]\tLoss: 1.534084\n",
      "Train Epoch: 39 [47000/49000 (96%)]\tLoss: 1.537205\n",
      "Train Epoch: 39 [48000/49000 (98%)]\tLoss: 1.571049\n",
      "\n",
      "Test set: Avg. loss: 1.5531, Accuracy: 19109/21000 (91.00%)\n",
      "\n",
      "Train Epoch: 40 [0/49000 (0%)]\tLoss: 1.545506\n",
      "Train Epoch: 40 [1000/49000 (2%)]\tLoss: 1.537807\n",
      "Train Epoch: 40 [2000/49000 (4%)]\tLoss: 1.510557\n",
      "Train Epoch: 40 [3000/49000 (6%)]\tLoss: 1.522845\n",
      "Train Epoch: 40 [4000/49000 (8%)]\tLoss: 1.521435\n",
      "Train Epoch: 40 [5000/49000 (10%)]\tLoss: 1.504969\n",
      "Train Epoch: 40 [6000/49000 (12%)]\tLoss: 1.548017\n",
      "Train Epoch: 40 [7000/49000 (14%)]\tLoss: 1.567979\n",
      "Train Epoch: 40 [8000/49000 (16%)]\tLoss: 1.531289\n",
      "Train Epoch: 40 [9000/49000 (18%)]\tLoss: 1.597073\n",
      "Train Epoch: 40 [10000/49000 (20%)]\tLoss: 1.565491\n",
      "Train Epoch: 40 [11000/49000 (22%)]\tLoss: 1.568238\n",
      "Train Epoch: 40 [12000/49000 (24%)]\tLoss: 1.560751\n",
      "Train Epoch: 40 [13000/49000 (27%)]\tLoss: 1.553528\n",
      "Train Epoch: 40 [14000/49000 (29%)]\tLoss: 1.522941\n",
      "Train Epoch: 40 [15000/49000 (31%)]\tLoss: 1.537750\n",
      "Train Epoch: 40 [16000/49000 (33%)]\tLoss: 1.544021\n",
      "Train Epoch: 40 [17000/49000 (35%)]\tLoss: 1.554764\n",
      "Train Epoch: 40 [18000/49000 (37%)]\tLoss: 1.538956\n",
      "Train Epoch: 40 [19000/49000 (39%)]\tLoss: 1.570789\n",
      "Train Epoch: 40 [20000/49000 (41%)]\tLoss: 1.529369\n",
      "Train Epoch: 40 [21000/49000 (43%)]\tLoss: 1.536340\n",
      "Train Epoch: 40 [22000/49000 (45%)]\tLoss: 1.535871\n",
      "Train Epoch: 40 [23000/49000 (47%)]\tLoss: 1.540812\n",
      "Train Epoch: 40 [24000/49000 (49%)]\tLoss: 1.562293\n",
      "Train Epoch: 40 [25000/49000 (51%)]\tLoss: 1.551349\n",
      "Train Epoch: 40 [26000/49000 (53%)]\tLoss: 1.574798\n",
      "Train Epoch: 40 [27000/49000 (55%)]\tLoss: 1.521434\n",
      "Train Epoch: 40 [28000/49000 (57%)]\tLoss: 1.562929\n",
      "Train Epoch: 40 [29000/49000 (59%)]\tLoss: 1.569648\n",
      "Train Epoch: 40 [30000/49000 (61%)]\tLoss: 1.526370\n",
      "Train Epoch: 40 [31000/49000 (63%)]\tLoss: 1.541968\n",
      "Train Epoch: 40 [32000/49000 (65%)]\tLoss: 1.565610\n",
      "Train Epoch: 40 [33000/49000 (67%)]\tLoss: 1.559945\n",
      "Train Epoch: 40 [34000/49000 (69%)]\tLoss: 1.560395\n",
      "Train Epoch: 40 [35000/49000 (71%)]\tLoss: 1.593543\n",
      "Train Epoch: 40 [36000/49000 (73%)]\tLoss: 1.578609\n",
      "Train Epoch: 40 [37000/49000 (76%)]\tLoss: 1.571203\n",
      "Train Epoch: 40 [38000/49000 (78%)]\tLoss: 1.575941\n",
      "Train Epoch: 40 [39000/49000 (80%)]\tLoss: 1.541163\n",
      "Train Epoch: 40 [40000/49000 (82%)]\tLoss: 1.553528\n",
      "Train Epoch: 40 [41000/49000 (84%)]\tLoss: 1.533239\n",
      "Train Epoch: 40 [42000/49000 (86%)]\tLoss: 1.524893\n",
      "Train Epoch: 40 [43000/49000 (88%)]\tLoss: 1.505653\n",
      "Train Epoch: 40 [44000/49000 (90%)]\tLoss: 1.541993\n",
      "Train Epoch: 40 [45000/49000 (92%)]\tLoss: 1.541301\n",
      "Train Epoch: 40 [46000/49000 (94%)]\tLoss: 1.583845\n",
      "Train Epoch: 40 [47000/49000 (96%)]\tLoss: 1.516083\n",
      "Train Epoch: 40 [48000/49000 (98%)]\tLoss: 1.533337\n",
      "\n",
      "Test set: Avg. loss: 1.5528, Accuracy: 19124/21000 (91.07%)\n",
      "\n",
      "Train Epoch: 41 [0/49000 (0%)]\tLoss: 1.527165\n",
      "Train Epoch: 41 [1000/49000 (2%)]\tLoss: 1.554255\n",
      "Train Epoch: 41 [2000/49000 (4%)]\tLoss: 1.541122\n",
      "Train Epoch: 41 [3000/49000 (6%)]\tLoss: 1.531491\n",
      "Train Epoch: 41 [4000/49000 (8%)]\tLoss: 1.558303\n",
      "Train Epoch: 41 [5000/49000 (10%)]\tLoss: 1.579318\n",
      "Train Epoch: 41 [6000/49000 (12%)]\tLoss: 1.507403\n",
      "Train Epoch: 41 [7000/49000 (14%)]\tLoss: 1.564964\n",
      "Train Epoch: 41 [8000/49000 (16%)]\tLoss: 1.518623\n",
      "Train Epoch: 41 [9000/49000 (18%)]\tLoss: 1.527369\n",
      "Train Epoch: 41 [10000/49000 (20%)]\tLoss: 1.547196\n",
      "Train Epoch: 41 [11000/49000 (22%)]\tLoss: 1.516209\n",
      "Train Epoch: 41 [12000/49000 (24%)]\tLoss: 1.527843\n",
      "Train Epoch: 41 [13000/49000 (27%)]\tLoss: 1.545041\n",
      "Train Epoch: 41 [14000/49000 (29%)]\tLoss: 1.544713\n",
      "Train Epoch: 41 [15000/49000 (31%)]\tLoss: 1.541173\n",
      "Train Epoch: 41 [16000/49000 (33%)]\tLoss: 1.555183\n",
      "Train Epoch: 41 [17000/49000 (35%)]\tLoss: 1.577616\n",
      "Train Epoch: 41 [18000/49000 (37%)]\tLoss: 1.543325\n",
      "Train Epoch: 41 [19000/49000 (39%)]\tLoss: 1.596027\n",
      "Train Epoch: 41 [20000/49000 (41%)]\tLoss: 1.520822\n",
      "Train Epoch: 41 [21000/49000 (43%)]\tLoss: 1.527220\n",
      "Train Epoch: 41 [22000/49000 (45%)]\tLoss: 1.528390\n",
      "Train Epoch: 41 [23000/49000 (47%)]\tLoss: 1.510388\n",
      "Train Epoch: 41 [24000/49000 (49%)]\tLoss: 1.545313\n",
      "Train Epoch: 41 [25000/49000 (51%)]\tLoss: 1.525986\n",
      "Train Epoch: 41 [26000/49000 (53%)]\tLoss: 1.512515\n",
      "Train Epoch: 41 [27000/49000 (55%)]\tLoss: 1.537468\n",
      "Train Epoch: 41 [28000/49000 (57%)]\tLoss: 1.564048\n",
      "Train Epoch: 41 [29000/49000 (59%)]\tLoss: 1.537558\n",
      "Train Epoch: 41 [30000/49000 (61%)]\tLoss: 1.544376\n",
      "Train Epoch: 41 [31000/49000 (63%)]\tLoss: 1.538884\n",
      "Train Epoch: 41 [32000/49000 (65%)]\tLoss: 1.521279\n",
      "Train Epoch: 41 [33000/49000 (67%)]\tLoss: 1.541217\n",
      "Train Epoch: 41 [34000/49000 (69%)]\tLoss: 1.540966\n",
      "Train Epoch: 41 [35000/49000 (71%)]\tLoss: 1.548936\n",
      "Train Epoch: 41 [36000/49000 (73%)]\tLoss: 1.549412\n",
      "Train Epoch: 41 [37000/49000 (76%)]\tLoss: 1.536021\n",
      "Train Epoch: 41 [38000/49000 (78%)]\tLoss: 1.549907\n",
      "Train Epoch: 41 [39000/49000 (80%)]\tLoss: 1.575510\n",
      "Train Epoch: 41 [40000/49000 (82%)]\tLoss: 1.568390\n",
      "Train Epoch: 41 [41000/49000 (84%)]\tLoss: 1.593083\n",
      "Train Epoch: 41 [42000/49000 (86%)]\tLoss: 1.528863\n",
      "Train Epoch: 41 [43000/49000 (88%)]\tLoss: 1.533433\n",
      "Train Epoch: 41 [44000/49000 (90%)]\tLoss: 1.540890\n",
      "Train Epoch: 41 [45000/49000 (92%)]\tLoss: 1.527374\n",
      "Train Epoch: 41 [46000/49000 (94%)]\tLoss: 1.527787\n",
      "Train Epoch: 41 [47000/49000 (96%)]\tLoss: 1.522512\n",
      "Train Epoch: 41 [48000/49000 (98%)]\tLoss: 1.535988\n",
      "\n",
      "Test set: Avg. loss: 1.5532, Accuracy: 19065/21000 (90.79%)\n",
      "\n",
      "Train Epoch: 42 [0/49000 (0%)]\tLoss: 1.531572\n",
      "Train Epoch: 42 [1000/49000 (2%)]\tLoss: 1.554681\n",
      "Train Epoch: 42 [2000/49000 (4%)]\tLoss: 1.537847\n",
      "Train Epoch: 42 [3000/49000 (6%)]\tLoss: 1.576224\n",
      "Train Epoch: 42 [4000/49000 (8%)]\tLoss: 1.548877\n",
      "Train Epoch: 42 [5000/49000 (10%)]\tLoss: 1.517520\n",
      "Train Epoch: 42 [6000/49000 (12%)]\tLoss: 1.567130\n",
      "Train Epoch: 42 [7000/49000 (14%)]\tLoss: 1.542514\n",
      "Train Epoch: 42 [8000/49000 (16%)]\tLoss: 1.557388\n",
      "Train Epoch: 42 [9000/49000 (18%)]\tLoss: 1.542404\n",
      "Train Epoch: 42 [10000/49000 (20%)]\tLoss: 1.548000\n",
      "Train Epoch: 42 [11000/49000 (22%)]\tLoss: 1.559747\n",
      "Train Epoch: 42 [12000/49000 (24%)]\tLoss: 1.543833\n",
      "Train Epoch: 42 [13000/49000 (27%)]\tLoss: 1.550727\n",
      "Train Epoch: 42 [14000/49000 (29%)]\tLoss: 1.528835\n",
      "Train Epoch: 42 [15000/49000 (31%)]\tLoss: 1.581366\n",
      "Train Epoch: 42 [16000/49000 (33%)]\tLoss: 1.572577\n",
      "Train Epoch: 42 [17000/49000 (35%)]\tLoss: 1.554501\n",
      "Train Epoch: 42 [18000/49000 (37%)]\tLoss: 1.518706\n",
      "Train Epoch: 42 [19000/49000 (39%)]\tLoss: 1.516001\n",
      "Train Epoch: 42 [20000/49000 (41%)]\tLoss: 1.547337\n",
      "Train Epoch: 42 [21000/49000 (43%)]\tLoss: 1.550025\n",
      "Train Epoch: 42 [22000/49000 (45%)]\tLoss: 1.553551\n",
      "Train Epoch: 42 [23000/49000 (47%)]\tLoss: 1.537719\n",
      "Train Epoch: 42 [24000/49000 (49%)]\tLoss: 1.585647\n",
      "Train Epoch: 42 [25000/49000 (51%)]\tLoss: 1.518160\n",
      "Train Epoch: 42 [26000/49000 (53%)]\tLoss: 1.520937\n",
      "Train Epoch: 42 [27000/49000 (55%)]\tLoss: 1.528219\n",
      "Train Epoch: 42 [28000/49000 (57%)]\tLoss: 1.547226\n",
      "Train Epoch: 42 [29000/49000 (59%)]\tLoss: 1.570130\n",
      "Train Epoch: 42 [30000/49000 (61%)]\tLoss: 1.584062\n",
      "Train Epoch: 42 [31000/49000 (63%)]\tLoss: 1.535112\n",
      "Train Epoch: 42 [32000/49000 (65%)]\tLoss: 1.548788\n",
      "Train Epoch: 42 [33000/49000 (67%)]\tLoss: 1.546693\n",
      "Train Epoch: 42 [34000/49000 (69%)]\tLoss: 1.567234\n",
      "Train Epoch: 42 [35000/49000 (71%)]\tLoss: 1.518961\n",
      "Train Epoch: 42 [36000/49000 (73%)]\tLoss: 1.552859\n",
      "Train Epoch: 42 [37000/49000 (76%)]\tLoss: 1.521338\n",
      "Train Epoch: 42 [38000/49000 (78%)]\tLoss: 1.517361\n",
      "Train Epoch: 42 [39000/49000 (80%)]\tLoss: 1.554320\n",
      "Train Epoch: 42 [40000/49000 (82%)]\tLoss: 1.523670\n",
      "Train Epoch: 42 [41000/49000 (84%)]\tLoss: 1.540110\n",
      "Train Epoch: 42 [42000/49000 (86%)]\tLoss: 1.540456\n",
      "Train Epoch: 42 [43000/49000 (88%)]\tLoss: 1.561753\n",
      "Train Epoch: 42 [44000/49000 (90%)]\tLoss: 1.543959\n",
      "Train Epoch: 42 [45000/49000 (92%)]\tLoss: 1.585869\n",
      "Train Epoch: 42 [46000/49000 (94%)]\tLoss: 1.543790\n",
      "Train Epoch: 42 [47000/49000 (96%)]\tLoss: 1.526313\n",
      "Train Epoch: 42 [48000/49000 (98%)]\tLoss: 1.591431\n",
      "\n",
      "Test set: Avg. loss: 1.5534, Accuracy: 19038/21000 (90.66%)\n",
      "\n",
      "Train Epoch: 43 [0/49000 (0%)]\tLoss: 1.516917\n",
      "Train Epoch: 43 [1000/49000 (2%)]\tLoss: 1.577980\n",
      "Train Epoch: 43 [2000/49000 (4%)]\tLoss: 1.547273\n",
      "Train Epoch: 43 [3000/49000 (6%)]\tLoss: 1.564655\n",
      "Train Epoch: 43 [4000/49000 (8%)]\tLoss: 1.564155\n",
      "Train Epoch: 43 [5000/49000 (10%)]\tLoss: 1.522869\n",
      "Train Epoch: 43 [6000/49000 (12%)]\tLoss: 1.553017\n",
      "Train Epoch: 43 [7000/49000 (14%)]\tLoss: 1.562295\n",
      "Train Epoch: 43 [8000/49000 (16%)]\tLoss: 1.524305\n",
      "Train Epoch: 43 [9000/49000 (18%)]\tLoss: 1.555119\n",
      "Train Epoch: 43 [10000/49000 (20%)]\tLoss: 1.570994\n",
      "Train Epoch: 43 [11000/49000 (22%)]\tLoss: 1.567867\n",
      "Train Epoch: 43 [12000/49000 (24%)]\tLoss: 1.545504\n",
      "Train Epoch: 43 [13000/49000 (27%)]\tLoss: 1.537383\n",
      "Train Epoch: 43 [14000/49000 (29%)]\tLoss: 1.512130\n",
      "Train Epoch: 43 [15000/49000 (31%)]\tLoss: 1.542640\n",
      "Train Epoch: 43 [16000/49000 (33%)]\tLoss: 1.521033\n",
      "Train Epoch: 43 [17000/49000 (35%)]\tLoss: 1.529336\n",
      "Train Epoch: 43 [18000/49000 (37%)]\tLoss: 1.537235\n",
      "Train Epoch: 43 [19000/49000 (39%)]\tLoss: 1.554315\n",
      "Train Epoch: 43 [20000/49000 (41%)]\tLoss: 1.542339\n",
      "Train Epoch: 43 [21000/49000 (43%)]\tLoss: 1.541738\n",
      "Train Epoch: 43 [22000/49000 (45%)]\tLoss: 1.554696\n",
      "Train Epoch: 43 [23000/49000 (47%)]\tLoss: 1.532523\n",
      "Train Epoch: 43 [24000/49000 (49%)]\tLoss: 1.545133\n",
      "Train Epoch: 43 [25000/49000 (51%)]\tLoss: 1.517377\n",
      "Train Epoch: 43 [26000/49000 (53%)]\tLoss: 1.519092\n",
      "Train Epoch: 43 [27000/49000 (55%)]\tLoss: 1.542278\n",
      "Train Epoch: 43 [28000/49000 (57%)]\tLoss: 1.555212\n",
      "Train Epoch: 43 [29000/49000 (59%)]\tLoss: 1.513125\n",
      "Train Epoch: 43 [30000/49000 (61%)]\tLoss: 1.522537\n",
      "Train Epoch: 43 [31000/49000 (63%)]\tLoss: 1.558965\n",
      "Train Epoch: 43 [32000/49000 (65%)]\tLoss: 1.540680\n",
      "Train Epoch: 43 [33000/49000 (67%)]\tLoss: 1.532861\n",
      "Train Epoch: 43 [34000/49000 (69%)]\tLoss: 1.591827\n",
      "Train Epoch: 43 [35000/49000 (71%)]\tLoss: 1.524482\n",
      "Train Epoch: 43 [36000/49000 (73%)]\tLoss: 1.540771\n",
      "Train Epoch: 43 [37000/49000 (76%)]\tLoss: 1.534152\n",
      "Train Epoch: 43 [38000/49000 (78%)]\tLoss: 1.584755\n",
      "Train Epoch: 43 [39000/49000 (80%)]\tLoss: 1.552859\n",
      "Train Epoch: 43 [40000/49000 (82%)]\tLoss: 1.565825\n",
      "Train Epoch: 43 [41000/49000 (84%)]\tLoss: 1.523198\n",
      "Train Epoch: 43 [42000/49000 (86%)]\tLoss: 1.521514\n",
      "Train Epoch: 43 [43000/49000 (88%)]\tLoss: 1.534091\n",
      "Train Epoch: 43 [44000/49000 (90%)]\tLoss: 1.554637\n",
      "Train Epoch: 43 [45000/49000 (92%)]\tLoss: 1.559059\n",
      "Train Epoch: 43 [46000/49000 (94%)]\tLoss: 1.524933\n",
      "Train Epoch: 43 [47000/49000 (96%)]\tLoss: 1.563872\n",
      "Train Epoch: 43 [48000/49000 (98%)]\tLoss: 1.563475\n",
      "\n",
      "Test set: Avg. loss: 1.5531, Accuracy: 19082/21000 (90.87%)\n",
      "\n",
      "Train Epoch: 44 [0/49000 (0%)]\tLoss: 1.539181\n",
      "Train Epoch: 44 [1000/49000 (2%)]\tLoss: 1.518062\n",
      "Train Epoch: 44 [2000/49000 (4%)]\tLoss: 1.600603\n",
      "Train Epoch: 44 [3000/49000 (6%)]\tLoss: 1.563377\n",
      "Train Epoch: 44 [4000/49000 (8%)]\tLoss: 1.578123\n",
      "Train Epoch: 44 [5000/49000 (10%)]\tLoss: 1.576211\n",
      "Train Epoch: 44 [6000/49000 (12%)]\tLoss: 1.529011\n",
      "Train Epoch: 44 [7000/49000 (14%)]\tLoss: 1.571853\n",
      "Train Epoch: 44 [8000/49000 (16%)]\tLoss: 1.534571\n",
      "Train Epoch: 44 [9000/49000 (18%)]\tLoss: 1.511053\n",
      "Train Epoch: 44 [10000/49000 (20%)]\tLoss: 1.573080\n",
      "Train Epoch: 44 [11000/49000 (22%)]\tLoss: 1.544930\n",
      "Train Epoch: 44 [12000/49000 (24%)]\tLoss: 1.522506\n",
      "Train Epoch: 44 [13000/49000 (27%)]\tLoss: 1.521665\n",
      "Train Epoch: 44 [14000/49000 (29%)]\tLoss: 1.596884\n",
      "Train Epoch: 44 [15000/49000 (31%)]\tLoss: 1.594222\n",
      "Train Epoch: 44 [16000/49000 (33%)]\tLoss: 1.507828\n",
      "Train Epoch: 44 [17000/49000 (35%)]\tLoss: 1.565173\n",
      "Train Epoch: 44 [18000/49000 (37%)]\tLoss: 1.530878\n",
      "Train Epoch: 44 [19000/49000 (39%)]\tLoss: 1.530142\n",
      "Train Epoch: 44 [20000/49000 (41%)]\tLoss: 1.528579\n",
      "Train Epoch: 44 [21000/49000 (43%)]\tLoss: 1.536945\n",
      "Train Epoch: 44 [22000/49000 (45%)]\tLoss: 1.524259\n",
      "Train Epoch: 44 [23000/49000 (47%)]\tLoss: 1.544173\n",
      "Train Epoch: 44 [24000/49000 (49%)]\tLoss: 1.496847\n",
      "Train Epoch: 44 [25000/49000 (51%)]\tLoss: 1.565352\n",
      "Train Epoch: 44 [26000/49000 (53%)]\tLoss: 1.595490\n",
      "Train Epoch: 44 [27000/49000 (55%)]\tLoss: 1.525092\n",
      "Train Epoch: 44 [28000/49000 (57%)]\tLoss: 1.569303\n",
      "Train Epoch: 44 [29000/49000 (59%)]\tLoss: 1.527934\n",
      "Train Epoch: 44 [30000/49000 (61%)]\tLoss: 1.531804\n",
      "Train Epoch: 44 [31000/49000 (63%)]\tLoss: 1.535581\n",
      "Train Epoch: 44 [32000/49000 (65%)]\tLoss: 1.515966\n",
      "Train Epoch: 44 [33000/49000 (67%)]\tLoss: 1.523669\n",
      "Train Epoch: 44 [34000/49000 (69%)]\tLoss: 1.549738\n",
      "Train Epoch: 44 [35000/49000 (71%)]\tLoss: 1.528232\n",
      "Train Epoch: 44 [36000/49000 (73%)]\tLoss: 1.543076\n",
      "Train Epoch: 44 [37000/49000 (76%)]\tLoss: 1.546824\n",
      "Train Epoch: 44 [38000/49000 (78%)]\tLoss: 1.592106\n",
      "Train Epoch: 44 [39000/49000 (80%)]\tLoss: 1.517302\n",
      "Train Epoch: 44 [40000/49000 (82%)]\tLoss: 1.537752\n",
      "Train Epoch: 44 [41000/49000 (84%)]\tLoss: 1.544866\n",
      "Train Epoch: 44 [42000/49000 (86%)]\tLoss: 1.564671\n",
      "Train Epoch: 44 [43000/49000 (88%)]\tLoss: 1.525050\n",
      "Train Epoch: 44 [44000/49000 (90%)]\tLoss: 1.585652\n",
      "Train Epoch: 44 [45000/49000 (92%)]\tLoss: 1.521788\n",
      "Train Epoch: 44 [46000/49000 (94%)]\tLoss: 1.563605\n",
      "Train Epoch: 44 [47000/49000 (96%)]\tLoss: 1.575642\n",
      "Train Epoch: 44 [48000/49000 (98%)]\tLoss: 1.548714\n",
      "\n",
      "Test set: Avg. loss: 1.5527, Accuracy: 19106/21000 (90.98%)\n",
      "\n",
      "Train Epoch: 45 [0/49000 (0%)]\tLoss: 1.525547\n",
      "Train Epoch: 45 [1000/49000 (2%)]\tLoss: 1.556783\n",
      "Train Epoch: 45 [2000/49000 (4%)]\tLoss: 1.545155\n",
      "Train Epoch: 45 [3000/49000 (6%)]\tLoss: 1.553455\n",
      "Train Epoch: 45 [4000/49000 (8%)]\tLoss: 1.535519\n",
      "Train Epoch: 45 [5000/49000 (10%)]\tLoss: 1.530804\n",
      "Train Epoch: 45 [6000/49000 (12%)]\tLoss: 1.511286\n",
      "Train Epoch: 45 [7000/49000 (14%)]\tLoss: 1.523294\n",
      "Train Epoch: 45 [8000/49000 (16%)]\tLoss: 1.538913\n",
      "Train Epoch: 45 [9000/49000 (18%)]\tLoss: 1.542198\n",
      "Train Epoch: 45 [10000/49000 (20%)]\tLoss: 1.543761\n",
      "Train Epoch: 45 [11000/49000 (22%)]\tLoss: 1.525430\n",
      "Train Epoch: 45 [12000/49000 (24%)]\tLoss: 1.547256\n",
      "Train Epoch: 45 [13000/49000 (27%)]\tLoss: 1.539214\n",
      "Train Epoch: 45 [14000/49000 (29%)]\tLoss: 1.525757\n",
      "Train Epoch: 45 [15000/49000 (31%)]\tLoss: 1.550966\n",
      "Train Epoch: 45 [16000/49000 (33%)]\tLoss: 1.523520\n",
      "Train Epoch: 45 [17000/49000 (35%)]\tLoss: 1.522232\n",
      "Train Epoch: 45 [18000/49000 (37%)]\tLoss: 1.507356\n",
      "Train Epoch: 45 [19000/49000 (39%)]\tLoss: 1.540722\n",
      "Train Epoch: 45 [20000/49000 (41%)]\tLoss: 1.532966\n",
      "Train Epoch: 45 [21000/49000 (43%)]\tLoss: 1.531599\n",
      "Train Epoch: 45 [22000/49000 (45%)]\tLoss: 1.561458\n",
      "Train Epoch: 45 [23000/49000 (47%)]\tLoss: 1.530418\n",
      "Train Epoch: 45 [24000/49000 (49%)]\tLoss: 1.573784\n",
      "Train Epoch: 45 [25000/49000 (51%)]\tLoss: 1.597408\n",
      "Train Epoch: 45 [26000/49000 (53%)]\tLoss: 1.565143\n",
      "Train Epoch: 45 [27000/49000 (55%)]\tLoss: 1.547530\n",
      "Train Epoch: 45 [28000/49000 (57%)]\tLoss: 1.515445\n",
      "Train Epoch: 45 [29000/49000 (59%)]\tLoss: 1.581426\n",
      "Train Epoch: 45 [30000/49000 (61%)]\tLoss: 1.552166\n",
      "Train Epoch: 45 [31000/49000 (63%)]\tLoss: 1.538749\n",
      "Train Epoch: 45 [32000/49000 (65%)]\tLoss: 1.549417\n",
      "Train Epoch: 45 [33000/49000 (67%)]\tLoss: 1.576856\n",
      "Train Epoch: 45 [34000/49000 (69%)]\tLoss: 1.560688\n",
      "Train Epoch: 45 [35000/49000 (71%)]\tLoss: 1.551736\n",
      "Train Epoch: 45 [36000/49000 (73%)]\tLoss: 1.576915\n",
      "Train Epoch: 45 [37000/49000 (76%)]\tLoss: 1.517629\n",
      "Train Epoch: 45 [38000/49000 (78%)]\tLoss: 1.531444\n",
      "Train Epoch: 45 [39000/49000 (80%)]\tLoss: 1.520380\n",
      "Train Epoch: 45 [40000/49000 (82%)]\tLoss: 1.544832\n",
      "Train Epoch: 45 [41000/49000 (84%)]\tLoss: 1.605524\n",
      "Train Epoch: 45 [42000/49000 (86%)]\tLoss: 1.587598\n",
      "Train Epoch: 45 [43000/49000 (88%)]\tLoss: 1.578542\n",
      "Train Epoch: 45 [44000/49000 (90%)]\tLoss: 1.582671\n",
      "Train Epoch: 45 [45000/49000 (92%)]\tLoss: 1.560604\n",
      "Train Epoch: 45 [46000/49000 (94%)]\tLoss: 1.503740\n",
      "Train Epoch: 45 [47000/49000 (96%)]\tLoss: 1.528733\n",
      "Train Epoch: 45 [48000/49000 (98%)]\tLoss: 1.532652\n",
      "\n",
      "Test set: Avg. loss: 1.5529, Accuracy: 19067/21000 (90.80%)\n",
      "\n",
      "Train Epoch: 46 [0/49000 (0%)]\tLoss: 1.552110\n",
      "Train Epoch: 46 [1000/49000 (2%)]\tLoss: 1.570062\n",
      "Train Epoch: 46 [2000/49000 (4%)]\tLoss: 1.521509\n",
      "Train Epoch: 46 [3000/49000 (6%)]\tLoss: 1.535211\n",
      "Train Epoch: 46 [4000/49000 (8%)]\tLoss: 1.543471\n",
      "Train Epoch: 46 [5000/49000 (10%)]\tLoss: 1.537049\n",
      "Train Epoch: 46 [6000/49000 (12%)]\tLoss: 1.525672\n",
      "Train Epoch: 46 [7000/49000 (14%)]\tLoss: 1.529333\n",
      "Train Epoch: 46 [8000/49000 (16%)]\tLoss: 1.523375\n",
      "Train Epoch: 46 [9000/49000 (18%)]\tLoss: 1.559785\n",
      "Train Epoch: 46 [10000/49000 (20%)]\tLoss: 1.519864\n",
      "Train Epoch: 46 [11000/49000 (22%)]\tLoss: 1.540071\n",
      "Train Epoch: 46 [12000/49000 (24%)]\tLoss: 1.566668\n",
      "Train Epoch: 46 [13000/49000 (27%)]\tLoss: 1.554644\n",
      "Train Epoch: 46 [14000/49000 (29%)]\tLoss: 1.545052\n",
      "Train Epoch: 46 [15000/49000 (31%)]\tLoss: 1.568619\n",
      "Train Epoch: 46 [16000/49000 (33%)]\tLoss: 1.542925\n",
      "Train Epoch: 46 [17000/49000 (35%)]\tLoss: 1.551689\n",
      "Train Epoch: 46 [18000/49000 (37%)]\tLoss: 1.569502\n",
      "Train Epoch: 46 [19000/49000 (39%)]\tLoss: 1.543047\n",
      "Train Epoch: 46 [20000/49000 (41%)]\tLoss: 1.515771\n",
      "Train Epoch: 46 [21000/49000 (43%)]\tLoss: 1.532880\n",
      "Train Epoch: 46 [22000/49000 (45%)]\tLoss: 1.547645\n",
      "Train Epoch: 46 [23000/49000 (47%)]\tLoss: 1.563567\n",
      "Train Epoch: 46 [24000/49000 (49%)]\tLoss: 1.523730\n",
      "Train Epoch: 46 [25000/49000 (51%)]\tLoss: 1.591107\n",
      "Train Epoch: 46 [26000/49000 (53%)]\tLoss: 1.527757\n",
      "Train Epoch: 46 [27000/49000 (55%)]\tLoss: 1.517234\n",
      "Train Epoch: 46 [28000/49000 (57%)]\tLoss: 1.563946\n",
      "Train Epoch: 46 [29000/49000 (59%)]\tLoss: 1.518075\n",
      "Train Epoch: 46 [30000/49000 (61%)]\tLoss: 1.587833\n",
      "Train Epoch: 46 [31000/49000 (63%)]\tLoss: 1.546650\n",
      "Train Epoch: 46 [32000/49000 (65%)]\tLoss: 1.528584\n",
      "Train Epoch: 46 [33000/49000 (67%)]\tLoss: 1.517300\n",
      "Train Epoch: 46 [34000/49000 (69%)]\tLoss: 1.504979\n",
      "Train Epoch: 46 [35000/49000 (71%)]\tLoss: 1.562923\n",
      "Train Epoch: 46 [36000/49000 (73%)]\tLoss: 1.540467\n",
      "Train Epoch: 46 [37000/49000 (76%)]\tLoss: 1.548021\n",
      "Train Epoch: 46 [38000/49000 (78%)]\tLoss: 1.541141\n",
      "Train Epoch: 46 [39000/49000 (80%)]\tLoss: 1.531132\n",
      "Train Epoch: 46 [40000/49000 (82%)]\tLoss: 1.527213\n",
      "Train Epoch: 46 [41000/49000 (84%)]\tLoss: 1.539089\n",
      "Train Epoch: 46 [42000/49000 (86%)]\tLoss: 1.518313\n",
      "Train Epoch: 46 [43000/49000 (88%)]\tLoss: 1.530339\n",
      "Train Epoch: 46 [44000/49000 (90%)]\tLoss: 1.563234\n",
      "Train Epoch: 46 [45000/49000 (92%)]\tLoss: 1.572153\n",
      "Train Epoch: 46 [46000/49000 (94%)]\tLoss: 1.559665\n",
      "Train Epoch: 46 [47000/49000 (96%)]\tLoss: 1.548320\n",
      "Train Epoch: 46 [48000/49000 (98%)]\tLoss: 1.507930\n",
      "\n",
      "Test set: Avg. loss: 1.5517, Accuracy: 19105/21000 (90.98%)\n",
      "\n",
      "Train Epoch: 47 [0/49000 (0%)]\tLoss: 1.544682\n",
      "Train Epoch: 47 [1000/49000 (2%)]\tLoss: 1.525797\n",
      "Train Epoch: 47 [2000/49000 (4%)]\tLoss: 1.536420\n",
      "Train Epoch: 47 [3000/49000 (6%)]\tLoss: 1.541839\n",
      "Train Epoch: 47 [4000/49000 (8%)]\tLoss: 1.554437\n",
      "Train Epoch: 47 [5000/49000 (10%)]\tLoss: 1.532779\n",
      "Train Epoch: 47 [6000/49000 (12%)]\tLoss: 1.543199\n",
      "Train Epoch: 47 [7000/49000 (14%)]\tLoss: 1.537673\n",
      "Train Epoch: 47 [8000/49000 (16%)]\tLoss: 1.516561\n",
      "Train Epoch: 47 [9000/49000 (18%)]\tLoss: 1.542511\n",
      "Train Epoch: 47 [10000/49000 (20%)]\tLoss: 1.559740\n",
      "Train Epoch: 47 [11000/49000 (22%)]\tLoss: 1.511834\n",
      "Train Epoch: 47 [12000/49000 (24%)]\tLoss: 1.547734\n",
      "Train Epoch: 47 [13000/49000 (27%)]\tLoss: 1.556742\n",
      "Train Epoch: 47 [14000/49000 (29%)]\tLoss: 1.549534\n",
      "Train Epoch: 47 [15000/49000 (31%)]\tLoss: 1.524909\n",
      "Train Epoch: 47 [16000/49000 (33%)]\tLoss: 1.514241\n",
      "Train Epoch: 47 [17000/49000 (35%)]\tLoss: 1.513262\n",
      "Train Epoch: 47 [18000/49000 (37%)]\tLoss: 1.529460\n",
      "Train Epoch: 47 [19000/49000 (39%)]\tLoss: 1.542526\n",
      "Train Epoch: 47 [20000/49000 (41%)]\tLoss: 1.529094\n",
      "Train Epoch: 47 [21000/49000 (43%)]\tLoss: 1.545615\n",
      "Train Epoch: 47 [22000/49000 (45%)]\tLoss: 1.561864\n",
      "Train Epoch: 47 [23000/49000 (47%)]\tLoss: 1.556540\n",
      "Train Epoch: 47 [24000/49000 (49%)]\tLoss: 1.549656\n",
      "Train Epoch: 47 [25000/49000 (51%)]\tLoss: 1.521625\n",
      "Train Epoch: 47 [26000/49000 (53%)]\tLoss: 1.547527\n",
      "Train Epoch: 47 [27000/49000 (55%)]\tLoss: 1.566649\n",
      "Train Epoch: 47 [28000/49000 (57%)]\tLoss: 1.544234\n",
      "Train Epoch: 47 [29000/49000 (59%)]\tLoss: 1.541847\n",
      "Train Epoch: 47 [30000/49000 (61%)]\tLoss: 1.572932\n",
      "Train Epoch: 47 [31000/49000 (63%)]\tLoss: 1.535668\n",
      "Train Epoch: 47 [32000/49000 (65%)]\tLoss: 1.521539\n",
      "Train Epoch: 47 [33000/49000 (67%)]\tLoss: 1.556088\n",
      "Train Epoch: 47 [34000/49000 (69%)]\tLoss: 1.506810\n",
      "Train Epoch: 47 [35000/49000 (71%)]\tLoss: 1.503262\n",
      "Train Epoch: 47 [36000/49000 (73%)]\tLoss: 1.529076\n",
      "Train Epoch: 47 [37000/49000 (76%)]\tLoss: 1.555659\n",
      "Train Epoch: 47 [38000/49000 (78%)]\tLoss: 1.572225\n",
      "Train Epoch: 47 [39000/49000 (80%)]\tLoss: 1.522422\n",
      "Train Epoch: 47 [40000/49000 (82%)]\tLoss: 1.534024\n",
      "Train Epoch: 47 [41000/49000 (84%)]\tLoss: 1.584317\n",
      "Train Epoch: 47 [42000/49000 (86%)]\tLoss: 1.522028\n",
      "Train Epoch: 47 [43000/49000 (88%)]\tLoss: 1.564493\n",
      "Train Epoch: 47 [44000/49000 (90%)]\tLoss: 1.522026\n",
      "Train Epoch: 47 [45000/49000 (92%)]\tLoss: 1.538652\n",
      "Train Epoch: 47 [46000/49000 (94%)]\tLoss: 1.566119\n",
      "Train Epoch: 47 [47000/49000 (96%)]\tLoss: 1.559207\n",
      "Train Epoch: 47 [48000/49000 (98%)]\tLoss: 1.528226\n",
      "\n",
      "Test set: Avg. loss: 1.5532, Accuracy: 19021/21000 (90.58%)\n",
      "\n",
      "Train Epoch: 48 [0/49000 (0%)]\tLoss: 1.567849\n",
      "Train Epoch: 48 [1000/49000 (2%)]\tLoss: 1.538248\n",
      "Train Epoch: 48 [2000/49000 (4%)]\tLoss: 1.560346\n",
      "Train Epoch: 48 [3000/49000 (6%)]\tLoss: 1.512405\n",
      "Train Epoch: 48 [4000/49000 (8%)]\tLoss: 1.549327\n",
      "Train Epoch: 48 [5000/49000 (10%)]\tLoss: 1.530492\n",
      "Train Epoch: 48 [6000/49000 (12%)]\tLoss: 1.535559\n",
      "Train Epoch: 48 [7000/49000 (14%)]\tLoss: 1.544848\n",
      "Train Epoch: 48 [8000/49000 (16%)]\tLoss: 1.545700\n",
      "Train Epoch: 48 [9000/49000 (18%)]\tLoss: 1.524718\n",
      "Train Epoch: 48 [10000/49000 (20%)]\tLoss: 1.581539\n",
      "Train Epoch: 48 [11000/49000 (22%)]\tLoss: 1.510303\n",
      "Train Epoch: 48 [12000/49000 (24%)]\tLoss: 1.585227\n",
      "Train Epoch: 48 [13000/49000 (27%)]\tLoss: 1.508925\n",
      "Train Epoch: 48 [14000/49000 (29%)]\tLoss: 1.546859\n",
      "Train Epoch: 48 [15000/49000 (31%)]\tLoss: 1.526823\n",
      "Train Epoch: 48 [16000/49000 (33%)]\tLoss: 1.549379\n",
      "Train Epoch: 48 [17000/49000 (35%)]\tLoss: 1.527685\n",
      "Train Epoch: 48 [18000/49000 (37%)]\tLoss: 1.535299\n",
      "Train Epoch: 48 [19000/49000 (39%)]\tLoss: 1.522816\n",
      "Train Epoch: 48 [20000/49000 (41%)]\tLoss: 1.536709\n",
      "Train Epoch: 48 [21000/49000 (43%)]\tLoss: 1.542130\n",
      "Train Epoch: 48 [22000/49000 (45%)]\tLoss: 1.524313\n",
      "Train Epoch: 48 [23000/49000 (47%)]\tLoss: 1.528559\n",
      "Train Epoch: 48 [24000/49000 (49%)]\tLoss: 1.532562\n",
      "Train Epoch: 48 [25000/49000 (51%)]\tLoss: 1.523974\n",
      "Train Epoch: 48 [26000/49000 (53%)]\tLoss: 1.564506\n",
      "Train Epoch: 48 [27000/49000 (55%)]\tLoss: 1.524221\n",
      "Train Epoch: 48 [28000/49000 (57%)]\tLoss: 1.510634\n",
      "Train Epoch: 48 [29000/49000 (59%)]\tLoss: 1.565760\n",
      "Train Epoch: 48 [30000/49000 (61%)]\tLoss: 1.496552\n",
      "Train Epoch: 48 [31000/49000 (63%)]\tLoss: 1.549186\n",
      "Train Epoch: 48 [32000/49000 (65%)]\tLoss: 1.537834\n",
      "Train Epoch: 48 [33000/49000 (67%)]\tLoss: 1.518773\n",
      "Train Epoch: 48 [34000/49000 (69%)]\tLoss: 1.548453\n",
      "Train Epoch: 48 [35000/49000 (71%)]\tLoss: 1.527668\n",
      "Train Epoch: 48 [36000/49000 (73%)]\tLoss: 1.528623\n",
      "Train Epoch: 48 [37000/49000 (76%)]\tLoss: 1.518483\n",
      "Train Epoch: 48 [38000/49000 (78%)]\tLoss: 1.534285\n",
      "Train Epoch: 48 [39000/49000 (80%)]\tLoss: 1.557014\n",
      "Train Epoch: 48 [40000/49000 (82%)]\tLoss: 1.534876\n",
      "Train Epoch: 48 [41000/49000 (84%)]\tLoss: 1.543863\n",
      "Train Epoch: 48 [42000/49000 (86%)]\tLoss: 1.523021\n",
      "Train Epoch: 48 [43000/49000 (88%)]\tLoss: 1.526843\n",
      "Train Epoch: 48 [44000/49000 (90%)]\tLoss: 1.586059\n",
      "Train Epoch: 48 [45000/49000 (92%)]\tLoss: 1.573795\n",
      "Train Epoch: 48 [46000/49000 (94%)]\tLoss: 1.582682\n",
      "Train Epoch: 48 [47000/49000 (96%)]\tLoss: 1.556715\n",
      "Train Epoch: 48 [48000/49000 (98%)]\tLoss: 1.543598\n",
      "\n",
      "Test set: Avg. loss: 1.5516, Accuracy: 19114/21000 (91.02%)\n",
      "\n",
      "Train Epoch: 49 [0/49000 (0%)]\tLoss: 1.537154\n",
      "Train Epoch: 49 [1000/49000 (2%)]\tLoss: 1.572713\n",
      "Train Epoch: 49 [2000/49000 (4%)]\tLoss: 1.615766\n",
      "Train Epoch: 49 [3000/49000 (6%)]\tLoss: 1.530720\n",
      "Train Epoch: 49 [4000/49000 (8%)]\tLoss: 1.529298\n",
      "Train Epoch: 49 [5000/49000 (10%)]\tLoss: 1.550786\n",
      "Train Epoch: 49 [6000/49000 (12%)]\tLoss: 1.503988\n",
      "Train Epoch: 49 [7000/49000 (14%)]\tLoss: 1.520911\n",
      "Train Epoch: 49 [8000/49000 (16%)]\tLoss: 1.548859\n",
      "Train Epoch: 49 [9000/49000 (18%)]\tLoss: 1.522001\n",
      "Train Epoch: 49 [10000/49000 (20%)]\tLoss: 1.517341\n",
      "Train Epoch: 49 [11000/49000 (22%)]\tLoss: 1.515732\n",
      "Train Epoch: 49 [12000/49000 (24%)]\tLoss: 1.522240\n",
      "Train Epoch: 49 [13000/49000 (27%)]\tLoss: 1.530799\n",
      "Train Epoch: 49 [14000/49000 (29%)]\tLoss: 1.539017\n",
      "Train Epoch: 49 [15000/49000 (31%)]\tLoss: 1.505512\n",
      "Train Epoch: 49 [16000/49000 (33%)]\tLoss: 1.547376\n",
      "Train Epoch: 49 [17000/49000 (35%)]\tLoss: 1.508615\n",
      "Train Epoch: 49 [18000/49000 (37%)]\tLoss: 1.532738\n",
      "Train Epoch: 49 [19000/49000 (39%)]\tLoss: 1.559739\n",
      "Train Epoch: 49 [20000/49000 (41%)]\tLoss: 1.587546\n",
      "Train Epoch: 49 [21000/49000 (43%)]\tLoss: 1.581924\n",
      "Train Epoch: 49 [22000/49000 (45%)]\tLoss: 1.503729\n",
      "Train Epoch: 49 [23000/49000 (47%)]\tLoss: 1.550354\n",
      "Train Epoch: 49 [24000/49000 (49%)]\tLoss: 1.521552\n",
      "Train Epoch: 49 [25000/49000 (51%)]\tLoss: 1.542783\n",
      "Train Epoch: 49 [26000/49000 (53%)]\tLoss: 1.567642\n",
      "Train Epoch: 49 [27000/49000 (55%)]\tLoss: 1.529742\n",
      "Train Epoch: 49 [28000/49000 (57%)]\tLoss: 1.523295\n",
      "Train Epoch: 49 [29000/49000 (59%)]\tLoss: 1.523349\n",
      "Train Epoch: 49 [30000/49000 (61%)]\tLoss: 1.519441\n",
      "Train Epoch: 49 [31000/49000 (63%)]\tLoss: 1.503616\n",
      "Train Epoch: 49 [32000/49000 (65%)]\tLoss: 1.575179\n",
      "Train Epoch: 49 [33000/49000 (67%)]\tLoss: 1.517259\n",
      "Train Epoch: 49 [34000/49000 (69%)]\tLoss: 1.527717\n",
      "Train Epoch: 49 [35000/49000 (71%)]\tLoss: 1.536194\n",
      "Train Epoch: 49 [36000/49000 (73%)]\tLoss: 1.575941\n",
      "Train Epoch: 49 [37000/49000 (76%)]\tLoss: 1.554623\n",
      "Train Epoch: 49 [38000/49000 (78%)]\tLoss: 1.543967\n",
      "Train Epoch: 49 [39000/49000 (80%)]\tLoss: 1.567886\n",
      "Train Epoch: 49 [40000/49000 (82%)]\tLoss: 1.538291\n",
      "Train Epoch: 49 [41000/49000 (84%)]\tLoss: 1.526303\n",
      "Train Epoch: 49 [42000/49000 (86%)]\tLoss: 1.559376\n",
      "Train Epoch: 49 [43000/49000 (88%)]\tLoss: 1.592170\n",
      "Train Epoch: 49 [44000/49000 (90%)]\tLoss: 1.553717\n",
      "Train Epoch: 49 [45000/49000 (92%)]\tLoss: 1.548420\n",
      "Train Epoch: 49 [46000/49000 (94%)]\tLoss: 1.510180\n",
      "Train Epoch: 49 [47000/49000 (96%)]\tLoss: 1.563852\n",
      "Train Epoch: 49 [48000/49000 (98%)]\tLoss: 1.569219\n",
      "\n",
      "Test set: Avg. loss: 1.5527, Accuracy: 19083/21000 (90.87%)\n",
      "\n",
      "Train Epoch: 50 [0/49000 (0%)]\tLoss: 1.582777\n",
      "Train Epoch: 50 [1000/49000 (2%)]\tLoss: 1.580295\n",
      "Train Epoch: 50 [2000/49000 (4%)]\tLoss: 1.531850\n",
      "Train Epoch: 50 [3000/49000 (6%)]\tLoss: 1.560269\n",
      "Train Epoch: 50 [4000/49000 (8%)]\tLoss: 1.560887\n",
      "Train Epoch: 50 [5000/49000 (10%)]\tLoss: 1.512929\n",
      "Train Epoch: 50 [6000/49000 (12%)]\tLoss: 1.522030\n",
      "Train Epoch: 50 [7000/49000 (14%)]\tLoss: 1.510079\n",
      "Train Epoch: 50 [8000/49000 (16%)]\tLoss: 1.567201\n",
      "Train Epoch: 50 [9000/49000 (18%)]\tLoss: 1.541885\n",
      "Train Epoch: 50 [10000/49000 (20%)]\tLoss: 1.547999\n",
      "Train Epoch: 50 [11000/49000 (22%)]\tLoss: 1.518512\n",
      "Train Epoch: 50 [12000/49000 (24%)]\tLoss: 1.532796\n",
      "Train Epoch: 50 [13000/49000 (27%)]\tLoss: 1.550707\n",
      "Train Epoch: 50 [14000/49000 (29%)]\tLoss: 1.542435\n",
      "Train Epoch: 50 [15000/49000 (31%)]\tLoss: 1.534030\n",
      "Train Epoch: 50 [16000/49000 (33%)]\tLoss: 1.552407\n",
      "Train Epoch: 50 [17000/49000 (35%)]\tLoss: 1.518712\n",
      "Train Epoch: 50 [18000/49000 (37%)]\tLoss: 1.545652\n",
      "Train Epoch: 50 [19000/49000 (39%)]\tLoss: 1.532783\n",
      "Train Epoch: 50 [20000/49000 (41%)]\tLoss: 1.566014\n",
      "Train Epoch: 50 [21000/49000 (43%)]\tLoss: 1.530095\n",
      "Train Epoch: 50 [22000/49000 (45%)]\tLoss: 1.563437\n",
      "Train Epoch: 50 [23000/49000 (47%)]\tLoss: 1.533507\n",
      "Train Epoch: 50 [24000/49000 (49%)]\tLoss: 1.548644\n",
      "Train Epoch: 50 [25000/49000 (51%)]\tLoss: 1.537144\n",
      "Train Epoch: 50 [26000/49000 (53%)]\tLoss: 1.516267\n",
      "Train Epoch: 50 [27000/49000 (55%)]\tLoss: 1.537357\n",
      "Train Epoch: 50 [28000/49000 (57%)]\tLoss: 1.536128\n",
      "Train Epoch: 50 [29000/49000 (59%)]\tLoss: 1.551903\n",
      "Train Epoch: 50 [30000/49000 (61%)]\tLoss: 1.547863\n",
      "Train Epoch: 50 [31000/49000 (63%)]\tLoss: 1.536401\n",
      "Train Epoch: 50 [32000/49000 (65%)]\tLoss: 1.543005\n",
      "Train Epoch: 50 [33000/49000 (67%)]\tLoss: 1.579432\n",
      "Train Epoch: 50 [34000/49000 (69%)]\tLoss: 1.538475\n",
      "Train Epoch: 50 [35000/49000 (71%)]\tLoss: 1.532392\n",
      "Train Epoch: 50 [36000/49000 (73%)]\tLoss: 1.524699\n",
      "Train Epoch: 50 [37000/49000 (76%)]\tLoss: 1.533522\n",
      "Train Epoch: 50 [38000/49000 (78%)]\tLoss: 1.564352\n",
      "Train Epoch: 50 [39000/49000 (80%)]\tLoss: 1.517493\n",
      "Train Epoch: 50 [40000/49000 (82%)]\tLoss: 1.515236\n",
      "Train Epoch: 50 [41000/49000 (84%)]\tLoss: 1.533475\n",
      "Train Epoch: 50 [42000/49000 (86%)]\tLoss: 1.563616\n",
      "Train Epoch: 50 [43000/49000 (88%)]\tLoss: 1.531649\n",
      "Train Epoch: 50 [44000/49000 (90%)]\tLoss: 1.519689\n",
      "Train Epoch: 50 [45000/49000 (92%)]\tLoss: 1.562844\n",
      "Train Epoch: 50 [46000/49000 (94%)]\tLoss: 1.575322\n",
      "Train Epoch: 50 [47000/49000 (96%)]\tLoss: 1.521368\n",
      "Train Epoch: 50 [48000/49000 (98%)]\tLoss: 1.503219\n",
      "\n",
      "Test set: Avg. loss: 1.5521, Accuracy: 19098/21000 (90.94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc_torch = test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test_acc_torch = test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ----------------------------------Pytorch Full ANN  end------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the neural networks performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = [test_acc_torch, test_acc_ANN*100]\n",
    "models_names = [\"torch\", \"ANN 2 layer\"]\n",
    "plt.bar(models_names, results)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('model')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}